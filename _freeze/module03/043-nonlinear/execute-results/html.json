{
  "hash": "e1c660fd9d993f4e93041d011fc5122d",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute:\n  warning: false\n  message: false\n---\n\n\n\n\n\n\n# Non-linear regression\n\nLinear relationships are simple to interpret since the rate of change is constant -- i.e. as $x$ changes, $y$ changes at a constant rate. For non-linear relationships -- as $x$ changes, $y$ changes at an unproportional rate. To simplify interpretation and enable the use of linear models, it is often recommended to transform non-linear data to make it approximately linear.\n\nTransformation is usually possible for monotonic relationships (i.e. relationships that are always increasing or decreasing) such as exponential growth curves. The most common transformations are:\n\n| Name  | Transformation   |\n|-------|:----------------:|\n| Inverse | $y = \\frac{1}{x}$ |\n| Root | $y = \\sqrt[a]{x}$ |\n| Exponential | $y = e^x$ |\n| Logarithmic | $y = \\log_a(x)$ |\n\nHowever, in the case of non-monotonic relationships (e.g. polynomials, asymptotic, logistic) transformation may not be enough to make the data meet the assumptions. In this section, we look at fitting non-linear models.\n\n$$ Y_i = f(x_i, \\beta) + \\epsilon_i $$\nwhere $f(x_i, \\beta)$ is a nonlinear function of the parameters $\\beta$.\n\nThe assumptions for non-linear regression are INE (Independence, Normality, Equal variance). \n\n## Polynomial regression\n\nA polynomial equation is an extension to linear regression and can still be fitted using least squares. Typically, a polynomial equation has multiple terms of the same predictor (i.e. one $x$). The more terms in the polynomial, the more complex the model and the less likely it follows an actual biological relationship. A complex model may 'fit' the data well, but fail to represent the true relationship between the variables (overfitting).\n\n$$ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i $$\nwhere $k$ is the degree of the polynomial.\n\n- Linear: $y = \\beta_0 + \\beta_1 x$\n- Quadratic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n- Cubic: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$\n- Each level increases the power of the predictor by 1.\n\n### In practice\n\nTo fit a polynomial, we still use the `lm()` function. To create the polynomial terms, we use the `poly(x, n)` function, where `x` is the predictor and `n` is the degree of the polynomial.\n\nBelow is an example with asymptotic data -- it increases rapidly to a certain point and then levels off. The linear model (blue) does not capture the complexity of the relationship. The polynomial model with 10 terms (green) fits the data well, but it is too complex between $x$ = 5-10. In this case, the polynomial model with 2 terms (red), aka the quadratic model, fits the data the best.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate some data with an asymptotic relationship\nset.seed(442) # set seed\n\nasymptotic <- tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100 * (1 - exp(-0.5 * predictor)) + rnorm(length(predictor), mean = 0, sd = 10)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lin <- lm(response ~ predictor, asymptotic) # blue\nmod_poly2 <- lm(response ~ poly(predictor, 2), asymptotic) # red\nmod_poly10 <- lm(response ~ poly(predictor, 10), asymptotic) # green\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point(size = 2) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_lin)), color = \"slateblue\", size = 1.5, linetype = 2) +\n  geom_line(aes(y = predict(mod_poly2)), color = \"brown\", size = 1, linetype = 1) +\n  geom_line(aes(y = predict(mod_poly10)), color = \"seagreen\", size = 1.5, linetype = 2) +\n  theme_classic()\n```\n:::\n\n\n\n\nThis is also evident when comparing the `summary()` of the models. The linear model explains the least amount of variation in $y$ (`Multiple R-squared` = 57.01%). The 2-term polynomial model explains 81.95% of variation (`Adjusted R-squared`), whereas the 10-term polynomial model explains 86.21% of variation (`Adjusted R-squared`). The first three terms of the polynomial model are significant, the fourth is almost significant, but the remainder are not. Comparing the 2 and 10-term polynomial models, are the extra 8 terms worth an extra 4.26% of variation explained? Considering the 10-term polynomial is very complex and overfitted -- the principle of parsimony would say 'no'.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_lin)\nsummary(mod_poly2)\nsummary(mod_poly10)\n```\n:::\n\n\n\n\nThe hypothesis of a polynomial model is similar to that of multiple linear regression. A significance test is done for each individual coefficient, but we **only consider the significance of the highest order term**. For example, if a 10-term polynomial model is used but $x^10$ is not significant, then a simpler model should be considered.\n\n$$H_0: \\beta_k = 0$$\n$$H_0: \\beta_k \\neq 0$$\nThere is also an overall model significance, but if the highest order term is significant, the model will be significant.\n\n$$H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0$$\n$$H_0: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0$$\nIn the case that all coefficients are zero, the model is better represented by the mean of the data.\n\nAs for interpretation of the quadratic model, we can write the equation, but the coefficients do not have much meaning.\n\n$$\\text{response} = 79.82 + 159.37 \\cdot \\text{predictor} + -106.94 \\cdot \\text{predictor}^2$$\nGiven it is a quadratic equation, we could calculate the peak of the curve ($x_{peak} = -\\frac{b}{2a}$ and substitute to get $y_{peak} = c - \\frac{b^2}{4a}$). For a polynomial of a higher degree, it would not be meaningful.\n\nAlthough a little out of order, we can also check the assumptions. Recall that linearity is not an assumption for a non-linear model -- we can disregard the shape of the line for the *Residuals vs Fitted* plot, and focus on the distribution of the point around the line (which is even). The residuals fall on the line for the *Normal-QQ* plot indicating residuals are normally distributed, and the *Scale-Location* plot shows the residuals evenly distributed (equal varince), and there are no extreme outliers in the *Residuals vs Leverage* plot.  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(mod_poly2)\n```\n:::\n\n\n\n\n# Non-linear models\n\nTo fit a non-linear model, we use the `nls()` function. While it is still a regression model that minimises the sum of squares, as there is no closed-form analytical solution, so `nls()` (i.e. nonlinear least squares) fits the model iteratively with numerical methods. Thus it **requires initial parameter guesses**, which can be obtained from domain knowledge, data exploration, or trial and error.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnls(formula, data, start)\n```\n:::\n\n\n\n\n- `formula`: a formula object, with the response variable on the left of a ~ operator, and the predictor variable(s) on the right.\n- `data`: a data frame containing the variables in the model.\n- `start`: a named list of starting values for the parameters in the model.\n\nThe `nls()` function can be used to fit most non-linear models including but not limited to exponential, polynomial, asymptotic, and logistic models.\n\nSeveral self-starting functions are becoming available in R. These automatically estimate the initial parameters. There are some pre-defined functions available (e.g. `nlraa::SSexpf()`, `SSasymp()` and `SSlogis()`). These should give the same (or near identical) results as estimating the parameters as the model still uses nonlinear least squares.\n\nIn the event a slightly different function is required a new self-starting function can be defined -- this is quite [complex](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/selfStart), and beyond the scope of this course.\n\n## Exponential regression\n\nAn exponential relationship either grows or decays at an increasing rate. The equation for an exponential relationship is:\n\n$y = y_0e^{kx}$\n\nWhere,\n\n-   $y$ is the response and $x$ is the predictor\n-   $y_0$ is the value of $y$ when $x = 0$\n-   $k$ is the rate of change\n\n$k$ can be estimated with the equation $slope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}$, but usually a value of 1 is a good starting point. If it is a decay curve, $k$ will be negative (i.e. -1).\n\nBecause of the equation for the exponential curve, $y_0$ cannot be zero.\n\n### In practice\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\ngrowth <- tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = abs(exp(0.5 * predictor) + rnorm(length(predictor), mean = 1, sd = 5))\n)\n\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n```\n:::\n\n\n\n\nBased on the plot, we can estimate the initial parameters for the model. The lower limit ($y_0$) is around 0, and the rate of change ($k$) is around 1. We can then fit the model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_exponential <- nls(response ~ y0 * exp(k * predictor),\n  data = growth,\n  start = list(y0 = 0.1, k = 1)\n)\n```\n:::\n\n\n\n\nAfter we fit out model, we assess the assumptions plots and check our residuals for INE.\n\n- The *residuals vs fitted* and *standardised residuals vs fitted* plots are interpreted similar to linear regression -- there is an even spread of points around the '0' horizontal line which indicates equal variances. Recall linearity is not an assumption.\n- The *Normal Q-Q* plot shows the points are on the line, i.e. residuals are normally distributed.\n- The *Autocorrelation* plot is new but interpreted similarly to *residuals vs fitted* -- there should be random even scatter around the '0' horizontal line. This is a test for independence, and if there is a pattern (e.g. quadratic, line), it suggests the residuals are not independent.\n\nNonlinear regression is often done with purpose (i.e. the data has a certain shape or there is a known relationship), so the assumptions are often met. If they are not met, the fit and statistics may not be as reliable -- but the relationship may be too complex to model altogether.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumption plots\nlibrary(nlstools)\nresids <- nlsResiduals(mod_exponential)\nplot(resids)\n```\n:::\n\n\n\n\n### Interpretation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_exponential)\n```\n:::\n\n\n\n\nThe model is significant given the p-value is less than 0.05 for all parameters. If this were real data (e.g. population growth), the parameters themselves could be useful, e.g. rate of change. The parameterised model equation is: $ y = 1.17 \\cdot e^{-0.484x} $.\n\nThe R-squared value is not reported for nonlinear models as the sum of squares is not partitioned into explained and unexplained components. The **residual standard error** and plots can be used instead to compare between models.\n\nThe `Number of iterations to convergence` is the number of times the computer changed the parameters to try and get a better fit.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_exponential)), color = \"red\", size = 2)\n```\n:::\n\n\n\n\n### Poor estimation\n\nIf the starting values are too far (most likely the 'rate of change' term), the model will not run and there will be an error. However, there is some flexibility allowed. Below we use some inaccurate initial estimates -- the model still reaches the same result (fit, parameters, errors), it just takes more tries (iterations).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_exponential <- nls(response ~ y0 * exp(k * predictor),\n  data = growth,\n  start = list(y0 = 10, k = 2)\n)\n\nsummary(mod_exponential)\n```\n:::\n\n\n\n\n### Self-starting function\n\nThe self-starting function for exponential curves can be found in the `nlraa` package. The function is `SSexpf()` and has the same formula as above -- but has different names for parameters ($y_0$ = $a$, $k$ = $c$). We can re-define the names of the parameters when we use the function. It reaches the same result with less effort and typically fewer iterations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# currently not evaluating this code as decay object not found\nlibrary(nlraa)\nmod_exponential <- nls(response ~ SSexpf(predictor, y0, k), data = decay)\nsummary(mod_exponential)\n```\n:::\n\n\n\n\n\n## Horizontal asymptotic regression\n\nAn asymptotic relationship is a type of non-linear relationship where the response variable approaches a limit as the predictor variable increases. This is common in growth curves, where growth is rapid at first and then slows down as it approaches a maximum (e.g. age vs height, population growth, diminishing returns). There are multiple equations for an asymptotic relationship, but we will cover the equation that is covered by the self-starting function `SSasymp()`.\n\n$$ y = Asym + (R_0-Asym) \\cdot e^{-e^{lrc} \\cdot x} $$\n\n- $R_0$ is value of $y$ when $x = 0$.\n- $Asym$ is the upper limit: the maximum value of $y$.\n- $lrc$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n### In practice\n\nWith the `nls()` function, we first need to estimate our initial parameters for $R_0$, $Asym$ and $lrc$. We can do this by plotting the data and making an educated guess. For the asymptotic data, we can see that the lower limit ($R_0$) is around 0 and the upper limit ($Asym$) is around 100. The rate of change ($lrc$) is a little harder to estimate, so we will guess a value of 0.8. \n\nIf the model returns an error (singular gradient matrix at initial parameter estimates), we can try different values -- the most likely culprit will be the rate of change ($lrc$).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100),\n    arrow = arrow(length = unit(0.5, \"cm\")),\n    color = \"red\"\n  ) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model\nfit_asymptotic <- nls(response ~ Asym + (R0 - Asym) * (exp(-exp(lrc) * predictor)),\n  data = asymptotic,\n  start = list(R0 = 0, Asym = 100, lrc = 0.8)\n)\n```\n:::\n\n\n\n\nThe alternative is to use `SSasymp()` and skip the estimation of the parameters. We simply list the predictor, then the names we want the three parameters to be called. The order is pre-defined, so we must label Asym, R0 and lrc in that order. We could use letters (a, b, c) or any other interpretable names.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_asymptotic <- nls(response ~ SSasymp(predictor, Asym, R0, lrc), data = asymptotic)\n```\n:::\n\n\n\n\nAfter we fit out model, we assess the assumptions plots and check our residuals for INE -- they look fine.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assumption plots\nlibrary(nlstools)\nresids <- nlsResiduals(mod_asymptotic)\nplot(resids)\n```\n:::\n\n\n\n\nFinally, we evaluate the model. The model is significant since the p-value is less than 0.05 for all parameters. There is not much to interpret - except the `Residual standard error`. We can compare models directly with this error term. R^2^ is not calculated for non-linear models. \n\nThe resulting model equation would be $y = 98.5 + (-14.5-98.5) \\cdot e^{-e^{-0.463} \\cdot x}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_asymptotic)\n```\n:::\n\n\n\n\nWe can then plot the original data and the line of best fit.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(asymptotic, aes(predictor, response)) +\n  geom_point(color = \"black\") + # Original data points\n  geom_line(y = predict(mod_asymptotic), color = \"red\", size = 1) + # Fitted line\n  theme_minimal() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n```\n:::\n\n\n\n\n## Logistic regression\n\nLogistic regression is a type of non-linear regression typically used when the response variable is binary (0 or 1). The logistic function is an S-shaped or sigmoid curve that models the probability of the response variable being 1. The equation that `SSlogis()` (base R) assumes $y$ is positive and uses:\n\n$$ y = \\frac{Asym}{1+e^{\\frac{xmid-x}{scal}}} $$\nwhere\n\n- $Asym$ is the upper limit: the maximum value of $y$.\n- $xmid$ is the value of $x$ when $y$ is halfway between the lower and upper limits (inflection point).\n- $scal$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\n### In practice\n\nSo if we were to estimate the initial parameters...\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\nAnd we would estimate the initial parameters for the model. The upper limit ($Asym$) is around 300, the inflection point ($xmid$) is around 5, and the rate of change ($scal$) is around 1.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_logistic <- nls(response ~ Asym / (1 + exp((xmid - predictor) / scal)),\n  data = logistic,\n  start = list(Asym = 300, xmid = 5, scal = 1)\n)\n```\n:::\n\n\n\n\nBut we can also use the self-starting function `SSlogis()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_logistic <- nls(response ~ SSlogis(predictor, Asym, xmid, scal), data = logistic)\n```\n:::\n\n\n\n\nThe assumptions seem to be met.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresids <- nlsResiduals(mod_logistic)\nplot(resids)\n```\n:::\n\n\n\n\nAnd then we interpret.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_logistic)\n```\n:::\n\n\n\n\nThe model is significant since the p-value is less than 0.05 for all parameters. The parameterised model is $y = \\frac{310}{1+e^{\\frac{4.93-x}{1.35}}}$. The `Residual standard error` is 4.41 (which is not bad considering $y$ ranges from 0 ot 300).\n\nFinally, we visualise our model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(logistic, aes(predictor, response)) +\n  geom_point(color = \"black\") + # Original data points\n  geom_line(y = predict(mod_logistic), color = \"red\", size = 1) + # Fitted line\n  theme_minimal() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n```\n:::\n\n\n\n\n# Comparing Models\n\nWhile the examples so far have a distinct and clear relationship, it is often not as clear with real-world data. So how can we compare which model best fits the data?\n\nWe have come across several options already, e.g. R^2^ and residual standard error. The residual standard error (RSE) is the square root of the sum of squared residuals divided by the degrees of freedom. The lower RSE is, the better the model fits the data. There are many other 'prediction quality' metrics!\n\nSome commonly used examples include:\n\n- [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation)\n- [mean absolute error (MAE)](https://en.wikipedia.org/wiki/Mean_absolute_error)\n- [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion)\n",
    "supporting": [
      "043-nonlinear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}