{
  "hash": "366f426056e1cd29e6ee8c4b989f7c41",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 11 - Multiple Linear Regression\"\nembed-resources: true\nresources:\n  - data/ENVX1002_wk11_practical_data_Regression.xlsx\n  - data/Housing.csv\n  - data/energy_data.csv\n  - data/Advertising_Budget_and_Sales.csv\n  \n---\n\n\n\n\n\n\n\n\n::: callout-tip\n## Learning outcomes\n\n-   Learn to perform MLR and interpret the results using R;\n-   Undertake hypothesis testing to determine if model is significant\n-   Undertake hypothesis testing to determine if the true partial regression slope $\\neq$ 0\n-   Check assumptions are filled prior to assessing model output\n-   Assess model summary in terms of fit and P-values\n-   Consider more parsimonious models\n:::\n\n## Before you begin\n\nCreate your Quarto document and save it as `Lab-11.Rmd` or similar. The following data files are required:\n\n1) [ENVX1002_wk11_practical_data_Regression.xlsx](data/ENVX1002_wk11_practical_data_Regression.xlsx)\n\n\nLast week you explored simple linear regression and assessed the output of your models.\n\nThis week we will build upon this and venture into multiple linear regression.\n\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\n\nDon't forget to save as you go!\n\n## Exercise 1: Corn yields\n\nData: *Corn* spreadsheet\n\nIn this data\n\n-   y = P content of corn\\\n-   x~1~ = inorganic P content of soil\\\n-   x~2~ = organic P content of soil\\\n-   n = 17 sites (The original data had 18 sites, one is removed here.)\n\nAim of our investigation: Understand the relationship between Organic and Inorganic phosphorous contents in the soil, and the phosphorous content of corn. This will allow us to see which type of phosphorous is being taken up by the corn.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nCorn <- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Corn\")\nhead(Corn)\n```\n:::\n\n\n\n\n\n\n<br>\n\n### 1.1 Examine the correlations\n\nSome people find it difficult to visually interpret graphical summaries of data in more than 2 dimensions; however, 3-dimensional surface plots are reasonably common in statistics although not usually in descriptive statistics.\n\nInitially we will examine the pairwise correlations to \"get a feel\" for the data. We will then make a 3-dimensional surface plot using the `lattice` package .\n\nUsing R, we can calculate the correlation matrix quite easily.\n\nNote the use of `round()` to limit the number of significant digits.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(cor(Corn),3)\n```\n:::\n\n\n\n\n\n\na)  What do the results of the correlation matrix tell you?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** These results tell you that the correlation between CornP and InorgP the highest (r = 0.720) The correlation between CornP and OrgP (r = 0.212) or between OrgP and InorgP (r = 0.399) is much weaker.\n\n-----\n:::\n\nb)  Based on the correlation matrix, if we were to fit a single predictor model involving EITHER InorgP OR OrgP, then which model would be more successful?\n\n|           (Hint, the *r*^2^ is exactly that for a single predictor regression, the square of the correlation, *r*).\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** The more successful model would be $CornP = \\beta_0 + \\beta_1 InorgP$ as the correlation between CornP and InorgP is much higher than the relationship than CornP and OrgP.\n\n-----\n:::\n\nThe pairs plot creates scatterplots between each possible pair of variables. Like a single scatterplot the pairs plot allows us to visually observe any trends.\n\nc)  Observing the pairs plot below, do you see any strong trends? How well does this link to your correlation matrix?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(Corn)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**c)** We read the pairs plot like a correlation matrix. We can observe a slight trend between CornP and InorgP. The plots of CornP and OrgP, and that of OrgP and InorgP show much weaker trends and a more even scatter of points.\n\n-----\n:::\n\n<br>\n\n#### Simple 3-D plot\n\nUnlike a simple plot we can creat for a simple linear regression, it is a bit more complex to visualise a model with more predictors. One way we can visualise the relationship is with a 3-D plot, which can be made using the function `levelplot()` in lattice.\n\nHere we plot the OrgP and InorgP in the axes and the levels in the plot are CornP.\n\nNote the package [Viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) has been called, this is through personal choice.\n\nThe Viridis package has a range of assembled colour ramps which are easier for the reader to differentiate the colours, especially when printed in grayscale, or if the reader is colourblind.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lattice, quiet = T)\nlibrary(viridis, quiet = T) # will need to install.packages(\"viridis\")   \n\nlevelplot(CornP ~ InorgP + OrgP, data = Corn\n          , col.regions = viridis(100))\n```\n:::\n\n\n\n\n\n\nThe level plot shows us the x (InorgP) and y variable (OrgP), with the colour scale representing the z variable, which in this case is Phosphorous being taken up by the corn (cornP). From the plot we can see that with higher levels of OrgP and InorgP in the soil, the Phosphorous content in the corn is generally higher.\n\nIt is clear that the 3-D surface plot does not have colours everywhere, but this relates of course to the underlying data. In this case we don't have continuous data in both directions, so the response (the colour) is only plotted where we have input variables.\n\nIf we did have continuous data in both directions the plot would look more like a heatmap, [here are some examples](https://bookdown.org/content/b298e479-b1ab-49fa-b83d-a57c2b034d49/correlation.html#using-the-levelplot-function-of-lattice).\n\n<br>\n\n### 1.2 Fit the model\n\nWe will now use regression to estimate the joint effects of both inorganic phosphorus and organic phosphorus on the phosphorus content of corn.\n\n$CornP = \\beta_0 + \\beta_1 InorgP + \\beta_2 OrgP + error$\n\nThis is fairly simple and follows the same structure as simple linear regression and uses `lm()`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMLR.Corn <- lm(CornP ~ InorgP + OrgP, data=Corn)\n```\n:::\n\n\n\n\n\n\n<br>\n\n### 1.3 Check assumptions\n\nLet's check the assumptions of regression are met via residual diagnostics.\n\na)  Are there any apparent problems with normality of CornP residuals or equality of variance for this small data set?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(MLR.Corn)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** No there is not as the distribution is approximately normal and the variance is constant. In the residuals vs Leverage plot there is a point sitting close to the top right corner, but this is most likely because our dataset is small.\n\nInterpretation of the assumptions is quite subjective and often comes down to time and experience. If, for example in your Project 3 it is not obvious whether the assumptions have been met, make your decision and provide justification so that the viewer is able to come to the same conclusion as you. e.g. if you have a smaller dataset as you see in this exercise, it may be harder to tell whether assumptions have been met.\n\n-----\n:::\n\n<br>\n\n### 1.4 Model output\n\nAfter checking our assumptions and we are happy with them, we can interpret our model output.\n\na)  Incorporating the partial regression coefficient estimates, what is the model equation?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(MLR.Corn)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** CornP = 66.4654 + 1.2902 * InorgP - 0.1110 * OrgP\n\n-----\n:::\n\n<br>\n\nSimple linear regression allowed us to describe the relationship incorporating our regression coefficient estimate. We would interpret it as follows:\n\n*\"As* $x$ increases by 1, $y$ decreases by $b_1$ units\" (depending on the direction of the relationship).\n\nThis week it is a bit different because we are dealing with **partial** regression coefficients instead.\n\nInstead, we would say:\n\n*\"as* $x_1$ increases by 1, $y$ decreases by $b_1$ units given all other partial regression coefficients are held constant\".\n\nApplied to our model, if we wanted to describe the relationship between InorgP and CornP, we would say:\n\n*\"As InorgP increases by 1, CornP increases by 1.2902, given OrgP is held constant.\"*\n\nb)  Given the above, how would you interpret the relationship between OrgP and CornP?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** As OrgP increases by 1, CornP decreases by 0.1110, given InorgP is held constant.\n\n-----\n:::\n\n<br>\n\n### 1.5 Is the model useful?\n\nRemember now that the F-test and T-test are testing slightly different things.\n\n<br>\n\n#### F-test\n\n$H_0:$ all $\\beta_k = 0$, i.e. $\\beta_1 = \\beta_2 = 0$\n\n$H_1:$ at least 1 $\\beta_k \\neq 0$, i.e. our model is significant\n\nWe will find the P-value for this test at the end of the summary output.\n\n<br>\n\n#### t-test\n\n$H_0: \\beta_k = 0$\n\n$H_1: \\beta_k \\neq 0$\n\nWhere $\\beta_k$ refers to one of the model partial regression coefficients. In the case of our model we only have 2: $b_1$ (InorgP) and $b_2$ (OrgP).\n\n<br>\n\nNow it is your turn:\n\na)  Looking at the `summary()` output, is our overall model significant?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** Our model is significant as the P-value derived from the F-test statistic is less than 0.05 (P = 0.005). As P < 0.05, we reject our null hypothesis and can say at least one of our predictors are significant. \n\n-----\n:::\nb) Which independent variable is a significant predictor of corn yield?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** InorgP (P = 0.002)\n\n-----\n:::\n\n\n<br>\n\n### 1.6 How good is the model?\n\na) How much of the variation in CornP content is explained by the two independent variables?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** This question refers to the r-squared adjusted. As the adjusted r-squared = 0.4575, we can say that the model (containing our two independent variables) explains 45.8% of variation in CornP. \n\n-----\n:::\n\nb) Run the model again but this time with *only* the significant independent variable. How do the model performance criteria (r^2^-adj, P-values, Residual Standard Error) change?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** By removing OrgP (deemed non-significant as P = 0.662) and only retaining InorgP (P = 0.002), we can see that the r-squared adjusted has increased to 0.4864, indicating an improvement in model fit. \n\n-----\n:::\n\n<br>\n\n### 1.7 Our conclusions\n\nWriting up conclusions for multiple linear regression are similar to simple linear regression, just with a couple of extra P-values to state. \n\nWe would first mention that our overall model is significant as we rejected the null hypothesis (P = 0.05). We could then describe the hypothesis test results for our predictor variables. Finally, we would describe the model fit and our adjusted-r^2^. \n\nRemember the scientific conclusion then relates our findings back to the context, answering aims. \n\na) What would our statistical conclusion be?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** Statistical conclusion:\n\nThe F-test indicated our model is significant (P \\< 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0.\n\nOur t-test upon each of our partial regression coefficients supports this, as the P-value for InorgP was 0.002, much smaller than 0.05 and we can therefore reject the null hypothesis that this coefficient is equal to 0. In contrast, the P-value for OrgP was greater than 0.05 (P = 0.662), and so we fail to reject the null hypothesis, concluding OrgP is not a significant predictor in this model.\n\nThe fit of this model is moderate, with a residual standard error of 12.25 and an r^2^-adj = 0.4575.\n\nFurthermore this fit could be improved by the removal of the OrgP predictor within the model (r^2^-adj = 0.4864).\n\n-----\n:::\n\nb)  What would our Scientific conclusion be?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** Scientific conclusion:\n\nInorganic phosphorous is a significant predictor of phosphorous content in corn (P \\< 0.05), whereas organic phosphorous is not (P \\> 0.05). The model accounts for 45.8% of variation in the phosphorous content of corn.\n\n-----\n:::\n\n<br>\n\n## Exercise 2: Water quality\n\nData: *Microbes* spreadsheet\n\nThis exercise will use data from the [NOAA Fisheries data portal](https://www.webapps.nwfsc.noaa.gov/apex/parrdata/inventory/datasets). The dataset contains the results of microbial study of Pudget Sound, an estuary in Seattle, U.S.A.\n\nThe dataset contains the following variables:\n\n-   total_bacteria = Total bacteria (cells per ml) --\\> this will be our response variable\n\n-   water_temp = Water temperature (°C)\n\n-   carbon_L_h = microbial production (µg carbon per L per hour)\n\n-   NO3 = Nitrate (µm)\n\nFirst thing to do, is read in the data. This time we will be using the *Microbes* spreadsheet:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmic <- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Microbes\")\n\n# Check the structure\nstr(mic)\n```\n:::\n\n\n\n\n\n\n<br>\n\n### 2.1 Examine the correlations\n\nFor this dataset we may expect to see some correlations;\n\n-   Warmer water temperature we would expect to see a higher amount of bacterial growth\n\n-   Carbon is a proxy for microbial production, so if we see a higher rate of carbon production, we would expect to see higher levels of bacteria\n\n-   NO3 (Nitrate) is an essential nutrient for plants and some bacteria species metabolise this\n\na)  Let's test this. Observe the correlation matrix and pairs plots. Do you notice any strong correlations?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(mic)\n\npairs(mic)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**a)** The strongest correlation with total_bacteria is with NO3 (r = -0.759), followed by Carbon (r = 0.663) and water_temp (r = 0.645). \n\nObserving the pairs plot, there is something weird happening with NO3, so there may be another kind of relationship occurring here. \n\nThere are also some correlations between independent variables, but they are not strong enough to exclude from the model. \n\n-----\n:::\n\n<br>\n\n### 2.2 Fit the model\n\nWe can now fit the model to see how much these predictors account for the variation in total bacteria.\n\n$total bacteria = \\beta_0 + \\beta_1 watertemp + \\beta_2 carbon + \\beta_3 NO3 + error$\n\nThere are two forms the lm code can take; you can either specify which variables you want to include by naming each one, or if only your desired variables are within your dataset, you can use the \\~. to specify all columns.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(mic) # tells us column names within the dataset\n\n# Form 1:\nmic.lm <- lm(total_bacteria ~ water_temp + carbon_L_h + NO3, data = mic)\n\n# Form 2\nmic.lm <- lm(total_bacteria ~ ., data = mic)\n```\n:::\n\n\n\n\n\n\n<br>\n\n### 2.3 Check assumptions\n\nLet’s check the assumptions of regression are met via residual diagnostics.\n\na)  Are there any apparent problems with normality of total_bacteria residuals or equality of variance for this data set?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(mic.lm)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** Residuals vs fitted looks ok, as does normality. \n\nThere seems to be some slight clustering but most likely due to the data being obtained from two sampling periods (both within the same season), or potentially from different points. Water quality and microbial activity is highly variable, so this is not entirely unexpected. Nevertheless we can move on with this in mind. \n\nNote this question did not mention the Residuals vs Leverage plot. There is one value in the top right corner outside the first threshold. This indicates there may be a potential outlier within the predictors influencing the regression model, or maybe the current model is not the best for this data. \n\nYou are welcome to investigate this further, but for the sake of our example, we will move onto interpretation of the output. \n\n-----\n:::\n\n<br>\n\n### 2.4 Model output\n\nAfter investigating the assumptions, they seem to be ok, so we can move onto the model summary.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mic.lm)\n```\n:::\n\n\n\n\n\n\na)  Incorporating the partial regression coefficient estimates, what is the equation for this model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** total_bacteria = 766294 + 40051 * water_temp + 84425 * carbon_L_h - 16586 * NO3  \n\n-----\n:::\n\nb)  Like you did in Exercise 1.4, how would you interpret the relationship between total_bacteria and water_temp?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** As water_temp increases by 1, total_bacteria increases by 40051, given all other partial regression coefficients remain constant.   \n\n-----\n:::\n\n<br>\n\n### 2.5 Is the model useful?\n\na)  Observing the P-value of the F-statistic in the summary, can we say our model is significant?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n\n-----\n\n**a)** The P-value for the F-statistic is 1.318e-10, which is smaller than 0.05. We therefore reject the null hypothesis and can say our model is indeed significant as at least one of our predictors are significant. \n\n-----\n:::\n\nb)  Are any predictors significant?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** Yes; as the P-value for N03 is 0.00268 (P < 0.05), we can reject the null hypothesis that the true partial regression slope for this variable is not zero.  \n\nWater_temp and carbon_L_h are not significant predictors (P > 0.05), i.e. as we cannot reject the null hypothesis that the true partial regression slope of each is non-zero.\n\n-----\n:::\n\n<br>\n\n### 2.6 How good is the model?\n\na)  How much of the variation in total bacteria is explained by the three independent variables?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** The model is not the best; the Residual Standard error is huge, even in terms of the response variable, and the r^2^-adjusted is only 0.5913. \n\nThe model accounts for 59.1% of variation in total bacteria counts. \n\n-----\n:::\n\nb)  Run the model again but this time excluding the variable with the largest P-value. How do the model performance criteria (r^2^-adj, P-values, Residual Standard Error) change?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** The least significant predictor is carbon_L_h (P = 0.21624).\n\nAfter removing carbon_L_h, the r^2^-adj has decreased (r^2^-adj = 0.5868) and Residual Standard Error has increased (251900). In terms of p-values, that of water_temp has decreased, but is still non-significant. The P-value for NO3 has decreased substantially.\n\nFrom this, we can see the model has not been improved by eliminating the predictor.\n  \n-----\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(total_bacteria ~ water_temp + NO3, data = mic))\n```\n:::\n\n\n\n\n\n\n<br>\n\n### 2.7 Conclusions\n\na)  What would the statistical conclusion be for this model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** Statistical conclusion:\n\n  The F-test indicated our model is significant (P < 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0. \n\nOur t-test upon each of our partial regression coefficients supports this, as the P-value for NO3 was 0.003, smaller than 0.05 and we can therefore reject the null hypothesis that this coefficient is equal to 0. In contrast, the P-value for water_temp and carbon_L_h were greater than 0.05 (P = 0.095 and 0.216, respectively), and so we fail to reject the null hypothesis, concluding water_temp and carbon_L_h were not significant predictors within this model.\n\nThe fit of this model is moderate, with a residual standard error of 250500 and an r^2^ = 0.5913. \n\nNote: remember residual standard error is on the same scale as the response variable, so although it is a huge number, it is not as large when we think of the scale.  \n\nFurthermore this fit was not substantially improved by the removal of the water_temp predictor within the model (r^2^-adj = 0.587).\n\n-----\n:::\n\nb)  What would our scientific conclusion be?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**b)** Scientific conclusion: \n  \n  Nitrate is a significant predictor of total bacteria count in (P < 0.05), whereas water temperature and Carbon production rate were not is not in this case (P > 0.05). The model accounts for 59.1% of variation in the bacteria counts measured in water samples collected from Puget Sound. \n\n-----\n:::\n\n<br>\n\n## Exercise 3: Dippers\n\nData: *Dippers* spreadsheet\n\nWe will revisit the Dippers dataset from last week, but now incorporating other factors which may be influencing the distribution.\n\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\n\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\n\nTwenty-two sites were included in the survey. Variables are as follows\n\n-   water hardness\n\n-   river-bed slope\n\n-   the numbers of caddis fly larvae\n\n-   the numbers of stonefly larvae\n\n-   the number of breeding pairs of dippers per 10 km of river\n\nIn the analyses, the four invertebrate variables were transformed using a Log(Number+1) transformation.\n\n<br>\n\nNow it is your turn to work through the steps as above. What other factors are influencing the number of breeding pairs of Dippers?\n\na)  Read in the data from today's Excel sheet, the corresponding sheet name is \"Dippers\"\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n**a)** Use the following code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl, quietly = TRUE)\nDippers <- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\"\n                     ,\"Dippers\")\n```\n:::\n\n\n\n\n\n\n-----\n:::\n\nb)  Investigate a correlation matrix and pairs plot of the dataset, are there signs of a relationship between breeding pair density and other independent variables?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(Dippers)\n```\n\n```{.r .cell-code}\ncor(Dippers)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**b)** Looking at the pairs scatterplots there seems to be a positive relationship between Br_Dens and RiverSlope, LogCadd, and with LogStone. We can follow this up using correlation, which indicates there is a moderate positive relationship of 0.710 between Br_Dens and RiverSlope, 0.763 Br_Dens and LogStone, and 0.613 between Br_Dens and LogCadd.  \n\n-----\n:::\n\nc)  Let's investigate further. Run the model incorporating all of our predictors, but before looking at our model output, are the assumptions ok?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run model\ndipper.lm <- lm(Br_Dens ~ ., data=Dippers)\n\n# Check assumptions\npar(mfrow = c(2,2))\nplot(dipper.lm)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**c)** Assumptions look ok:\n  \n  * Residuals vs fitted: Points evenly scattered around mean.\n\n  * Normal Q-Q: Points follow line.\n\n  * Scale-Location: Points evenly scattered, no fanning. \n\n  * Residuals vs Leverage: No points occurring at top right or bottom right corners outside the dotted red lines. A couple of points are close, but not outside, so all is good. \n\nWe can continue to interpreting the output!\n\n-----\n:::\n\n<br>\n\nOnce you are happy assumptions are good, you can interpret the model output using `summary()`.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dipper.lm)\n```\n:::\n\n\n\n\n\n\n-----\n:::\n\nd)  What is the equation for our model, incorporating our partial regression coefficients?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**d)** Br_Dens = -1.530702 + 0.006916 * Hardness + 0.067924 * RiverSlope + 0.317611 * LogCadd + 0.752987 * LogStone\n\n-----\n:::\n\ne)  Based on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd this time?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**e)** The F-test statistic tells us whether our overall model is significant, which in this case it is (P = 1.971e-05). This time, unlike when we only used LogCadd as the predictor, our P-value from the F-Statistic is now different to the P-value of LogCadd (P = 0.16674).  \n\nAs our P - value for the model is < 0.05, we can reject the null hypothesis and conclude our model is significant and at least one of our partial regression slopes are non-zero. \n\n-----\n:::\n\nf)  Is LogCadd still a significant predictor of Dipper breeding pair density?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n  \n**f)** Unlike last week, the P-value for LogCadd is now greater than 0.05 (P = 0.16674), and so we fail to reject the null hypothesis that the partial regression slope is equal to zero.  We can therefore say LogCadd not a significant predictor of Dipper breeding pair density. \n\n-----\n:::\n\ng)  What are the significant predictors of this model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**g)** The only significant predictor for this model is LogStone (P < 0.05). \n\n-----\n:::\n\nh)  How good is the fit of our model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n  \n**h)** Our model fit is much better compared to the simple linear model from last week. \n\nOur residual standard is (Residual SE = 0.7753) and the r^2^-adjusted is closer to 1 than 0 (r^2^-adj = 0.7281). \n\nOur residual SE is a low value, and relative to the range of Br_Dens (min = 2.9, max = 8.4), which is pretty good and also much better than last week. \n\n-----\n:::\n\ni)  What might we do to improve the model fit?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**i)** Consider elimination of a non-significant predictor variable. \n\n-----\n:::\n\nj)  What statistical and scientific conclusions can we make from this model output?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n  \n**j) ** \nStatistical:\n\nAs the F-test statistic is less than 0.05, we can reject the null hypothesis that all partial regression coefficients are equal to zero. Furthermore, LogStone was the only significant predictor (P < 0.05) and therefore we can only reject the null for this predictor.   \n\nThe model fit is good, with an r^2^-adjusted value of 0.7281 and Residual standard error of 0.7753.  \n\nScientific:\n  \nThe number of Stonefly Larvae is a significant predictor of Dipper breeding pair density (P < 0.05), whereas other predictors used, such as water hardness, river slope and the number of Caddis Fly larvae are not significant. The overall model accounts for 72.8% of variation in Dipper breeding pair density.  \n\nThe significance of Stonefly larvae over Caddis fly larvae, and the positive relationship in the partial regression coefficient, suggests that Dippers may have a preference for Stonefly Larvae over Caddis Fly larvae.  \n\n-----\n:::\n\n<br>\n\nAnother week of linear models done! Great work fitting Multiple Linear Regression! Next week we break away and explore non-linear functions.\n\n# Bonus Take Home Exercises\nUse the template below to test the hypotheses for each exercise. \n\n1.  Scatterplot and correlation\n\n2.  Fit the model\n\n3.  Check assumptions\n\n4.  P-value and model fit\n\n    a)   Is the model significant?\n\n    b)   Are the predictors significant?\n\n    c)   How good is the model fit?\n\n5.  Conclusions\n\n## Exercise 1: House Prices\n\nData:\n- [Housing](data/Housing.csv)\n\nThis exercise will use a dataset from [kaggle](https://www.kaggle.com/datasets/vikramamin/housing-linear-regression)to explore the variables affecting house prices.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n1. State Aim:\n> To  \n\n2.  Scatterplot and correlation\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Read in data\nhousing<- read_csv(\"data/Housing.csv\")\n\nstr(housing) #There is a character variable that will mess with the analysis\n\n#Clean data\nhousing<- housing%>%\n  select(-Address)\n\nstr(housing)\n```\n:::\n\n\n\n\n\n\nLooking at the correlation matrix and the plots, it looks like the average number of bedrooms for a house in the area (`bedroom_no`) is fairly correlated to the average number of rooms for a house in the area (`room_no`). Additionally, `bedroom_no` has a very weak relationship to Price, so I will leave it out of the model to try to keep all the variables independent from each other. Everything else will go in the model.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(housing)\n```\n\n```{.r .cell-code}\nround(cor(housing), 3)\n```\n:::\n\n\n\n\n\n\n2.  Fit the model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhousing_mlr <- lm(Price~ area_income+ area_house_age + room_no + area_pop, data = housing)\n```\n:::\n\n\n\n\n\n\n\n3.  Check assumptions\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(housing_mlr)\n```\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\n```\n:::\n\n\n\n\n\n\n> Assumptions are beautiful and prefect. Relationship is linear, data is normally distributed, there is no fanning in the scale-location plot, and the residuals vs leverage doesn't even show the Cook's distance line.\n\n4.  P-value and model fit\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(housing_mlr)\n```\n:::\n\n\n\n\n\n\n    a)   Is the model significant? Yes, the f tests shows p < 0.05\n\n    b)   Are the predictors significant? All the predictors are significant, with p-values < 0.05\n\n    c)   How good is the model fit? The model has an excellent fit. The R^2^ = 0.9179, which indicates that the model explains ~92% of the variation in the data.\n\n5.  Conclusions\n\n>  The F-test indicated our model is significant (P < 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0. \n\n> Our t-test upon each of our partial regression coefficients supports this, as area income, area house age, room number and area population all have p-values below 0.05.\n\n> The fit of the model is excellent, with ~92% of the variation explained by our model.\n\n--------\n:::\n\n## Exercise 2: Energy use\n\nData:\n\n- [energy](data/energy_data.csv)\n\nUse the dataset from [kaggle](https://www.kaggle.com/datasets/govindaramsriram/energy-consumption-dataset-linear-regression) to explore which variables affect energy consumption.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Read data\nenergy<- read_csv(\"data/energy_data.csv\")\n\nstr(energy)\n\n#Keep only the numeric variables\nenergy_numeric <- energy%>%\n  select(-building_type, -day_of_week)\n```\n:::\n\n\n\n\n\n\n1.  Scatterplot and correlation\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(energy_numeric)\n```\n\n```{.r .cell-code}\nround(cor(energy_numeric),3)\n```\n:::\n\n\n\n\n\n> Not too much correlation between independent variables, but `appliances_used` has a very weak relationship with `energy_consumption`. I'm going to leave it out of my model.\n\n2.  Fit the model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nenergy_lm <-lm(energy_consumption~ square_footage + occupant_number, data = energy_numeric )\n```\n:::\n\n\n\n\n\n\n3.  Check assumptions\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(energy_lm)\n```\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\n```\n:::\n\n\n\n\n\n> All the assumptions look fine. The points are starting to veer off the line at the edges of the QQ plot, but I'm not worried about it.\n\n4.  P-value and model fit\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(energy_lm)\n```\n:::\n\n\n\n\n\n\n    a)   Is the model significant? yes, the f-test shows p< 0.05\n\n    b)   Are the predictors significant? yes, the t tests for theindividual predictors all have p < 0.05\n\n    c)   How good is the model fit? The model has a decent fit, with an adjusted R^2^ of 0.65\n\n5.  Conclusions\n>  The F-test indicated our model is significant (P < 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0. \n\n> Our t-test upon each of our partial regression coefficients supports this, as area income, area house age, room number and area population all have p-values below 0.05.\n\n> The fit of the model is good, with 65% of the variation explained by our model.\n\n-------\n:::\n## Exercise 3: Sales vs advertising budget\n\nData:\n\n- [advertising budget](data/Advertising_Budget_and_Sales.csv)\n\nUse the dataset from [kaggle](https://www.kaggle.com/datasets/yasserh/advertising-sales-dataset) to explore how different advertising budgets affect sales. \n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n-----\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsales<- read_csv(\"data/Advertising_Budget_and_Sales.csv\")\n\nstr(sales)\n```\n:::\n\n\n\n\n\n\n1.  Scatterplot and correlation\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(sales)\n```\n\n```{.r .cell-code}\nround(cor(sales),3)\n```\n:::\n\n\n\n\n\n> The first variable is just the row ID, so I'm going to ignore it. All the independent variables look like they have a relationship with the target variable, though the relationship with `newspaper_ad_budget` is rather weak. There aren't any particularly strong correlations between the budgets, so I will include all of them in my model.\n\n2.  Fit the model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsales_mlr<- lm(Sales~tv_ad_budget+radio_ad_budget+ newspaper_ad_budget, data = sales)\n```\n:::\n\n\n\n\n\n\n\n3.  Check assumptions\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(sales_mlr)\n```\n\n```{.r .cell-code}\npar(mfrow = c(1,1))\n```\n:::\n\n\n\n\n\n> All the assumptions look fine. The residuals vs fitted plot is a bit squished, but the line is relatively straight, so I'm not worried about it.\n\n4.  P-value and model fit\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sales_mlr)\n```\n:::\n\n\n\n\n\n\n\n    a)   Is the model significant? Yes, the F-test shows p< 0.05.\n\n    b)   Are the predictors significant? The tv and radio ad budgets are both significant, but the newspaper ad budget was not significant (p = 0.86)\n\n    c)   How good is the model fit? The model has an excellent fit with an adjusted R^2^ of 0.8956\n\n5.  Conclusions\n>  The F-test indicated our model is significant (P < 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0. \n\n> Our t-test upon each of our partial regression coefficients supports this, as `tv_ad_budget` and `radio_ad_budget` both have p-values below 0.05. However, `newspaper_ad_budget` was not a significant predictor, with p = 0.86.\n\n> The fit of the model is excellent, with ~90% of the variation explained by our model. Since the `newspaper_ad_budget` was not significant, we could try running the model again without this variable, to see if it imporves the fit.\n\n------\n:::\n\n<br> <br> <!--The code below reduces white space at the end of the output doc-->\n\n::: {.tocify-extend-page data-unique=\"tocify-extend-page\" style=\"height: 0;\"}\n:::\n",
    "supporting": [
      "Lab11_files/figure-typst"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}