{
  "hash": "c872e217a2a55000fb68676db257802e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 12 - Non-linear models\"\nembded-resources: true\nresources:\n  - data/east_creek.csv\n---\n\n\n\n\n\n\n\n\n::: callout-tip\n## Learning outcomes\n\n-   Calculate “by hand” the initial estimates of the parameters of a\n    non-linear model\n-   Interpret tables of regression coefficients for polynomials to\n    perform hypothesis testing\n-   Fit polynomials and non-linear models to data using least-squares\n    fitting using the SOLVER add-in in Excel\n-   Fit polynomials and non-linear models to data in R, and interpret\n    the outputs\n:::\n\n## Before we begin\n\nCreate your Quarto document and save it as `Lab-12.qmd` or similar.\n\nThe following data files are required:\n\n1)  [east_creek.csv](data/east_creek.csv)\n\nOver the past few weeks you have explored linear models and how to\ninterpret model summary output. Again we have stepped up the complexity,\nnow venturing into the world of non-linear models.\n\nThis practical focuses on fitting non-linear models to data with an\nemphasis on 3 important classes of functions that all budding biologists\nand environmental scientists should know\n\n-   polynomials,\n-   exponential models, and\n-   logistic models.\n\n**A question before we begin:**\n\nWhat are some advantages and disadvantages of non-linear models as\ncompared to polynomials?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n| Polynomials | Non-linear models(exponential/logistic) |\n|-------------------------------------------|-----------------------------|\n| Parameters hard to interpret | Parameters interpretable in terms of biological and environmental processes |\n| Can be fitted with analytical or numerical methods | Fitting parameters can be difficult as use numerical methods which rely on good initial estimates of parameters |\n| Hypothesis testing and estimating R^2^ straightforward | Hypothesis testing and estimating R^2^ is a bit more difficult – not absence of R^2^ in R output |\n| Difficult to model horizontal asymptotes | Can accommodate horizontal asymptotes |\n| Only polynomials of even order restricted to positive values | Constrained to positive values |\n\n------------------------------------------------------------------------\n:::\n\n<br>\n\n## Polynomials\n\n::: {.pad-box-mini style=\"background-color: #f2f2f2; padding:10px;\"}\n**Quadratic**\n\n$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n\nwhere the parameters are the y-intercept (*b~0~*), the linear component\n(*b~1~*) and the quadratic component (*b~2~*).\n\nIf *b~2~* is negative then the shape of the function is convex upwards,\ni.e. *y* increases with *x* until reaches a peak and then *y* decreases.\n\nIt is easy to understand so it has been commonly used for modelling the\nresponse of yield to inputs such as fertiliser, seeding rates. This is\ndespite much criticism for being unrealistic.\n\nLimitations:\n\n(i) rate of increase to peak is same as rate of decrease past peak\n\n(ii) does not level off as *x* becomes small or very large, *y* just\n     keeps increasing or decreasing.\n\n**Cubic**\n\n$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3$\n\nCompared to the quadratic model which has 1 turning point, a cubic model\nhas 2 turning points.\n:::\n\n<br>\n\n### Exercise 1: Interpreting polynomials\n\nA study was performed to examine the soil properties that control the\nwithin-field variation in crop yield. The focus of this question is on\nsoil pH which (among other things) controls the availability of nutrients\nto plants.\n\nThis exercise does not require you to read in any data, but rather focus\non interpreting the model output and comparing the models.\n\nThe figure below shows the raw observations of yield plotted against pH\nwith three models fitted; a linear model, quadratic polynomial and a\ncubic polynomial.\n\na)  which line corresponds to which model?\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<br>\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**a)**\n\n-   The blue line is the linear model – it is straight.\n\nNote: If the quadratic or higher order terms are very small for some\nvalues of the predictor the line could also appear to be straight.\n\n-   The red line is the quadratic, it has one turning point (\\~pH = 7.4)\n    where the slope of the line = 0.\n\n-   The black line is the cubic, it has two turning points, one is\n    around pH \\~ 7.5, the other is not on the range of the plot but you\n    can see that the curve is changing direction and bending up at a pH\n    of \\~ 6.8, as it transitions from concave down to concave up as\n    values get smaller. This is useful as it allows for a gentler\n    increase in the slope of the fitted line at smaller values of pH.\n\n------------------------------------------------------------------------\n:::\n\nb)  based on the output from the 3 models below, which model fits the\n    data best? Note: no hypothesis testing yet, just how well the model\n    fits the data (R^2^).\n\n<!-- -->\n\n1.  Linear model:\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n2.  Quadratic model:\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n3.  Cubic model:\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**b)** The R^2^ assesses how well the model fits the data. The best\nmodel based on this metric is the cubic polynomial (R^2^ = 0.255), then\nthe quadratic polynomial (R^2^ = 0.231) and finally the linear model\n(R^2^ = 0.00016).\n\nIt should be noted that this does not consider the model complexity\n(i.e. aiming for the most parsimonious model - the one with least\npredictors). For example, is the extra complexity of the cubic\npolynomial worth an increase in R^2^ of 0.024? Formal hypothesis testing\ncan address this question.\n\n------------------------------------------------------------------------\n:::\n\n<br>\n\nc)  Use the R output to perform hypothesis testing to find the best\n    model. Write out the hypotheses you are testing.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**c)** The general approach is to start with the most complex model and\nwork backwards to the simplest model.\n\nFirst we examine the cubic polynomial and more specifically the cubic\nterm (the most complex term) where the hypotheses we are testing are:\n\nH~0~: $\\beta_3 = 0$,\n\nH~1~: $\\beta_3 \\neq 0$\n\nThe line of R output that addresses this is the 4th line of the\ncoefficients table (i.e. highest order):\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nThe P-value is \\> 0.05 so we fail to reject the null hypothesis. This\nmeans we should remove the cubic term.\n\nNote that the estimates of the quadratic and linear terms are\nconditional on the cubic term also being in the model so will change if\nwe fit a quadratic polynomial (see the R output).\n\nNext we consider the quadratic polynomial model:\n\nH~0~: $\\beta_2 = 0$,\n\nH~1~: $\\beta_2 \\neq 0$\n\nThe line of R output that addresses this is the 3rd line of the\ncoefficients table (i.e. highest order):\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nThe P-value is \\< 0.05 so we reject the null hypothesis and would keep\nthe quadratic term in the model. The best model to use is the quadratic\npolynomial.\n\nNote that the linear term is also significant but even if it were not,\nwe would keep it in the model as in most situations it is hard to\njustify a situation where a biological or environmental process could be\nmodelled by a quadratic term alone.\n\nIn summary for polynomials we only test the highest order term in each\npolynomial in deciding whether to keep use the polynomial at all.\n\n------------------------------------------------------------------------\n:::\n\n<br>\n\n### Exercise 2: Fitting polynomials in R\n\nThis exercise will use real data from a yield-fertiliser trial in\nBedfordshire, United Kingdom.\n\nFirst thing we can do is fit a linear model to the fertiliser data:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create fertiliser and yield objects\nfert <- c(0, 100, 170, 225)\nyield <- c(3.32, 5.23, 5.41, 5.02)\n\n# Fits a linear model and saves it to an object called lin.mod\nlin.mod <- lm(yield ~ fert)\n\n# Summarises key features of model\nsummary(lin.mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ fert)\n\nResiduals:\n      1       2       3       4 \n-0.4469  0.6727  0.2995 -0.5252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 3.766929   0.634765   5.934   0.0272 *\nfert        0.007904   0.004243   1.863   0.2035  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7134 on 2 degrees of freedom\nMultiple R-squared:  0.6344,\tAdjusted R-squared:  0.4515 \nF-statistic:  3.47 on 1 and 2 DF,  p-value: 0.2035\n```\n\n\n:::\n:::\n\n\n\n\n\n\na)  What is the model fit like in this model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**a)** Our model fit is ok, with R^2^ of 0.6344.\n\n------------------------------------------------------------------------\n:::\n\nFit and plot a quadratic polynomial in R. In R a quadratic polynomial\ncan be fitted using the following lines of code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a new variable which is the square of the fertilizer rates\nfert2 <- fert^2\n\n# fit the quadratic model incorporating fert2\nquad.mod <- lm(yield ~ fert + fert2)\n\nsummary(quad.mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yield ~ fert + fert2)\n\nResiduals:\n        1         2         3         4 \n-0.005611  0.024528 -0.032791  0.013874 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  3.326e+00  4.324e-02   76.92  0.00828 **\nfert         2.786e-02  9.014e-04   30.91  0.02059 * \nfert2       -9.064e-05  3.921e-06  -23.12  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0436 on 1 degrees of freedom\nMultiple R-squared:  0.9993,\tAdjusted R-squared:  0.998 \nF-statistic: 731.7 on 2 and 1 DF,  p-value: 0.02613\n```\n\n\n:::\n:::\n\n\n\n\n\n\nb)  What is the fit like for our quadratic model? is it better than our\n    linear model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**b)** Our model fit even better, with R^2^ of 0.9993.\n\n------------------------------------------------------------------------\n:::\n\nIn Excel it is easy to fit a line, by creating a scatterplot, then **add\nTrendline...** and selecting **Polynomial** (2nd order).\n\nTo fit our polynomial line in R, we need to obtain model predictions\nfirst.\n\nTo plot model predictions you first need to predict at fine intervals of\nthe predictor to make a continuous plot that is not jagged or stepped.\nTo create a new prediction dataset you can use the following code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# creates a sequence of numbers from 0 to 225 going up in increments of 1\nnew.fert <- seq(0, 225, 1)\n```\n:::\n\n\n\n\n\n\nWe can use our model to predict at the values in the new prediction\ndataset, in this case `new.fert`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew.pred <- predict(quad.mod, list(fert = new.fert, fert2 = new.fert^2))\n```\n:::\n\n\n\n\n\n\nThe general form of the `predict` function is\n`predict(model object, list object)`.\n\nThe list object tells R what object contains the data we will use to\npredict. For example in our case the model was built on fert and fert2\nso we have to tell the predict function what object contains the new\nvalues for each of these, in our case new.fert.\n\nNow we plot the raw observation as points and add an overlay of the\nmodel fit as lines:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fert, yield, xlab = \"Fertilizer\", ylab = \"Yield\")\nlines(new.fert, new.pred) # Adds lines to original plot\n```\n:::\n\n\n\n\n\n\n<br>\n\n## Exponential function\n\n::: {.pad-box-mini style=\"background-color: #f2f2f2; padding:10px;\"}\n$y=y_0e^{kx}$\n\nwhere the parameters are *y~0~* which is the multiplier which expresses\nthe starting or final value, and *k* which is negative for exponential\ndecay and positive for the exponential growth.\n\nThe half life (for decay) or doubling time (for growth) can be\ncalculated as\n\n$\\frac{log_e 2}{k}$\n\nLimitations:\n\n(i) harder to fit than polynomials\n\n(ii) exponential growth has no horizontal asymptote; keeps going up.\n:::\n\n<br>\n\n### Exercise 3: Initial estimates for exponential growth function\n\nIn this exercise we will find initial estimates of the parameters of an\nexponential growth model by visual assessment of plots of the data\nand/or rough calculations. The initial estimates of the parameters are\nneeded as starting points for the iterative fitting methods we will use\nin the practicals, e.g. SOLVER in Excel and the `nls()` function in R.\n\nThe plot and table below presents the population of the world from\n1650-1965.\n\nWe wish to model the data with an exponential growth function of the\nform;\n\n$y=y_0e^{kx}$\n\nwhere\n\n-   *y* is the population in the year *x*,\n\n-   y~0~ is the population in 1650 and\n\n-   *k* is the rate constant.\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n<br>\n\na)  Provide an initial estimate of y~0~.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**a)** Use the first measurement, in 1650 the world population was 0.5\nbillion.\n\n------------------------------------------------------------------------\n:::\n\nThe parameter *k* can be estimated from a linear model fitted to log~e~\npopulation against year.\n\nRather than formally fitting a linear model you could estimate the slope\napproximately by using the smallest and largest value to estimate the\nslope and therefore *k*.\n\nb)  Use this approach to estimate k.\n\nHint: $$\nslope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}\n$$\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**b)** The smallest value is 0.5 in 1650 and the largest value is 3\nbillion in 1960.\n\nWe log~e~ the population values and calculate the slope of the line\nbetween the 2 observations. The slope is an estimate of k:\n\n$$\nslope = k = \\frac{log_e 3 - log_e 0.5}{1960-1650} = \\frac{1.792}{310}  = 0.00577987\n$$\n\n------------------------------------------------------------------------\n:::\n\nc)  For an exponential growth model the doubling time of a population\n    can be estimated by log~e~2 /k.\n\nExamine the graph and/or table to estimate the doubling time and use\nthis to estimate *k*. You will have to make *k* the subject in the\nequation for estimating the doubling time.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**c)** The smallest value is 0.5 in 1650 and the year it has doubled (1\nbillion) is in 1804.\n\nUsing the difference between the two years, we can solve for *k* :\n\n1804-1650 = 154\n\n$154 = \\frac{log_e 2}{k}$ ; multiply both sides by *k*\n\n$154k = log_e 2$ ; divide both sides by 154\n\n$k = \\frac{log_e 2}{154} = 0.004501$\n\nAlternatively, you can also use the slope calculation to obtain *k* for\nthe doubling time:\n\n$$\nslope = k = \\frac{log_e 1 - log_e 0.5}{1804-1650} = \\frac{0.693}{154}  = 0.004501\n$$\n\n------------------------------------------------------------------------\n:::\n\nd)  How similar were the estimates of *k*?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**d)** Both estimates are of a similar magnitude. It should be noted\nthat the estimate of the slope (b1) is an approximation. If we formally\nestimate the slope (`lm(log(pop)~year`) the value is 0.005835.\n\n------------------------------------------------------------------------\n:::\n\n<br>\n\n### Exercise 4 : Exponential growth models\n\nThis data is from Jenkins & Adams (2010) who studied soil respiration\nrates against temperature for different vegetation communities in the\nSnowy Mountains. They fitted an exponential growth model to the data.\n\nThe purpose of this exercise is to illustrate the dangers of using\nExcel’s in-built functions for statistics more complex than calculating\nmeans and fitting simple models.\n\nPlot the data in Excel and using the **Add Trendline...** option. Make\nsure tick the option for displaying the equation in the graph.\n\na)  The researchers performed the experiment up to a temperature of 40\n    degrees C, would you expect exponential growth in the respiration\n    rate to continue if high temperatures were considered? Is there a\n    better model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**a)** Eventually it would get too hot and the microbes which cause soil\nrespiration would start to “die” or the community structure would\nchange.\n\nSome sort of horizontal asymptote could be reached, which could be\nmodelled by a logistic model, or a decline is needed after a peak; there\nare non-linear models that accommodate such data that follows such a\npattern.\n\n------------------------------------------------------------------------\n:::\n\nNow fit the same model in R using the nls function. Code to get you\nstarted is:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp<-c(5,10,20,25,30,35,40)\nrespiration<-c(1,2,4,6,8,11,18)\n\n##Initial parameters\nexp.mod<-c(y0=1.0,k=0.1)\n\n##Fits exponential model\nres.exp<-nls(respiration ~ y0 * exp(k*temp), start=exp.mod,trace=T)\n\n##Summarise model\nsummary(res.exp)\n```\n:::\n\n\n\n\n\n\nb)  Now you can fit a line to the plot. Does this look similar to your\n    trendline in Excel?\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plots raw data\nplot(temp,respiration,xlab='Temperature',ylab='Respiration')\n\n#Creates new dataset for predictions ( 5 to 40 at an interval of 1)\ntemp.new<-seq(5,40,1)\n\n#Makes predictions onto temp.new\npred.exp<-predict(res.exp,list(temp=temp.new))\n\n#Adds model fit to existing plots\nlines (temp.new,pred.exp)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**b)** Yes, they do look similar.\n\n------------------------------------------------------------------------\n:::\n\nCompare the parameters values between Excel and R. You can extract the\nRSS value from an `nls` object by using the code below:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndeviance(res.exp)\n```\n:::\n\n\n\n\n\n\nc)  Calculate the RSS value for the Excel exponential model. Based on\n    this, which is the better model?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**c)** The better model is the one fitted in R which is optimized to fit\nthe actual observations rather than the approximate linearisation\napproach used by Excel.\n\n------------------------------------------------------------------------\n:::\n\nWhen faced with the need to fit an exponential function, one approach\nthat was used before computing power became readily accessible was to\nlog the *y* values which linearises the relationship with *x*, enabling\nthe modeller to use a simple linear model.\n\nIf we linearise, model would be $log_e(y) = b_0 + b_1x$, where $e^{b_0}$\nis the *y~0~* parameter in an exponential model, and *b~1~* is the *k*\nparameter in the exponential model. This is similar to what was\ndemonstrated in the lecture this week.\n\nIn Excel, log the soil respiration data and fit a linear model. You will\nsee that the fitted model gives the same values as the exponential model\nfitted to the untransformed data.\n\nIf you compare the R^2^ values for both you will see they are the same.\nThis means that Excel reports the R^2^ of the linear model fitted to\nlogged respiration as the R^2^ of the exponential model fitted to the\nraw data. This is naughty of Excel.\n\n::: {.pad-box-mini style=\"background-color: #f2f2f2; padding:10px;\"}\nFor the dataset used here the exponential model fits it so well the\nExcel approach is only slightly different to the correct approach used\nin R.\n\nIn cases where the model does not fit the data so well the differences\nwould be larger. Logarithm makes smaller values larger and larger values\nsmaller.\n\n**WHY IS THIS SUB-OPTIMAL?**\n\n-   Regression modelling assumes that the residuals are normally\n    distributed so logging normally distributed data will change the\n    distribution to a non-normal one – it is best to analyse the data\n    without transformation.\n\n-   Modelling data on the logged scale reduces the impact that larger\n    values have on minimising the RSS but when you plot the fitted model\n    with the original data you may observe large discrepancies for\n    larger values. In other words, using a linear model on the log(data)\n    can result in a higher discrepancy for larger values when plotting\n    the fitted model on the original data. Therefore, the model fitted\n    to the logged data is not necessarily the best on the original data.\n\n-   The reporting of the R-squared on the logged data on a model\n    purported to be fitted to untransformed data is just wrong as the\n    R-squared on the log scale will be better as the variation in the\n    data has been reduced but we really want to know how well an\n    exponential model fits the raw data.\n:::\n\n<br>\n\n## Logistic function\n\n::: {.pad-box-mini style=\"background-color: #f2f2f2; padding:10px;\"}\n\nThere are several versions of the logistic function. We will use the following:\n\n$$ y = \\frac{Asym}{1+e^{\\frac{xmid-x}{scal}}} $$\n\nwhere\n\n- $Asym$ is the maximum value of $y$ (upper limit, horizontal asymptote).\n- $xmid$ is the value of $x$ when $y$ is halfway between the lower and upper \nlimits (inflection point, $y = 0.5 \\times Asym$).\n- $scal$ is the rate of change: the rate at which $y$ approaches the upper limit.\n\nCommonly used to model growth that has a sigmoid shape, i.e. where\ngrowth is initially slow, then picks up to a maximum, then slows down\nagain as the system reaches a maximum.\n\nLimitations:\n\nHarder to fit than polynomials.\n:::\n\n<br>\n\n### Exercise 5: Logistic models\n\nIn this exercise you will model the yield of pasture over time (since\nsowing).\n\nNote, we will assume the yield at sowing = 0 is 0 t/ha, which \nallows us to use the equation above (that is pre-defined in `SSlogis()`).\n\nIf this were not the case, we would need to use a slightly different equation;\n$y = y_0 + \\frac{Asym}{1+e^{-scal(x-xmid)}}$, where $y_0$ is the yield at sowing.\n\na)  Fit the model in R using the `nls` function. Estimate the starting  \n    parameters manually (using the previous exercise and the equation above as\n    a guide). Then use the `SSlogis()` function to automatically estimate \n    the parameters (see L12, Tut12 or the [handbook](https://envx-resources.github.io/ENVX1002-handbook/module03/043-nonlinear.html)).\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**a)** Use the following code to read in the code and run the model:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data input\nweek <- c(9,14,21,28,42,57,63,70,79)\nyield <- c(8.93,10.80,18.59,22.33,39.35,56.11,61.73,64.62,67.08)\n\n# Estimating parameters\nlog.mod <- c(Asym=70.0, xmid=40.0, scal=1.0)\nyld.log <- nls(yield ~ Asym/(1 + exp((xmid - week)/scal)), start = log.mod, trace=T)\n\n# Self-starting function\nyld.log <- nls(yield ~ SSlogis(week, Asym, xmid, scal))\n```\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n:::\n\nb)  Plot the fitted model with the observations.\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**b)** Use the following code:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates new dataset for predictions (5 to 40 at an interval of 1)\nweek.new <- seq(0,80,1)\n\n#Makes predictions onto temp.new\npred.log <- predict(yld.log, list(week = week.new))\n\n#Adds model fit to existing plots\nplot(week, yield,\n     xlab='Time (weeks)', ylab='Yield (kg)')\nlines(week.new, pred.log)\n```\n\n```{.r .cell-code}\n## In ggplot\n# ggplot(data = data.frame(week, yield), aes(x = week, y = yield)) +\n#   geom_point() +\n#   geom_line(y = predict(yld.log, week.new), color = \"red\")\n```\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n:::\n\nc)  Compare the final model parameters when we provide starting estimates, and when we use `SSlogis()`. Are they the same?\n\n::: {.content-visible when-profile=\"solution\"}\n### Solution {style=\"color:green;\"}\n\n------------------------------------------------------------------------\n\n**c)** From the code below we can see the estimated parameters are 72.46\nfor $Asym$, 38.87 for $xmid$ and 14.85 for $scal$. This should be similar to\nwhat Solver obtains in Excel.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Examine model\nsummary(yld.log)\n```\n:::\n\n\n\n\n\n\n------------------------------------------------------------------------\n:::\n\n::: {.pad-box-mini style=\"background-color: #f2f2f2; padding:10px;\"}\n**STARTING VALUES**\n\nWhen fitting non-linear functions (i.e. logistic or exponential) using\niterative procedures such as `nls` or SOLVER the starting estimates of\nthe parameters need to be approximated. If the values are too far, the\nmodel will not run and return an error.\n\nThe best way to ensure that you have suitable starting values is to plot\nthe data with the predictions overlaid for your starting parameters. You\ncan then see how close your initial model is to the data.\n\nThe reason we go through the process of estimating parameters is because `nls` \nand SOLVER can fit *any* nonlinear equation. They are more versatile\n\nRealistically, most nonlinear relationships you would fit are covered by a\nself-starting function; i.e. `SSlogis()`, `SSasymp()`, or `nlraa::SSexpf()`.\nThese estimate the parameters for you and are more efficient to run.\nWe recommend using these when suitable.\n\n:::\n\n<br>\n\nThat's it for Module 3! Great work exploring non-linear models!\n\nThank you all (students and demonstrators!) for your hard work and\nenthusiasm throughout this Module. Good luck with Project 3 and the\nfinal exam!\n\n",
    "supporting": [
      "Lab12_files/figure-typst"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}