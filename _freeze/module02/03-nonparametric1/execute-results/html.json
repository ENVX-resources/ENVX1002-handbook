{
  "hash": "bd4e5ef6a62289c860d13eeb8386bed5",
  "result": {
    "engine": "knitr",
    "markdown": "# Chi-squared tests\n\n\n\n\n\n\n\n\n\nThe chi-squared distribution (where chi is pronounced ‘ky’) is a very widely used distribution in statistics. Its symbol is 2. It has MANY applications. Here we will consider only two of these applications – tests of agreement with expected outcomes, and contingency tables.\n\n7.1\tNotes on the 2 Distribution\n\nThe density function for a 2 distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the 2 density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g. 1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the 2 distribution approaches the shape (and properties) of a normal distribution.\n\n \nFigure 7.1 The shape of the 2 density function for various degrees of freedom.\n\nThe mean and variance of a 2 variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as  (Greek n), then\n\n\tMean of   variable = \t(i.e. mean = df)\n\tVariance of   variable = 2\t(i.e. variance = twice the df)\n\nCritical values of a 2 distribution are given in the 2 probability table that appears as Appendix A.5.\n\n \n7.2\tTesting Agreement of Frequency Data with Expectation Models\n\n\n7.2.1\tSteps in Chi-Squared Tests of Agreement\n\nThe process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\n1.\tChoose an appropriate hypothesis test for the type of data you have, and the type of question you’re asking.\n\n2.\tChoose the level of significance for the test.\n\n3.\tWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\n\n4.\tCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\n\n5.\tCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic – however there are alternative tests designed to accommodate these situations.\n\n6.\tCalculate the test statistic and degrees of freedom.\n\n7.\tObtain the P-value.\n\n8.\tDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n\n\n\n7.2.2\t Examples: Testing Whether Outcomes are Equally Probable\n\nEXAMPLE 1\n\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a “fair” coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a “fair” coin.\n\n \nEXAMPLE 2\n(from Mead et al, 2003)\n\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\n\nCheese\tFirst preference\nA\t5\nB\t7\nC\t18\nD\t10\nTotal\t40\n\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n\n\t =  =9.80.\n\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the   distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the   value of 9.80 is greater than the   value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since P < 0.05.)\n\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with \n\n\t =  =… \t\tetc.\n\n\n7.2.3 Example: Testing Whether Outcomes are in Expected Proportions\n(from Mead et al, 2003)\n\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen). \n\nThe figures obtained for the primula plants follow. \n\tNormal eye\tPrimrose Queen eye\tTotal\nFlat leaves\t328\t122\t450\nCrimped leaves\t77\t33\t110\nTotal\t405\t155\t560\n\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, ¾ of the total 560 observation in the first class of the characteristic and the remaining ¼ in the second class. Further, this model predicts that ¾ of the flat-leaved plants should have normal eyes, resulting in ¾ × ¾ of all the plants or 9/16 with flat leaves and normal eyes; the remaining ¼ of the flat-leaved plants, which is ¼ × ¾ or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes. \n\nThe calculation of these expected or predicted proportions is shown below. \n\n\tNormal eye\tPrimrose Queen eye\nFlat leaves\t¾ × ¾ = 9/16\t¼ × ¾ = 3/16\nCrimped leaves\t¾ × ¼ = 3/16\t¼ × ¼ = 1/16\n\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35. \n\nThe observed and expected frequencies are summarized in the table below.\n\n\tNormal eye\tPrimrose Queen eye\nFlat leaves\t328  (315)\t122  (105)\nCrimped leaves\t77  (105)\t33  (35)\n\n\n\t \t=  \n\t\t= 0.54 + 2.75 + 7.47 + 0.11 = 10.77. \n\n\nWe compare 10.77 with the 5% point of the   distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable. \n\nSee pp. 332-333 of Mead et al, 2003 for what to do next… after rejecting the model.\n\n\n\n7.3\tContingency Tables\n\n7.3.1\tExample: (2 x 2) Contingency Table\n\nConsider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\n\n\tRecovered\t\n\tYes\tNo\tTotal\nProcedure 1\t14\t6\t20\nProcedure 2\t8\t12\t20\nTotal\t22\t18\t40\n\nThis is one form of a 22 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\n\nSolution: \nWe will perform a statistical hypothesis test:\n\nH0: There is no difference in the true recovery rates for animals on either procedure\nH1: The recovery rates do differ.\n\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\n\n\tH0: p1 = p2\n\tH1: p1 ≠ p2\n\nEstimates of individual recovery rates are   = 14/20 = 0.7 and   = 8/20 = 0.4. Is this difference due to chance?\n\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n\n\t  =  .\n\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be 20 22/40 = 20 0.55 = 11 animals. In general this can be written as:\n\n\n \n\n\nSo the expected frequencies for the cells in the table are:\n\n \t \n \t \n \t \n \t \n \nExpected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\n\n\tRecovered\t\n\tYes\tNo\tTotal\nProcedure 1\t14 (11)\t6 (9)\t20\nProcedure 2\t8 (11)\t12 (9)\t20\nTotal\t22\t18\t40\n\nThe table shows observed (and expected) frequencies. The 2 test statistic is then calculated using:\n\n\t \n\nLarge values of 2 indicate discrepancies between observed and expected frequencies, i.e. large values indicate that H0 should be rejected in favour of H1.\n\nThe df of this 2 test is 1 for a 22 contingency table. In general, \n\n\n\n\nIf H0 is true, the observed 2 is just one observation from a 2 distribution with 1 df:\n\n \n\nSince  , there is (just) not sufficient evidence to reject H0. \nThus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n\n \n7.3.2\tExample: (4 x 3) Contingency Table\n\nThe second example is a 43 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n\n\t\tDisease Status\t\t\nVaccine\tNo\tMild\tSevere\tTotal\nControl\t100  (137.3)\t71  (42.6)\t29  (20.1)\t200\nA\t146  (133.9)\t32  (41.6)\t17  (19.6)\t195\nB\t149  (132.5)\t28  (41.2)\t16  (19.3)\t193\nC\t146  (137.3)\t37  (42.6)\t17  (20.1)\t200\nTotal\t541\t168\t79\t788\n\nThe table shows observed (and expected) frequencies.\n\nWe test H0 that there is no association between disease status and vaccination given, i.e. all vaccinations have equal effectiveness.\n\nAssuming H0 is true, the expected frequencies are calculated as follows:\n\ne.g. Expected frequency for an animal in the Control, No disease group:\n\n .\n\nAs before, the test statistic is   and this will have \n(4-1)(3-1) = 6 df.\n\nThis is how to do the test in the Stats menu in GenStat:\n\n\tStats > Statistical Tests > Contingency Tables…\n\nGenStat requires the data to be set up as a 43 Table type of spreadsheet (as opposed to a Variate or Matrix). It then reports the X2 test statistic and P-Value by selecting the Pearson method.\n\nPearson chi-squared value is 45.22 with 6 df.\n \nProbability level (under null hypothesis) p < 0.001\n\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\n\nLikelihood chi-squared value is 43.34 with 6 df.\n \nProbability level (under null hypothesis) p < 0.001\n \nThe ML 2 is calculated as follows:\n\n \n\nThe degrees of freedom are (r – 1)(c – 1) as before, where r and c are the numbers of rows and columns respectively.\n\nIn general, the 2 approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\n\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\n\n\tDisease Status\t\nVaccine\tNo\tMild\tSevere\tTotal\nControl\t10\t7\t3\t20\nA\t15\t3\t2\t20\nB\t15\t3\t2\t20\nC\t15\t4\t2\t21\nTotal\t55\t17\t9\t81\n\n\n \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}