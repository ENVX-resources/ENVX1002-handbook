---
title: "Lab 11 - Multiple Linear Regression"
embed-resources: true
---


```{r setup}
#| include: false
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, readxl, lattice, viridis)
knitr::opts_chunk$set(echo = TRUE) # change to TRUE for solutions, efficiency
#### STILL NEED to set eval=TRUE for solutions
```

::: callout-tip
## Learning outcomes

-   Learn to perform MLR and interpret the results using R;
-   Undertake hypothesis testing to determine if model is significant
-   Undertake hypothesis testing to determine if the true partial regression slope $\neq$ 0
-   Check assumptions are filled prior to assessing model output
-   Assess model summary in terms of fit and P-values
-   Consider more parsimonious models
:::

**DATA: ENVX1002_wk11_practical_data_Regression.xlsx**

Last week you explored simple linear regression and assessed the output of your models.

This week we will build upon this and venture into multiple linear regression.

Before you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.

Don't forget to save as you go!

## Exercise 1: Corn yields

Data: *Corn* spreadsheet

In this data

-   y = P content of corn\
-   x~1~ = inorganic P content of soil\
-   x~2~ = organic P content of soil\
-   n = 17 sites (The original data had 18 sites, one is removed here.)

Aim of our investigation: Understand the relationship between Organic and Inorganic phosphorous contents in the soil, and the phosphorous content of corn. This will allow us to see which type of phosphorous is being taken up by the corn.

```{r LoadData, echo=T}
library(readxl)
Corn <- read_xlsx("data/ENVX1002_practical_wk11_data_Regression.xlsx", "Corn")
head(Corn)
```

<br>

### 1.1 Examine the correlations

Some people find it difficult to visually interpret graphical summaries of data in more than 2 dimensions; however, 3-dimensional surface plots are reasonably common in statistics although not usually in descriptive statistics.

Initially we will examine the pairwise correlations to "get a feel" for the data. We will then make a 3-dimensional surface plot using the `lattice` package .

Using R, we can calculate the correlation matrix quite easily.

Note the use of `round()` to limit the number of significant digits.

```{r, echo=T}
round(cor(Corn),3)
```

a)  What do the results of the correlation matrix tell you?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** These results tell you that the correlation between CornP and InorgP the highest (r = 0.720) The correlation between CornP and OrgP (r = 0.212) or between OrgP and InorgP (r = 0.399) is much weaker.

---
:::

b)  Based on the correlation matrix, if we were to fit a single predictor model involving EITHER InorgP OR OrgP, then which model would be more successful?

|           (Hint, the *r*^2^ is exactly that for a single predictor regression, the square of the correlation, *r*).

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** The more successful model would be $CornP = \beta_0 + \beta_1 InorgP$ as the correlation between CornP and InorgP is much higher than the relationship than CornP and OrgP.

---
:::

The pairs plot creates scatterplots between each possible pair of variables. Like a single scatterplot the pairs plot allows us to visually observe any trends.

c)  Observing the pairs plot below, do you see any strong trends? How well does this link to your correlation matrix?

```{r, echo=T}
pairs(Corn)
```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**c)** We read the pairs plot like a correlation matrix. We can observe a slight trend between CornP and InorgP. The plots of CornP and OrgP, and that of OrgP and InorgP show much weaker trends and a more even scatter of points.

---
:::

<br>

#### Simple 3-D plot

Unlike a simple plot we can creat for a simple linear regression, it is a bit more complex to visualise a model with more predictors. One way we can visualise the relationship is with a 3-D plot, which can be made using the function `levelplot()` in lattice.

Here we plot the OrgP and InorgP in the axes and the levels in the plot are CornP.

Note the package [Viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) has been called, this is through personal choice.

The Viridis package has a range of assembled colour ramps which are easier for the reader to differentiate the colours, especially when printed in grayscale, or if the reader is colourblind.

```{r, echo=T, fig.height = 3}
library(lattice, quiet = T)
library(viridis, quiet = T) # will need to install.packages("viridis")   

levelplot(CornP ~ InorgP + OrgP, data = Corn
          , col.regions = viridis(100))
```

The level plot shows us the x (InorgP) and y variable (OrgP), with the colour scale representing the z variable, which in this case is Phosphorous being taken up by the corn (cornP). From the plot we can see that with higher levels of orgP and inorgP in the soil, the Phosphorous content in the corn is generally higher.

It is clear that the 3-D surface plot does not have colours everywhere, but this relates of course to the underlying data. In this case we don't have continuous data in both directions, so the response (the colour) is only plotted where we have input variables.

If we did have continuous data in both directions the plot would look more like a heatmap, [here are some examples](https://bookdown.org/content/b298e479-b1ab-49fa-b83d-a57c2b034d49/correlation.html#using-the-levelplot-function-of-lattice).

<br>

### 1.2 Fit the model

We will now use regression to estimate the joint effects of both inorganic phosphorus and organic phosphorus on the phosphorus content of corn.

$CornP = \beta_0 + \beta_1 InorgP + \beta_2 OrgP + error$

This is fairly simple and follows the same structure as simple linear regression and uses `lm()`.

```{r MLR, echo=T}
MLR.Corn <- lm(CornP ~ InorgP + OrgP, data=Corn)
```

<br>

### 1.3 Check assumptions

Let's check the assumptions of regression are met via residual diagnostics.

a)  Are there any apparent problems with normality of CornP residuals or equality of variance for this small data set?

```{r MLR_resid, echo=T, fig.height=6}
par(mfrow=c(2,2))
plot(MLR.Corn)
```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** No there is not as the distribution is approximately normal and the variance is constant. In the residuals vs Leverage plot there is a point sitting close to the top right corner, but this is most likely because our dataset is small.

Interpretation of the assumptions is quite subjective and often comes down to time and experience. If, for example in your Project 3 it is not obvious whether the assumptions have been met, make your decision and provide justification so that the viewer is able to come to the same conclusion as you. e.g. if you have a smaller dataset as you see in this exercise, it may be harder to tell whether assumptions have been met.

---
:::

<br>

### 1.4 Model output

After checking our assumptions and we are happy with them, we can interpret our model output.

a)  Incorporating the partial regression coefficient estimates, what is the model equation?

```{r MLR_summary, echo=T}
summary(MLR.Corn)
```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** CornP = 66.4654 + 1.2902 * InorgP - 0.1110 * OrgP

---
:::

<br>

Simple linear regression allowed us to describe the relationship incorporating our regression coefficient estimate. We would interpret it as follows:

*"As* $x$ increases by 1, $y$ decreases by $b_1$ units" (depending on the direction of the relationship).

This week it is a bit different because we are dealing with **partial** regression coefficients instead.

Instead, we would say:

*"as* $x_1$ increases by 1, $y$ decreases by $b_1$ units given all other partial regression coefficients are held constant".

Applied to our model, if we wanted to describe the relationship between InorgP and CornP, we would say:

*"As InorgP increases by 1, CornP increases by 1.2902, given OrgP is held constant."*

b)  Given the above, how would you interpret the relationship between OrgP and CornP?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** As OrgP increases by 1, CornP decreases by 0.1110, given InorgP is held constant.

---
:::

<br>

### 1.5 Is the model useful?

Remember now that the F-test and T-test are testing slightly different things.

<br>

#### F-test

$H_0:$ all $\beta_k = 0$, i.e. $\beta_1 = \beta_2 = 0$

$H_1:$ at least 1 $\beta_k \neq 0$, i.e. our model is significant

We will find the P-value for this test at the end of the summary output.

<br>

#### t-test

$H_0: \beta_k = 0$

$H_1: \beta_k \neq 0$

Where $\beta_k$ refers to one of the model partial regression coefficients. In the case of our model we only have 2: $b_1$ (InorgP) and $b_2$ (OrgP).

<br>

Now it is your turn:

a)  Looking at the `summary()` output, is our overall model significant?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** Our model is significant as the P-value derived from the F-test statistic is less than 0.05 (P = 0.005). As P < 0.05, we reject our null hypothesis and can say at least one of our predictors are significant. 

---
:::
b) Which independent variable is a significant predictor of corn yield?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** InorgP (P = 0.002)

---
:::


<br>

### 1.6 How good is the model?

a) How much of the variation in CornP content is explained by the two independent variables?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** This question refers to the r-squared adjusted. As the adjusted r-squared = 0.4575, we can say that the model (containing our two independent variables) explains 45.8% of variation in CornP. 

---
:::

b) Run the model again but this time with *only* the significant independent variable. How do the model performance criteria (r^2^-adj, P-values, Residual Standard Error) change?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** By removing OrgP (deemed non-significant as P = 0.662) and only retaining InorgP (P = 0.002), we can see that the r-squared adjusted has increased to 0.4864, indicating an improvement in model fit. 

---
:::

<br>

### 1.7 Our conclusions

Writing up conclusions for multiple linear regression are similar to simple linear regression, just with a couple of extra P-values to state. 

We would first mention that our overall model is significant as we rejected the null hypothesis (P = 0.05). We could then describe the hypothesis test results for our predictor variables. Finally, we would describe the model fit and our adjusted-r^2^. 

Remember the scientific conclusion then relates our findings back to the context, answering aims. 

a) What would our statistical conclusion be?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** Statistical conclusion:

The F-test indicated our model is significant (P \< 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0.

Our t-test upon each of our partial regression coefficients supports this, as the P-value for InorgP was 0.002, much smaller than 0.05 and we can therefore reject the null hypothesis that this coefficient is equal to 0. In contrast, the P-value for OrgP was greater than 0.05 (P = 0.662), and so we fail to reject the null hypothesis, concluding OrgP is not a significant predictor in this model.

The fit of this model is moderate, with a residual standard error of 12.25 and an r^2^-adj = 0.4575.

Furthermore this fit could be improved by the removal of the OrgP predictor within the model (r^2^-adj = 0.4864).

---
:::

b)  What would our Scientific conclusion be?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** Scientific conclusion:

Inorganic phosphorous is a significant predictor of phosphorous content in corn (P \< 0.05), whereas organic phosphorous is not (P \> 0.05). The model accounts for 45.8% of variation in the phosphorous content of corn.

---
:::

<br>

## Exercise 2: Water quality

Data: *Microbes* spreadsheet

This exercise will use data from the [NOAA Fisheries data portal](https://www.webapps.nwfsc.noaa.gov/apex/parrdata/inventory/datasets). The dataset contains the results of microbial study of Pudget Sound, an estuary in Seattle, U.S.A.

The dataset contains the following variables:

-   total_bacteria = Total bacteria (cells per ml) --\> this will be our response variable

-   water_temp = Water temperature (°C)

-   carbon_L_h = microbial production (µg carbon per L per hour)

-   NO3 = Nitrate (µm)

First thing to do, is read in the data. This time we will be using the *Microbes* spreadsheet:

```{r, echo=T}
mic <- read_xlsx("data/ENVX1002_practical_wk11_data_Regression.xlsx", "Microbes")

# Check the structure
str(mic)

```

<br>

### 2.1 Examine the correlations

For this dataset we may expect to see some correlations;

-   Warmer water temperature we would expect to see a higher amount of bacterial growth

-   Carbon is a proxy for microbial production, so if we see a higher rate of carbon production, we would expect to see higher levels of bacteria

-   NO3 (Nitrate) is an essential nutrient for plants and some bacteria species metabolise this

a)  Let's test this. Observe the correlation matrix and pairs plots. Do you notice any strong correlations?

```{r, echo=T}
cor(mic)

pairs(mic)
```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**a)** The strongest correlation with total_bacteria is with NO3 (r = -0.759), followed by Carbon (r = 0.663) and water_temp (r = 0.645). 

Observing the pairs plot, there is something weird happening with NO3, so there may be another kind of relationship occurring here. 

There are also some correlations between independent variables, but they are not strong enough to exclude from the model. 

---
:::

<br>

### 2.2 Fit the model

We can now fit the model to see how much these predictors account for the variation in total bacteria.

$total bacteria = \beta_0 + \beta_1 watertemp + \beta_2 carbon + \beta_3 NO3 + error$

There are two forms the lm code can take; you can either specify which variables you want to include by naming each one, or if only your desired variables are within your dataset, you can use the \~. to specify all columns.

```{r, echo=T}
names(mic) # tells us column names within the dataset

# Form 1:
mic.lm <- lm(total_bacteria ~ water_temp + carbon_L_h + NO3, data = mic)

# Form 2
mic.lm <- lm(total_bacteria ~ ., data = mic)

```

<br>

### 2.3 Check assumptions

Let’s check the assumptions of regression are met via residual diagnostics.

a)  Are there any apparent problems with normality of total_bacteria residuals or equality of variance for this data set?

```{r, echo=T}
par(mfrow=c(2,2))
plot(mic.lm)
```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** Residuals vs fitted looks ok, as does normality. 

There seems to be some slight clustering but most likely due to the data being obtained from two sampling periods (both within the same season), or potentially from different points. Water quality and microbial activity is highly variable, so this is not entirely unexpected. Nevertheless we can move on with this in mind. 

Note this question did not mention the Residuals vs Leverage plot. There is one value in the top right corner outside the first threshold. This indicates there may be a potential outlier within the predictors influencing the regression model, or maybe the current model is not the best for this data. 

You are welcome to investigate this further, but for the sake of our example, we will move onto interpretation of the output. 

---
:::

<br>

### 2.4 Model output

After investigating the assumptions, they seem to be ok, so we can move onto the model summary.

```{r, echo=T}
summary(mic.lm)
```

a)  Incorporating the partial regression coefficient estimates, what is the equation for this model?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** total_bacteria = 766294 + 40051 * water_temp + 84425 * carbon_L_h - 16586 * NO3  

---
:::

b)  Like you did in Exercise 1.4, how would you interpret the relationship between total_bacteria and water_temp?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** As water_temp increases by 1, total_bacteria increases by 40051, given all other partial regression coefficients remain constant.   

---
:::

<br>

### 2.5 Is the model useful?

a)  Observing the P-value of the F-statistic in the summary, can we say our model is significant?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}


---

**a)** The P-value for the F-statistic is 1.318e-10, which is smaller than 0.05. We therefore reject the null hypothesis and can say our model is indeed significant as at least one of our predictors are significant. 

---
:::

b)  Are any predictors significant?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** Yes; as the P-value for N03 is 0.00268 (P < 0.05), we can reject the null hypothesis that the true partial regression slope for this variable is not zero.  

Water_temp and carbon_L_h are not significant predictors (P > 0.05), i.e. as we cannot reject the null hypothesis that the true partial regression slope of each is non-zero.

---
:::

<br>

### 2.6 How good is the model?

a)  How much of the variation in total bacteria is explained by the three independent variables?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** The model is not the best; the Residual Standard error is huge, even in terms of the response variable, and the r^2^-adjusted is only 0.5913. 

The model accounts for 59.1% of variation in total bacteria counts. 

---
:::

b)  Run the model again but this time excluding the variable with the largest P-value. How do the model performance criteria (r^2^-adj, P-values, Residual Standard Error) change?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** The least significant predictor is carbon_L_h (P = 0.21624).

After removing carbon_L_h, the r^2^-adj has decreased (r^2^-adj = 0.5868) and Residual Standard Error has increased (251900). In terms of p-values, that of water_temp has decreased, but is still non-significant. The P-value for NO3 has decreased substantially.

From this, we can see the model has not been improved by eliminating the predictor.
  
---
:::

```{r, eval=T}
summary(lm(total_bacteria ~ water_temp + NO3, data = mic))

```

<br>

### 2.7 Conclusions

a)  What would the statistical conclusion be for this model?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** Statistical conclusion:

  The F-test indicated our model is significant (P < 0.05), allowing us to reject the null hypothesis and conclude at least one of our partial regression coefficients has a slope not equal to 0. 

Our t-test upon each of our partial regression coefficients supports this, as the P-value for NO3 was 0.003, smaller than 0.05 and we can therefore reject the null hypothesis that this coefficient is equal to 0. In contrast, the P-value for water_temp and carbon_L_h were greater than 0.05 (P = 0.095 and 0.216, respectively), and so we fail to reject the null hypothesis, concluding water_temp and carbon_L_h were not significant predictors within this model.

The fit of this model is moderate, with a residual standard error of 250500 and an r^2^ = 0.5913. 

Note: remember residual standard error is on the same scale as the response variable, so although it is a huge number, it is not as large when we think of the scale.  

Furthermore this fit was not substantially improved by the removal of the water_temp predictor within the model (r^2^-adj = 0.587).

---
:::

b)  What would our scientific conclusion be?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**b)** Scientific conclusion: 
  
  Nitrate is a significant predictor of total bacteria count in (P < 0.05), whereas water temperature and Carbon production rate were not is not in this case (P > 0.05). The model accounts for 59.1% of variation in the bacteria counts measured in water samples collected from Puget Sound. 

---
:::

<br>

## Exercise 3: Dippers

Data: *Dippers* spreadsheet

We will revisit the Dippers dataset from last week, but now incorporating other factors which may be influencing the distribution.

The file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.

Dippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.

Twenty-two sites were included in the survey. Variables are as follows

-   water hardness

-   river-bed slope

-   the numbers of caddis fly larvae

-   the numbers of stonefly larvae

-   the number of breeding pairs of dippers per 10 km of river

In the analyses, the four invertebrate variables were transformed using a Log(Number+1) transformation.

<br>

Now it is your turn to work through the steps as above. What other factors are influencing the number of breeding pairs of Dippers?

a)  Read in the data from today's Excel sheet, the corresponding sheet name is "Dippers"

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

**a)** Use the following code:

```{r, eval=T}
library(readxl, quietly = TRUE)
Dippers <- read_xlsx("data/ENVX1002_practical_wk11_data_Regression.xlsx"
                     ,"Dippers")
```

---
:::

b)  Investigate a correlation matrix and pairs plot of the dataset, are there signs of a relationship between breeding pair density and other independent variables?

```{r, eval=T}
pairs(Dippers)

cor(Dippers)
```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**b)** Looking at the pairs scatterplots there seems to be a positive relationship between Br_Dens and RiverSlope, LogCadd, and with LogStone. We can follow this up using correlation, which indicates there is a moderate positive relationship of 0.710 between Br_Dens and RiverSlope, 0.763 Br_Dens and LogStone, and 0.613 between Br_Dens and LogCadd.  

---
:::

c)  Let's investigate further. Run the model incorporating all of our predictors, but before looking at our model output, are the assumptions ok?

```{r , eval=T}
# Run model
dipper.lm <- lm(Br_Dens ~ ., data=Dippers)

# Check assumptions
par(mfrow = c(2,2))
plot(dipper.lm)

```

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**c)** Assumptions look ok:
  
  * Residuals vs fitted: Points evenly scattered around mean.

  * Normal Q-Q: Points follow line.

  * Scale-Location: Points evenly scattered, no fanning. 

  * Residuals vs Leverage: No points occurring at top right or bottom right corners outside the dotted red lines. A couple of points are close, but not outside, so all is good. 

We can continue to interpreting the output!

---
:::

<br>

Once you are happy assumptions are good, you can interpret the model output using `summary()`.

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

```{r , eval=T}
summary(dipper.lm)

```

---
:::

d)  What is the equation for our model, incorporating our partial regression coefficients?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**d)** Br_Dens = -1.530702 + 0.006916 * Hardness + 0.067924 * RiverSlope + 0.317611 * LogCadd + 0.752987 * LogStone

---
:::

e)  Based on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd this time?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**e)** The F-test statistic tells us whether our overall model is significant, which in this case it is (P = 1.971e-05). This time, unlike when we only used LogCadd as the predictor, our P-value from the F-Statistic is now different to the P-value of LogCadd (P = 0.16674).  

As our P - value for the model is < 0.05, we can reject the null hypothesis and conclude our model is significant and at least one of our partial regression slopes are non-zero. 

---
:::

f)  Is LogCadd still a significant predictor of Dipper breeding pair density?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}
  
**f)** Unlike last week, the P-value for LogCadd is now greater than 0.05 (P = 0.16674), and so we fail to reject the null hypothesis that the partial regression slope is equal to zero.  We can therefore say LogCadd not a significant predictor of Dipper breeding pair density. 

---
:::

g)  What are the significant predictors of this model?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**g)** The only significant predictor for this model is LogStone (P < 0.05). 

---
:::

h)  How good is the fit of our model?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---

  
**h)** Our model fit is much better compared to the simple linear model from last week. 

Our residual standard is (Residual SE = 0.7753) and the r^2^-adjusted is closer to 1 than 0 (r^2^-adj = 0.7281). 

Our residual SE is a low value, and relative to the range of Br_Dens (min = 2.9, max = 8.4), which is pretty good and also much better than last week. 

---
:::

i)  What might we do to improve the model fit?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**i)** Consider elimination of a non-significant predictor variable. 

---
:::

j)  What statistical and scientific conclusions can we make from this model output?

::: {.content-visible when-profile="solution"}
### Solution {style="color:green;"}

---
  
**j) ** 
Statistical:

As the F-test statistic is less than 0.05, we can reject the null hypothesis that all partial regression coefficients are equal to zero. Furthermore, LogStone was the only significant predictor (P < 0.05) and therefore we can only reject the null for this predictor.   

The model fit is good, with an r^2^-adjusted value of 0.7281 and Residual standard error of 0.7753.  

Scientific:
  
The number of Stonefly Larvae is a significant predictor of Dipper breeding pair density (P < 0.05), whereas other predictors used, such as water hardness, river slope and the number of Caddis Fly larvae are not significant. The overall model accounts for 72.8% of variation in Dipper breeding pair density.  

The significance of Stonefly larvae over Caddis fly larvae, and the positive relationship in the partial regression coefficient, suggests that Dippers may have a preference for Stonefly Larvae over Caddis Fly larvae.  

---
:::

<br>

Another week of linear models done! Great work fitting Multiple Linear Regression! Next week we break away and explore non-linear functions.

<br> <br> <!--The code below reduces white space at the end of the output doc-->

::: {.tocify-extend-page data-unique="tocify-extend-page" style="height: 0;"}
:::
