---
execute:
  warning: false
  message: false
---

```{r setup}
#| include: false
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse)

library(patchwork)
library(ggplot2)

```

# Non-linear regression

Linear relationships are simple to interpret since the rate of change is constant -- i.e. as $x$ changes, $y$ changes at a constant rate. For non-linear relationships -- as $x$ changes, $y$ changes at an unproportional rate. To simplify interpretation and enable the use of linear models, it is often recommended to transform non-linear data to make it approximately linear.

Transformation is usually possible for monotonic relationships (i.e. relationships that are always increasing or decreasing) such as exponential growth curves. The most common transformations are:

| Name  | Transformation   |
|-------|:----------------:|
| Inverse | $y = \frac{1}{x}$ |
| Root | $y = \sqrt[a]{x}$ |
| Exponential | $y = e^x$ |
| Logarithmic | $y = \log_a(x)$ |

However, in the case of non-monotonic relationships (e.g. polynomials, asymptotic, logistic) transformation may not be enough to make the data meet the assumptions. In this section, we look at fitting non-linear models.

$$ Y_i = f(x_i, \beta) + \epsilon_i $$
where $f(x_i, \beta)$ is a nonlinear function of the parameters $\beta$.

The assumptions for non-linear regression are INE (Independence, Normality, Equal variance). 

## Polynomial regression

A polynomial equation is an extension to linear regression and can still be fitted using least squares. Typically, a polynomial equation has multiple terms of the same predictor (i.e. one $x$). The more terms in the polynomial, the more complex the model and the less likely it follows an actual biological relationship. A complex model may 'fit' the data well, but fail to represent the true relationship between the variables (overfitting).

$$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + ... + \beta_k x_i^k + \epsilon_i $$
where $k$ is the degree of the polynomial.

- Linear: $y = \beta_0 + \beta_1 x$
- Quadratic: $y = \beta_0 + \beta_1 x + \beta_2 x^2$
- Cubic: $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$
- Each level increases the power of the predictor by 1.

### In practice

To fit a polynomial, we still use the `lm()` function. To create the polynomial terms, we use the `poly(x, n)` function, where `x` is the predictor and `n` is the degree of the polynomial.

Below is an example with asymptotic data -- it increases rapidly to a certain point and then levels off. The linear model (blue) does not capture the complexity of the relationship. The polynomial model with 10 terms (green) fits the data well, but it is too complex between $x$ = 5-10. In this case, the polynomial model with 2 terms (red), aka the quadratic model, fits the data the best.

```{r}
#| code-fold: true
# Generate some data with an asymptotic relationship
set.seed(442) # set seed

asymptotic = tibble(
  predictor = seq(0, 10, by = 0.2),
  response = 100*(1-exp(-0.5*predictor)) + rnorm(length(predictor), mean = 0, sd = 10))
```

```{r}
#| echo: true
mod_lin <- lm(response ~ predictor, asymptotic) # blue
mod_poly2 <- lm(response ~ poly(predictor, 2), asymptotic) # red
mod_poly10 <- lm(response ~ poly(predictor, 10), asymptotic) #green
```

```{r}
#| code-fold: true
ggplot(asymptotic, aes(x = predictor, y = response)) +
  geom_point(size = 2) +
  labs(x = "Predictor", y = "Response") +
  geom_line(aes(y = predict(mod_lin)), color = "slateblue", size = 1.5, linetype = 2) +
  geom_line(aes(y = predict(mod_poly2)), color = "brown", size = 1, linetype = 1) +
  geom_line(aes(y = predict(mod_poly10)), color = "seagreen", size = 1.5, linetype = 2) +
  theme_classic()
```

This is also evident when comparing the `summary()` of the models. The linear model explains the least amount of variation in $y$ (`Multiple R-squared` = 57.01%). The 2-term polynomial model explains 81.95% of variation (`Adjusted R-squared`), whereas the 10-term polynomial model explains 86.21% of variation (`Adjusted R-squared`). The first three terms of the polynomial model are significant, the fourth is almost significant, but the remainder are not. Comparing the 2 and 10-term polynomial models, are the extra 8 terms worth an extra 4.26% of variation explained? Considering the 10-term polynomial is very complex and overfitted -- the principle of parsimony would say 'no'.

```{r}
summary(mod_lin)
summary(mod_poly2)
summary(mod_poly10)
```

The hypothesis of a polynomial model is similar to that of multiple linear regression. A significance test is done for each individual coefficient, and the overall model hypothesis is below:

$$H_0: \beta_1 = \beta_2 = ... = \beta_k = 0$$
$$H_0: \beta_1 = \beta_2 = ... = \beta_k \neq 0$$
In the case that all coefficients are zero, the model is better represented by the mean of the data.

As for interpretation of the quadratic model, we can write the equation, but the coefficients do not have much meaning.

$$\text{response} = 79.82 + 159.37 \cdot \text{predictor} + -106.94 \cdot \text{predictor}^2$$
Given it is a quadratic equation, we could calculate the peak of the curve ($x_{peak} = -\frac{b}{2a}$ and substitute to get $y_{peak} = c - \frac{b^2}{4a}$). For a polynomial of a higher degree, it would not be meaningful.

Although a little out of order, we can also check the assumptions. Recall that linearity is not an assumption for a non-linear model -- we can disregard the shape of the line for the *Residuals vs Fitted* plot, and focus on the distribution of the point around the line (which is even). The residuals fall on the line for the *Normal-QQ* plot indicating residuals are normally distributed, and the *Scale-Location* plot shows the residuals evenly distributed (equal varince), and there are no extreme outliers in the *Residuals vs Leverage* plot.  

```{r}
par(mfrow=c(2,2))
plot(mod_poly2)
```

# Non-linear models

To fit a non-linear model, we use the `nls()` function. While it is still a regression model that minimises the sum of squares, as there is no closed-form analytical solution, so `nls()` fits the model iteratively with numerical methods. Thus it **requires initial parameter guesses**, which can be obtained from domain knowledge, data exploration, or trial and error.

```{r}
#| eval: false
#| echo: true
nls(formula, data, start)
```

- `formula`: a formula object, with the response variable on the left of a ~ operator, and the predictor variable(s) on the right.
- `data`: a data frame containing the variables in the model.
- `start`: a named list of starting values for the parameters in the model.

The `nls()` function can be used to fit most non-linear models including but not limited to exponential, polynomial, asymptotic, and logistic models.

## Horizontal asymptotic regression

An asymptotic relationship is a type of non-linear relationship where the response variable approaches a limit as the predictor variable increases. This is common in growth curves, where growth is rapid at first and then slows down as it approaches a maximum (e.g. age vs height, population growth, diminishing returns). The equation for an asymptotic relationship is:

$$ y = a + b(1 - e^{-cx}) $$

- $a$ is value of $y$ when $x = 0$.
- $b$ is the upper limit: the maximum value of $y$.
- $c$ is the rate of change $y$ approaches the upper limit.

## In practice

With the `nls()` function, we first need to estimate our initial parameters for $a$, $b$ and $c$. We can do this by plotting the data and making an educated guess. For the asymptotic data, we can see that the lower limit ($a$) is around 0 and the upper limit ($b$) is around 100. The rate of change ($c$) is a little harder to estimate, so we will guess a value of 0.8. If the model returns an error (singular gradient matrix at initial parameter estimates), we can try different values -- the most likely culprit will be the rate of change ($c$).

```{r}
#| code-fold: true
ggplot(data = asymptotic, aes(x = predictor, y = response)) +
  geom_point() + 
  geom_hline(yintercept = 100, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  ## plot the rate
  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100), 
               arrow = arrow(length = unit(0.5, "cm")), 
               color = "red") +
  labs(x = "Predictor", y = "Response") +
  theme_classic()
```

```{r}
#| echo: true

# Fit the model
mod_asymptotic <- nls(response ~ a + b*(1-exp(-c*predictor)), data = asymptotic, 
  start = list(a = 0, b = 100, c = 0.8))
```

After we fit out model, we assess the assumptions plots and check our residuals for INE -- they look fine.

```{r}
#| echo: true

# Assumption plots
library(nlstools)
resids <- nlsResiduals(mod_asymptotic)
plot(resids)
```

Finally, we evaluate the model. There is not much to interpret - except the `Residual standard error`. We can compare models directly with this error term. R^2^ is not calculated for non-linear models.

The `Number of iterations to convergence` is the number of times the computer changed the parameters to try and get a better fit.

The resulting model equation would be $y = -14.52 + 113.04(1 - e^{-0.63x})$. 

```{r}
#| echo: true
summary(mod_asymptotic)
```

We can then plot the original data and the line of best fit.

```{r}
#| code-fold: true
ggplot(asymptotic, aes(predictor, response)) +
  geom_point(color = "black") +  # Original data points
  geom_line(y = predict(mod_asymptotic), color = "red", size = 1) +  # Fitted line
  theme_minimal() +
  labs(x = "Predictor", y = "Response") +
  theme_classic()
```

### Poor estimation

If the starting values are too far (most likely $c$ term), the model will not run, and there will be an error. However, there is some flexibility allowed. We use very inaccurate initial estimates - and the model still gets to the same result, it just takes more tries (iterations) to get there.

```{r}
#| echo: true

# Fit the model
mod_asymptotic <- nls(response ~ a + b*(1-exp(-c*predictor)), data = asymptotic, 
  start = list(a = -100, b = 200, c = 15))

summary(mod_asymptotic)
```

# Logistic regression

Logistic regression is a type of non-linear regression typically used when the response variable is binary (0 or 1). The logistic function is an S-shaped curve that models the probability of the response variable being 1. The equation for logistic regression is:

$$ y = c + \frac{d-c}{1+e^{-b(x-a)}} $$

- $c$ is the lower limit: the minimum value of $y$.
- $d$ is the upper limit: the maximum value of $y$.
- $a$ is the value of $x$ when $y$ is halfway between the lower and upper limits.
- $b$ is the rate of change: the rate at which $y$ approaches the upper limit.

## In practice

So if we were to estimate the initial parameters...

```{r}
#| echo: false
logistic <- tibble(predictor = seq(0, 10, by = 0.2), 
  response = 10 + abs(300 * (1 / (1 + exp(-0.8 * (predictor - 5)))) + rnorm(length(predictor), mean = 0, sd = 10)))

ggplot(data = logistic, aes(x = predictor, y = response)) +
  geom_point() +
  geom_smooth(se=F) +
  labs(x = "Predictor", y = "Response") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 300, linetype = "dashed") +
  geom_vline(xintercept = 5, linetype = "dashed") +
  # label the lines above
  annotate("text", x = 9, y = 0, label = "c", size = 8, vjust = -1) +
  annotate("text", x = 0, y = 300, label = "d", size = 8, vjust = 1.5) +
  annotate("text", x = 5, y = 100, label = "a", size = 8, hjust = -1) +
  ## plot the rate
  geom_segment(aes(x = 2.5, y = 60, xend = 6, yend = 250), 
               arrow = arrow(length = unit(0.5, "cm")), 
               color = "red") +
  # label the rate
  annotate("text", x = 4, y = 180, label = "b", size = 8, colour = "red", hjust = -1) +
  theme_classic()
```

```{r}
mod_logistic <- nls(
  response ~ c + (d - c) / (1 + exp(-b * (predictor - a))),
  data = logistic,
  start = list(c = 20, d = 300, b = 5, a = 5)
)

summary(mod_logistic)

```

As above, we can only interpret the `Residual standard error`. The resulting model equation would be $y = 16.20 + 293.25 / (1 + e^{-0.81(x - 5.06)})$.

```{r}
#| code-fold: true
ggplot(logistic, aes(predictor, response)) +
  geom_point(color = "black") +  # Original data points
  geom_line(y = predict(mod_logistic), color = "red", size = 1) +  # Fitted line
  theme_minimal() +
  labs(x = "Predictor", y = "Response") +
  theme_classic()
```

# Self-starting functions

Several self-starting functions are becoming available in R. These automatically estimate the initial parameters ($a$, $b$, $c$, etc.). There are some pre-defined functions available (e.g. `SSasymp()`, `SSexp()` and `SSlogis()`) which may be slightly different.

To refit the asymptotic model using the self-starting function, we can use the `SSasymp()` function. Note - the parameters are labelled differently. The equation used is $y = Asym \cdot (1-e^{-e^{lrc} \cdot x})$. However, the `Residual standard error` is identical, which suggests this line fits as well as the previous line -- which we can see below as well.

- `input`: the predictor variable.
- `Asym`: the upper limit ($b$).
- `R0`: the lower limit ($a$).
- `lrc`: the logarithm of the rate of change ($c$).

```{r}
#| echo: true
#| code-fold: true

mod_asymptotic_ss <- nls(response ~ SSasymp(predictor, b, a, c), data = asymptotic)
summary(mod_asymptotic_ss)

# Plot both models
ggplot(data = asymptotic, aes(x = predictor, y = response)) +
  geom_point() +
  labs(x = "Predictor", y = "Response") +
  geom_line(aes(y = predict(mod_asymptotic)), color = "red", size = 2) +
  geom_line(aes(y = predict(mod_asymptotic_ss)), color = "blue", size = 1)
```

Likewise the logistic model can be refit using the `SSlogis()` function. The equation used is $ y = \frac{Asym}{1+exp \frac{xmid-input}{scal}} $ -- the lowest possible value is 0.

- `input`: the predictor variable.
- `Asym`: the upper limit ($d$).
- `xmid`: the value of $x$ when $y$ is halfway between the lower and upper limits ($a$).
- `scal`: the rate of change ($b$).

```{r}
#| echo: true
#| code-fold: true

mod_logistic_ss <- nls(response ~ SSlogis(predictor, d, a, b), data = logistic)
summary(mod_logistic_ss)

# Plot both models
ggplot(data = logistic, aes(x = predictor, y = response)) +
  geom_point() +
  labs(x = "Predictor", y = "Response") +
  geom_line(aes(y = predict(mod_logistic)), color = "red", size = 2) +
  geom_line(aes(y = predict(mod_logistic_ss)), color = "blue", size = 1)
```

Both models are very similar, but there is a slight difference -- the `nls()` model had a `Residual standard error` of 8.891, whereas the `SSlogis()` model has a `Residual standard error` of 8.824. Both models fit the data well, and the difference is negligible.

There are options to define your own [self-starting functions](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/selfStart), but this is beyond the scope of this course.