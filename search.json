[
  {
    "objectID": "module01/03-sampling_distributions.html",
    "href": "module01/03-sampling_distributions.html",
    "title": "Sampling distributions",
    "section": "",
    "text": "First some definitions‚Ä¶\nA simple random sample is a sample of size n drawn from a population of size \\(N\\) in such a way that every possible sample of size \\(n\\) has the same probability of being selected. Variability among the simple random samples drawn from the same population is called sampling variability, and the probability distribution that characterizes some aspect of the sampling variability, usually the mean but not always, is called a sampling distribution.\nSample means are extremely important because in experiments we apply treatments to samples of material and use the mean results or yields as measures of the effects of the treatments.\n\\(\\bar y\\) is just one estimate of \\(\\mu\\), say \\(\\bar y_1\\). If another sample of size \\(n\\) were drawn from the population, we would have a slightly different estimate of \\(\\mu\\), say \\(\\bar y_2\\). For example, if we take a sample of 20 from the herd of 100 cows in the Table below we may find that the mean is 16.025. If we calculate the mean for a second sample (say the values in columns 5 & 7 of the Table), we have \\(\\bar y  = 17.055\\). If we take other samples we could get more values of the sample mean, in general all different, and by taking sufficient samples we could obtain the distribution of the values of the sample means for a given size of sample. That is, this process could be repeated, giving many different estimates of \\(\\mu\\), say \\(\\bar y_1, \\bar y_2, \\bar y_3, \\bar y_4,...\\) This forms a distribution of possible sample means, \\(\\bar y\\)‚Äôs.\nProperties we would expect the distribution of the sample mean to have:\n\nThe mean value of the distribution of the sample mean would be the same as the mean value of the distribution of the original observations (since there is no reason for expecting it to be either greater or smaller). (Population mean of the \\(\\bar y\\)‚Äôs is \\(\\mu\\).)\nThe mean of a number of observations should be a better estimate of the population mean so that we would expect the spread of the distribution of the sample means to be less than that of the distribution of the original observations.\n\nTable Average weekly milk yields (in gallons) of a herd of 100 cows [p.¬†10 Mead et al.¬†(1993)]\n A result (without mathematical proof)‚Ä¶\nIf a variable y is distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the variable \\(\\bar y\\) which is the mean of a sample of \\(n\\) observations of \\(y\\), is distributed with mean \\(\\mu\\) and variance \\(sigma^2/n\\). The variance of the distribution of the sample mean is, as expected, less than the variance of an individual observation.\nThe square root of the variance of a sample mean is called the standard error of the mean rather than the standard deviation of the mean. This term is used to avoid confusion with the standard deviation of the distribution of individual observations.\nThe standard error of the mean is given by\n\\(\\sigma/\\sqrt n\\) or \\(\\sqrt{\\frac{\\sigma^2}{n}}\\).\nwhich is usually estimated by the standard deviation of the sample mean,\n\\(se(\\bar y)=s/\\sqrt n\\) = or \\(\\sqrt{\\frac{s^2}{n}}\\)\n\n$se(y) is a measure of accuracy for estimating \\(\\mu\\) such that:\n\n\\(se(\\bar y)\\) \\(\\uparrow\\) as \\(s\\) \\(\\uparrow\\)\n\\(se(\\bar y)\\) \\(\\downarrow\\) as \\(n\\) \\(\\uparrow\\).\n\nTo halve the size of a standard error, the sample size (n) needs to be increased \\(4\\times\\); a reduction to 1/3 requires a \\(9\\times\\) increase in \\(n\\), etc.\n\nExample: Root growth of rye grass\nSince the sample mean of \\(\\bar y = 305\\) mg and standard deviation of \\(s = 46\\) mg was based on \\(n = 10\\) pots, the standard error of \\(\\bar y\\) is\n\\(se(\\bar y) = \\frac{46}{\\sqrt{10}} = 14.5\\) mg.\n\ns &lt;- 46\nn &lt;- 10\nse &lt;- s / sqrt(n)\nse\n\nHow many observations are required to have a standard error of no more than 5 mg?\nSolution:\n\\(se(\\bar y) = 5 = \\frac{46}{\\sqrt n}\\) \\(\\sqrt n = 46/5 = 9.2\\) \\(n = 9.2^2 = 85\\)\nSo n = 85 observations would be required\nNotes - We now can also think of \\(\\bar y\\) being an entire distribution of values, but in practice we usually only observe one value of\\(\\bar y\\) - If the original data is normally distributed, \\(y \\sim N(\\mu,\\sigma^2)\\), the distribution of the sample mean is also normal, \\(y \\sim N(\\mu,\\sigma^2/n)\\)\nExample:\n\nRye grass root growth (mg dry weight), \\(y \\sim N(300, 50^2)\\)\nBased on samples of size n = 10, \\(\\bar y \\sim N(300, 50^2 / 10)\\), i.e.¬†\\(\\bar y \\sim N(300, 15.82)\\).\n\n Two other mathematical results concerning the distribution of the sample mean emphasize the importance of the normal distribution:\n\nIf the distribution of the original observation, \\(y\\), is normal, then the distribution of the sample mean, \\(\\bar y\\), is also normal.\nFor almost all forms of distribution of the original observation, \\(y\\), the distribution of the sample mean, \\(\\bar y\\), for a sample of \\(n\\) observations, tends to the normal distribution as \\(n\\) increases.\n\nContinuing from the 2nd result‚Ä¶\nIn fact the tendency for distributions of sample means to become normal is so strong that, for many forms of original distribution, if n is more than 5 the distribution of the sample mean is almost indistinguishable from the normal distribution. The result, which is known as the Central Limit Theorem, is extremely useful. It can be used to explain why so many biological quantities have an approximately normal distribution.\nExample: Distribution of the Sample Mean\nRye grass root growth (mg dry weight), \\(y \\sim N(300, 502)\\) A single root measurement is made. How likely is it that the dry weight exceeds 320 mg? (i.e.¬†\\(P(Y &gt; 320)\\))\nSolution:\n\\(P(Y &gt; 320) = 1 - P(Y \\leq 320)\\)\n\nmu &lt;- 300\nsigma &lt;- 50\n1 - pnorm(320, mu, sigma)\n\nBased on samples of size n = 10, \\(\\bar y ~ N(300, 502 / 10)\\), i.e.¬†\\(\\bar y \\sim N(300, 15.82)\\). How likely is it that a sample mean based on 10 observations exceeds 320 mg? (i.e.¬†P(\\(\\bar y &gt; 320\\)))\n\nmu &lt;- 300\nsigma &lt;- 50\nn &lt;- 10\n1 - pnorm(320, mu, sigma / sqrt(n))\n\nNote that it is less likely for a mean of 10 observations to be this ‚Äúfar‚Äù from the population mean, compared with a single value.\n\n\nThe Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p.¬†23 Mead et al.¬†(1993)]\n\nCreate a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"data/Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\nsd(carrots$Weight_g)\n\nggplot(carrots, aes(x = Weight_g)) +\n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "**üìï Module 1**",
      "Sampling distributions"
    ]
  },
  {
    "objectID": "module01/03-sampling_distributions.html#the-central-limit-theorem",
    "href": "module01/03-sampling_distributions.html#the-central-limit-theorem",
    "title": "Sampling distributions",
    "section": "",
    "text": "The Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p.¬†23 Mead et al.¬†(1993)]\n\nCreate a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"data/Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\nsd(carrots$Weight_g)\n\nggplot(carrots, aes(x = Weight_g)) +\n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "**üìï Module 1**",
      "Sampling distributions"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html",
    "href": "module01/01-exploring_data.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g.¬†sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).\n\n\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (‚Äúmu‚Äù) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (‚Äúsigma‚Äù) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read ‚Äúy bar‚Äù) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)‚Äôs to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the ‚Äúmiddle‚Äù value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read ‚Äúy tilde‚Äù) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g.¬†height) or discrete (e.g.¬†number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1 # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3 # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(\n    dnorm(x_values, mean_value, std_dev_A),\n    dnorm(x_values, mean_value, std_dev_B),\n    dnorm(x_values, mean_value, std_dev_C)\n  ),\n  curve = factor(c(\n    rep(\"A\", length(x_values)),\n    rep(\"B\", length(x_values)),\n    rep(\"C\", length(x_values))\n  ))\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(\n    title = \"Normal Distributions with Different Spreads\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  scale_color_manual(\n    values = c(\"red\", \"green\", \"blue\"),\n    labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")\n  ) +\n  theme_minimal()\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more ‚Äúspread‚Äù out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or ‚Äúspread‚Äù is the range ‚Äì the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you‚Äôve taken. [It‚Äôs unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y) - min(y)\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the ‚Äúspread‚Äù. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25)\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g.¬†body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n‚ÄúThe degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e.¬†the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.‚Äù\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]\n\n\n\n\n\n\nTables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5), # Assuming a Poisson distribution for count data\n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e.¬†\\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e.¬†\\(\\Sigma^r_{i=1}f_i/n \\times 100\\)\n\n\n\n\n\n\nFrequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"data/Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count) / sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\nContinuous measurements are free to take any whole or fractional number within their range e.g.¬†plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"data/Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ‚âà 95 mm (the centre of the data) and standard deviation ‚âà 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e.¬†a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e.¬†30 or more data values).\n\n\n\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) ‚Äúplugs‚Äù were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g.¬†Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the ‚Äúbox‚Äù\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a ‚Äú*‚Äù or \\(\\bullet\\).\n\nStep 3: Determining the ‚Äúwhisker‚Äù lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e.¬†to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e.¬†to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.\n\n\n\n\n\n\nThis is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we‚Äôll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the ‚Äúx‚Äù at the right hand side of the graph. The ‚Äú4‚Äù indicates that this is the 4th observation in the data set i.e.¬†the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1 + p2\n\n\n\n\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"data/TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1 + p2\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"data/Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1 + p2\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\nskewness(tccb)\nskewness(arthritis)\n\nKURTOSIS\nKurtosis is a measure of how ‚Äúpeaked‚Äù (leptokurtic) a frequency distribution is or how ‚Äúflattened‚Äù (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\nkurtosis(tccb)\nkurtosis(arthritis)\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g.¬†weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.\n\n\n\n\n\n\nMost natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is ‚Äúflatness‚Äù).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We‚Äôll look at these later.",
    "crumbs": [
      "**üìï Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#numerical-summaries",
    "href": "module01/01-exploring_data.html#numerical-summaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g.¬†sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).\n\n\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (‚Äúmu‚Äù) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (‚Äúsigma‚Äù) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read ‚Äúy bar‚Äù) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)‚Äôs to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the ‚Äúmiddle‚Äù value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read ‚Äúy tilde‚Äù) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g.¬†height) or discrete (e.g.¬†number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1 # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3 # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(\n    dnorm(x_values, mean_value, std_dev_A),\n    dnorm(x_values, mean_value, std_dev_B),\n    dnorm(x_values, mean_value, std_dev_C)\n  ),\n  curve = factor(c(\n    rep(\"A\", length(x_values)),\n    rep(\"B\", length(x_values)),\n    rep(\"C\", length(x_values))\n  ))\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(\n    title = \"Normal Distributions with Different Spreads\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  scale_color_manual(\n    values = c(\"red\", \"green\", \"blue\"),\n    labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")\n  ) +\n  theme_minimal()\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more ‚Äúspread‚Äù out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or ‚Äúspread‚Äù is the range ‚Äì the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you‚Äôve taken. [It‚Äôs unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y) - min(y)\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the ‚Äúspread‚Äù. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25)\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g.¬†body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n‚ÄúThe degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e.¬†the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.‚Äù\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]",
    "crumbs": [
      "**üìï Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#tabular-summaries",
    "href": "module01/01-exploring_data.html#tabular-summaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Tables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5), # Assuming a Poisson distribution for count data\n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e.¬†\\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e.¬†\\(\\Sigma^r_{i=1}f_i/n \\times 100\\)",
    "crumbs": [
      "**üìï Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#graphical-summaries",
    "href": "module01/01-exploring_data.html#graphical-summaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Frequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"data/Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count) / sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\nContinuous measurements are free to take any whole or fractional number within their range e.g.¬†plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"data/Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ‚âà 95 mm (the centre of the data) and standard deviation ‚âà 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e.¬†a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e.¬†30 or more data values).\n\n\n\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) ‚Äúplugs‚Äù were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g.¬†Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the ‚Äúbox‚Äù\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a ‚Äú*‚Äù or \\(\\bullet\\).\n\nStep 3: Determining the ‚Äúwhisker‚Äù lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e.¬†to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e.¬†to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.",
    "crumbs": [
      "**üìï Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#shapes-of-distributions",
    "href": "module01/01-exploring_data.html#shapes-of-distributions",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we‚Äôll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the ‚Äúx‚Äù at the right hand side of the graph. The ‚Äú4‚Äù indicates that this is the 4th observation in the data set i.e.¬†the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1 + p2\n\n\n\n\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"data/TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1 + p2\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"data/Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1 + p2\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\nskewness(tccb)\nskewness(arthritis)\n\nKURTOSIS\nKurtosis is a measure of how ‚Äúpeaked‚Äù (leptokurtic) a frequency distribution is or how ‚Äúflattened‚Äù (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\nkurtosis(tccb)\nkurtosis(arthritis)\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g.¬†weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.",
    "crumbs": [
      "**üìï Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "href": "module01/01-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Most natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is ‚Äúflatness‚Äù).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We‚Äôll look at these later.",
    "crumbs": [
      "**üìï Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "001-intro.html",
    "href": "001-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "A practical definition of ENVX is ‚ÄúThe use of statistical and computing methods to answer quantitative biological questions.‚Äù\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.\n\n\n\nENVX is at the interface of applied statistics, data science and life and environmental science\n\n\n\n\n\nYou will soon learn that the University of Sydney has a great tradition in research (with many of our efforts earning international recognition). Your teachers will include some experiences of or findings from their research in their classes, and you will have an opportunity to participate in research projects throughout your degree. ENVX comprises of three vertically integrated units designed to equip you to confidently approach the design and data analysis aspects of your research. The diagram below shows one view of the process of research (or experimentation).\n\n\n\nThe research process\n\n\n\n\nStatistics provides us with an avenue for exploring and reporting the findings of the research. In terms of reporting, it allows us to give a measure (generally a probability) of the extent to which our conclusion could be wrong. Statistics is a means of informing the decision-making process.\n\n\n\n\nData is the information we collect on the subjects, factors and variables we are interested in studying. Data may be textual (i.e.¬†words) or numerical.\n\n\nPopulations may be real and therefore able to be listed e.g.¬†all veterinarian practices in the Sydney region. They may be hypothetical and unable to all be listed e.g.¬†all dogs possible that could have a particular treatment applied to them.\nEach sampling unit (or observation) in the population (e.g.¬†plot of land, animal, etc.) has a value y (e.g.¬†nematode count, gestational length): \\(y_1,y_2,y_3,...y_N\\). Typically the population size (N) is very large - even infinite! The population can be described by population parameters e.g.¬†population mean = \\(\\mu\\), population variance = \\(\\sigma^2\\). These are generally characters from the Greek alphabet. Since populations are large, we usually cannot determine these values exactly.\nOften we wish to make generalisations about populations that are too large or too difficult to survey completely. In these cases we sample the population and use characteristics of the sample to extrapolate to characteristics of the larger population. We can define a sample (of size n) as drawn from population ‚Äúat random‚Äù. We call each piece of information recorded about a sampling unit or subject (e.g.¬†plant, person, animal, 1x1m plot of land) an observation.\n\n\n\nn = 20 units (v) sampled at random from population of N = 1,000 (O)\n\n\nThe sample is taken to be representative of the population ‚Äì but there are no guarantees! It is easier/quicker/cheaper to take a sample than study the entire population (since n &lt;&lt; N). The results obtained from the sample not important in themselves - the importance is in how it can be used to estimate population parameters i.e.¬†\\(\\bar{y}\\) estimates \\(\\mu\\); \\(s^2\\) estimates \\(\\sigma^2\\).\nExample: For a sample of \\(n = 10\\) cows, gestation length in cattle was measured in days. The sample mean \\(\\bar{y} = 345\\) days estimates the population mean \\(\\mu = ???\\) days. The sample standard deviation \\(s = 10\\) days estimates population standard deviation \\(\\sigma = ???\\) days.\n\n\n\nThe table below shows fresh weights of cabbages that were included in a field trial that was investigating the effects of irrigation frequency and plant spacing on cabbage yields. This fresh weight measurement is on a continuous scale.\nGenerally in an experiment, several different characteristics of the subjects are measured/recorded. We may score the level of insect damage to leaves of the plant on a scale of 0 to 3 (0 = no damage, 1 = slight damage, 2 = moderate damage, 3 = heavy damage). This is a discrete categorical scale as only certain values on the scale are defined, but it is also an ordered scale. If we had difficulty deciding how to categorize plants using this scale, we might choose to classify plants as either damaged or not damaged, which is a binary measurement scale.\nAlready you can begin to see that from just one fairly simple field trial quite a lot of data can be generated, also data of differing properties/types.\n\n\n\nYields of cabbage (mean fresh weight per head in kg) for 24 plots (Source: Mead, Curnow & Hasted (2003)).\n\n\nData may also be classified as either quantitative (e.g.¬†root length) or qualitative (e.g.¬†plant species). Quantitative observations are based on some sort of measurement e.g.¬†length, weight, temperature, pH. Qualitative observations are based on categories reflecting a quality or characteristic of the observed event e.g.¬†male vs.¬†female, diseased vs.¬†healthy, mutant vs.¬†wild type.\nThe most common types of variables are:\n\nContinuous (and interval) data can assume any value in some (possible unbounded) interval of real numbers. Examples are length, weight, temperature, volume, height.\nDiscrete variables assume only isolated values. E.g. trees per hectare, items per quadrat, number of diseased plants in a section of a glasshouse. They arise from counting ‚Äì usually either the number of successes in n trials (binary data) OR the number of occurrences of the event in an interval of time or space (count data).\nCategorical variables\n\nBinary variables (listed above as discrete variables) may also be thought of as categorical variables since the subject falls into either of 2 mutually exclusive categories (yes/no, alive/dead, diseased/not diseased etc.).\nOrdinal variables are not measured but nevertheless have a natural ordering. E.g. candidates for political office can be ranked by individual voters. The rank values have no inherent meaning outside the ‚Äúorder‚Äù that they provide. That is, a candidate ranked 2 is not twice as preferable as the person ranked 1. (Compare this with measurement variables where a plant 2 feet tall is twice as tall as a plant1 foot tall. With measurement variables such ratios are meaningful, while with ordinal variables they are not.)\nNominal data is qualitative data. Some examples are species, gender, genotype, phenotype, healthy/diseased. Unlike ranked data, there is no ‚Äúnatural‚Äù ordering that can be assigned to these categories.\n\n\nNote that some applied statistics texts will define the types of data slightly differently to that shown above.\nIn this unit of study most emphasis will be on the analysis of continuous measurement variables. However a few basic analyses for discrete and categorical variables will be covered.\nRecognising the type of data we have measured is REALLY important as it helps to determine the choice of analysis (and even the descriptive statistics we undertake e.g.¬†the mean of a score doesn‚Äôt make sense, but the median is a good alternative measure of central tendency).\n\n\n\nSometimes you will want to explore the relationship between two (or more) variables that you have measured in your research. You will explore the strength of the relationship and the nature of it (e.g.¬†linear, exponential etc.).\nIn the most simple case, we examine the amount of variability in one variable (Y, the dependent variable) that is explained by changes in another variable (X, the independent variable). Often the X variable is called the predictor and the Y variable the response.\n\n\n\nVariation is the norm. It occurs in both observational studies as well as designed experiments. For example, river flow varies from sampling time to sampling time and from sampling location to sampling location. Also, levels of soil contamination on a site vary from site to site.\nThis type of variation distinguishes the biological and environmental sciences from the physical sciences, which shows relatively little variability:\n\nDropping a ball from a certain height: the time to reach the ground is (nearly) identical each time;\nAmount of a chemical product produced when reagents are mixed are (nearly) identical and predictable.\n\nBiological data is far more variable due to environmental and genetic effects. To interpret biological data, we need to control variation by an appropriate experimental design, and adjust for variation by means of statistical analyses.\n\n\n\n\nWe use statistical distributions to determine the probability of occurrence of our particular sets of observations. A statistical distribution is a representation (using either mathematical formula or a table) of all possible outcomes of a given event. The most well-known distribution is the normal (or Gaussian) distribution for which the probability distribution function is the familiar bell-shaped curve.\n\n\n\nThe aim of sampling is to gain a representative picture of the population. There are various methods and strategies for doing this. Experimental groups/samples must be constructed without bias and must be large enough to give the researcher an acceptable level of confidence in the results.\n\n\n\nA hypothesis is a tentative explanation for the initial or ad hoc observations made. It suggests a cause and effect or associative relationship that is testable, e.g.¬†yield response to nitrogen (N) fertilizer. The purpose and design of an experiment is to test the hypothesis.\n\n\n\nWe decide whether or not a research outcome if significant by conducting a statistical test. Firstly we set arbitrary critical thresholds of probability (P-values). The occurrence of an event whose estimated probability is less than a critical threshold is regarded as a statistically significant outcome. The usual significance level chosen is P&lt;0.05. You will also see P&lt;0.01 and P&lt;0.001 used in research literature. Which statistical test you use (there are many!) depends on the type of data you have collected and the question you wish to ask.\n\n\n\nIn this unit of study, we will focus on the use of one statistical package called R. It has been designed specifically for statistical analysis and is freely available. It is a powerful tool for data analysis and is widely used in the biological and environmental sciences.\nYou will also learn to use Microsoft Excel to organize and summarise your data. In the next section some of the basic features of Excel are presented. See computer lab 1.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#what-is-envx",
    "href": "001-intro.html#what-is-envx",
    "title": "Introduction",
    "section": "",
    "text": "A practical definition of ENVX is ‚ÄúThe use of statistical and computing methods to answer quantitative biological questions.‚Äù\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.\n\n\n\nENVX is at the interface of applied statistics, data science and life and environmental science",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#introduction-to-research",
    "href": "001-intro.html#introduction-to-research",
    "title": "Introduction",
    "section": "",
    "text": "You will soon learn that the University of Sydney has a great tradition in research (with many of our efforts earning international recognition). Your teachers will include some experiences of or findings from their research in their classes, and you will have an opportunity to participate in research projects throughout your degree. ENVX comprises of three vertically integrated units designed to equip you to confidently approach the design and data analysis aspects of your research. The diagram below shows one view of the process of research (or experimentation).\n\n\n\nThe research process\n\n\n\n\nStatistics provides us with an avenue for exploring and reporting the findings of the research. In terms of reporting, it allows us to give a measure (generally a probability) of the extent to which our conclusion could be wrong. Statistics is a means of informing the decision-making process.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#data",
    "href": "001-intro.html#data",
    "title": "Introduction",
    "section": "",
    "text": "Data is the information we collect on the subjects, factors and variables we are interested in studying. Data may be textual (i.e.¬†words) or numerical.\n\n\nPopulations may be real and therefore able to be listed e.g.¬†all veterinarian practices in the Sydney region. They may be hypothetical and unable to all be listed e.g.¬†all dogs possible that could have a particular treatment applied to them.\nEach sampling unit (or observation) in the population (e.g.¬†plot of land, animal, etc.) has a value y (e.g.¬†nematode count, gestational length): \\(y_1,y_2,y_3,...y_N\\). Typically the population size (N) is very large - even infinite! The population can be described by population parameters e.g.¬†population mean = \\(\\mu\\), population variance = \\(\\sigma^2\\). These are generally characters from the Greek alphabet. Since populations are large, we usually cannot determine these values exactly.\nOften we wish to make generalisations about populations that are too large or too difficult to survey completely. In these cases we sample the population and use characteristics of the sample to extrapolate to characteristics of the larger population. We can define a sample (of size n) as drawn from population ‚Äúat random‚Äù. We call each piece of information recorded about a sampling unit or subject (e.g.¬†plant, person, animal, 1x1m plot of land) an observation.\n\n\n\nn = 20 units (v) sampled at random from population of N = 1,000 (O)\n\n\nThe sample is taken to be representative of the population ‚Äì but there are no guarantees! It is easier/quicker/cheaper to take a sample than study the entire population (since n &lt;&lt; N). The results obtained from the sample not important in themselves - the importance is in how it can be used to estimate population parameters i.e.¬†\\(\\bar{y}\\) estimates \\(\\mu\\); \\(s^2\\) estimates \\(\\sigma^2\\).\nExample: For a sample of \\(n = 10\\) cows, gestation length in cattle was measured in days. The sample mean \\(\\bar{y} = 345\\) days estimates the population mean \\(\\mu = ???\\) days. The sample standard deviation \\(s = 10\\) days estimates population standard deviation \\(\\sigma = ???\\) days.\n\n\n\nThe table below shows fresh weights of cabbages that were included in a field trial that was investigating the effects of irrigation frequency and plant spacing on cabbage yields. This fresh weight measurement is on a continuous scale.\nGenerally in an experiment, several different characteristics of the subjects are measured/recorded. We may score the level of insect damage to leaves of the plant on a scale of 0 to 3 (0 = no damage, 1 = slight damage, 2 = moderate damage, 3 = heavy damage). This is a discrete categorical scale as only certain values on the scale are defined, but it is also an ordered scale. If we had difficulty deciding how to categorize plants using this scale, we might choose to classify plants as either damaged or not damaged, which is a binary measurement scale.\nAlready you can begin to see that from just one fairly simple field trial quite a lot of data can be generated, also data of differing properties/types.\n\n\n\nYields of cabbage (mean fresh weight per head in kg) for 24 plots (Source: Mead, Curnow & Hasted (2003)).\n\n\nData may also be classified as either quantitative (e.g.¬†root length) or qualitative (e.g.¬†plant species). Quantitative observations are based on some sort of measurement e.g.¬†length, weight, temperature, pH. Qualitative observations are based on categories reflecting a quality or characteristic of the observed event e.g.¬†male vs.¬†female, diseased vs.¬†healthy, mutant vs.¬†wild type.\nThe most common types of variables are:\n\nContinuous (and interval) data can assume any value in some (possible unbounded) interval of real numbers. Examples are length, weight, temperature, volume, height.\nDiscrete variables assume only isolated values. E.g. trees per hectare, items per quadrat, number of diseased plants in a section of a glasshouse. They arise from counting ‚Äì usually either the number of successes in n trials (binary data) OR the number of occurrences of the event in an interval of time or space (count data).\nCategorical variables\n\nBinary variables (listed above as discrete variables) may also be thought of as categorical variables since the subject falls into either of 2 mutually exclusive categories (yes/no, alive/dead, diseased/not diseased etc.).\nOrdinal variables are not measured but nevertheless have a natural ordering. E.g. candidates for political office can be ranked by individual voters. The rank values have no inherent meaning outside the ‚Äúorder‚Äù that they provide. That is, a candidate ranked 2 is not twice as preferable as the person ranked 1. (Compare this with measurement variables where a plant 2 feet tall is twice as tall as a plant1 foot tall. With measurement variables such ratios are meaningful, while with ordinal variables they are not.)\nNominal data is qualitative data. Some examples are species, gender, genotype, phenotype, healthy/diseased. Unlike ranked data, there is no ‚Äúnatural‚Äù ordering that can be assigned to these categories.\n\n\nNote that some applied statistics texts will define the types of data slightly differently to that shown above.\nIn this unit of study most emphasis will be on the analysis of continuous measurement variables. However a few basic analyses for discrete and categorical variables will be covered.\nRecognising the type of data we have measured is REALLY important as it helps to determine the choice of analysis (and even the descriptive statistics we undertake e.g.¬†the mean of a score doesn‚Äôt make sense, but the median is a good alternative measure of central tendency).\n\n\n\nSometimes you will want to explore the relationship between two (or more) variables that you have measured in your research. You will explore the strength of the relationship and the nature of it (e.g.¬†linear, exponential etc.).\nIn the most simple case, we examine the amount of variability in one variable (Y, the dependent variable) that is explained by changes in another variable (X, the independent variable). Often the X variable is called the predictor and the Y variable the response.\n\n\n\nVariation is the norm. It occurs in both observational studies as well as designed experiments. For example, river flow varies from sampling time to sampling time and from sampling location to sampling location. Also, levels of soil contamination on a site vary from site to site.\nThis type of variation distinguishes the biological and environmental sciences from the physical sciences, which shows relatively little variability:\n\nDropping a ball from a certain height: the time to reach the ground is (nearly) identical each time;\nAmount of a chemical product produced when reagents are mixed are (nearly) identical and predictable.\n\nBiological data is far more variable due to environmental and genetic effects. To interpret biological data, we need to control variation by an appropriate experimental design, and adjust for variation by means of statistical analyses.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#statistical-distributions",
    "href": "001-intro.html#statistical-distributions",
    "title": "Introduction",
    "section": "",
    "text": "We use statistical distributions to determine the probability of occurrence of our particular sets of observations. A statistical distribution is a representation (using either mathematical formula or a table) of all possible outcomes of a given event. The most well-known distribution is the normal (or Gaussian) distribution for which the probability distribution function is the familiar bell-shaped curve.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#sampling",
    "href": "001-intro.html#sampling",
    "title": "Introduction",
    "section": "",
    "text": "The aim of sampling is to gain a representative picture of the population. There are various methods and strategies for doing this. Experimental groups/samples must be constructed without bias and must be large enough to give the researcher an acceptable level of confidence in the results.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#hypotheses",
    "href": "001-intro.html#hypotheses",
    "title": "Introduction",
    "section": "",
    "text": "A hypothesis is a tentative explanation for the initial or ad hoc observations made. It suggests a cause and effect or associative relationship that is testable, e.g.¬†yield response to nitrogen (N) fertilizer. The purpose and design of an experiment is to test the hypothesis.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#statistical-tests",
    "href": "001-intro.html#statistical-tests",
    "title": "Introduction",
    "section": "",
    "text": "We decide whether or not a research outcome if significant by conducting a statistical test. Firstly we set arbitrary critical thresholds of probability (P-values). The occurrence of an event whose estimated probability is less than a critical threshold is regarded as a statistically significant outcome. The usual significance level chosen is P&lt;0.05. You will also see P&lt;0.01 and P&lt;0.001 used in research literature. Which statistical test you use (there are many!) depends on the type of data you have collected and the question you wish to ask.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#software",
    "href": "001-intro.html#software",
    "title": "Introduction",
    "section": "",
    "text": "In this unit of study, we will focus on the use of one statistical package called R. It has been designed specifically for statistical analysis and is freely available. It is a powerful tool for data analysis and is widely used in the biological and environmental sciences.\nYou will also learn to use Microsoft Excel to organize and summarise your data. In the next section some of the basic features of Excel are presented. See computer lab 1.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "labs/Lab12.html",
    "href": "labs/Lab12.html",
    "title": "Lab 12 - Non-linear models",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nCalculate ‚Äúby hand‚Äù the initial estimates of the parameters of a non-linear model\nInterpret tables of regression coefficients for polynomials to perform hypothesis testing\nFit polynomials and non-linear models to data using least-squares fitting using the SOLVER add-in in Excel\nFit polynomials and non-linear models to data in R, and interpret the outputs",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#before-we-begin",
    "href": "labs/Lab12.html#before-we-begin",
    "title": "Lab 12 - Non-linear models",
    "section": "Before we begin",
    "text": "Before we begin\nCreate your Quarto document and save it as Lab-12.qmd or similar.\nThe following data files are required:\n\neast_creek.csv\n\nOver the past few weeks you have explored linear models and how to interpret model summary output. Again we have stepped up the complexity, now venturing into the world of non-linear models.\nThis practical focuses on fitting non-linear models to data with an emphasis on 3 important classes of functions that all budding biologists and environmental scientists should know\n\npolynomials,\nexponential models, and\nlogistic models.\n\nA question before we begin:\nWhat are some advantages and disadvantages of non-linear models as compared to polynomials?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#polynomials",
    "href": "labs/Lab12.html#polynomials",
    "title": "Lab 12 - Non-linear models",
    "section": "Polynomials",
    "text": "Polynomials\n\nQuadratic\n\\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nwhere the parameters are the y-intercept (b0), the linear component (b1) and the quadratic component (b2).\nIf b2 is negative then the shape of the function is convex upwards, i.e.¬†y increases with x until reaches a peak and then y decreases.\nIt is easy to understand so it has been commonly used for modelling the response of yield to inputs such as fertiliser, seeding rates. This is despite much criticism for being unrealistic.\nLimitations:\n\nrate of increase to peak is same as rate of decrease past peak\ndoes not level off as x becomes small or very large, y just keeps increasing or decreasing.\n\nCubic\n\\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nCompared to the quadratic model which has 1 turning point, a cubic model has 2 turning points.\n\n\n\nExercise 1: Interpreting polynomials\nA study was performed to examine the soil properties that control the within-field variation in crop yield. The focus of this question is on soil pH which among other things controls the availability of nutrients to plants.\nThis exercise does not require you to read in any data, but rather focus on interpreting the model output and comparing the models.\nThe figure below shows the raw observations of yield plotted against pH with three models fitted; a linear model, quadratic polynomial and a cubic polynomial.\n\nwhich line corresponds to which model?\n\n\n\nbased on the output from the 3 models below, which model fits the data best? Note: no hypothesis testing yet, just how well the model fits the data (r2).\n\n\n\nLinear model:\n\n\nQuadratic model:\n\n\nCubic model:\n\n\n\nUse the R output to perform hypothesis testing to find the best model. Write out the hypotheses you are testing.\n\n\n\n\nExercise 2: Fitting polynomials in R\nThis exercise will use real data from a yield-fertiliser trial in Bedfordshire, United Kingdom.\nFirst thing we can do is fit a linear model to the fertiliser data:\n\n# create fertiliser and yield objects\nfert&lt;-c(0,100,170,225)\nyield&lt;-c(3.32,5.23,5.41,5.02)\n\n# Fits a linear model and saves it to an object called lin.mod \nlin.mod&lt;-lm(yield~fert)\n\n# Summarises key features of model\nsummary(lin.mod)\n\n\nCall:\nlm(formula = yield ~ fert)\n\nResiduals:\n      1       2       3       4 \n-0.4469  0.6727  0.2995 -0.5252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 3.766929   0.634765   5.934   0.0272 *\nfert        0.007904   0.004243   1.863   0.2035  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7134 on 2 degrees of freedom\nMultiple R-squared:  0.6344,    Adjusted R-squared:  0.4515 \nF-statistic:  3.47 on 1 and 2 DF,  p-value: 0.2035\n\n\n\nWhat is the model fit like in this model?\n\nFit and plot a quadratic polynomial in R. In R a quadratic polynomial can be fitted using the following lines of code:\n\n# create a new variable which is the square of the fertilizer rates\nfert2&lt;-fert^2\n\n# fit the quadratic model incorporating fert2\nquad.mod&lt;-lm(yield~fert+fert2)\n\nsummary(quad.mod)\n\n\nCall:\nlm(formula = yield ~ fert + fert2)\n\nResiduals:\n        1         2         3         4 \n-0.005611  0.024528 -0.032791  0.013874 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  3.326e+00  4.324e-02   76.92  0.00828 **\nfert         2.786e-02  9.014e-04   30.91  0.02059 * \nfert2       -9.064e-05  3.921e-06  -23.12  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0436 on 1 degrees of freedom\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.998 \nF-statistic: 731.7 on 2 and 1 DF,  p-value: 0.02613\n\n\n\nWhat is the fit like for our quadratic model? is it better than our linear model?\n\nIn Excel it is easy to fit a line, by creating a scatterplot, then add Trendline‚Ä¶ and selecting Polynomial (2nd order).\nTo fit our polynomial line in R, we need to obtain model predictions first.\nTo plot model predictions you first need to predict at fine intervals of the predictor to make a continuous plot that is not jagged or stepped. To create a new prediction dataset you can use the following code:\n\n# creates a sequence of numbers from 0 to 225 going up in increments of 1\nnew.fert&lt;-seq(0,225,1) \n\nWe can use our model to predict at the values in the new prediction dataset, in this case new.fert.\n\nnew.pred&lt;-predict(quad.mod,list(fert=new.fert,fert2=new.fert^2))\n\nThe general form of the predict function is predict(model object, list object).\nThe list object tells R what object contains the data we will use to predict. For example in our case the model was built on fert and fert2 so we have to tell the predict function what object contains the new values for each of these, in our case new.fert.\nNow we plot the raw observation as points and add an overlay of the model fit as lines:\n\nplot(fert,yield,xlab='Fertilizer', ylab='Yield')\nlines(new.fert,new.pred) #Adds lines to original plot",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#exponential-function",
    "href": "labs/Lab12.html#exponential-function",
    "title": "Lab 12 - Non-linear models",
    "section": "Exponential function",
    "text": "Exponential function\n\n\\(y=y_0e^{kx}\\)\nwhere the parameters are y0 which is the multiplier which expresses the starting or final value, and k which is negative for exponential decay and positive for the exponential growth.\nThe half life (for decay) or doubling time (for growth) can be calculated as\n\\(\\frac{log_e 2}{k}\\)\nLimitations:\n\nharder to fit than polynomials\nexponential growth has no horizontal asymptote; keeps going up.\n\n\n\n\nExercise 3: Initial estimates for exponential growth function\nIn this exercise we will find initial estimates of the parameters of an exponential growth model by visual assessment of plots of the data and/or rough calculations. The initial estimates of the parameters are needed as starting points for the iterative fitting methods we will use in the practicals, e.g.¬†SOLVER in Excel and the nls() function in R.\nThe plot and table below presents the population of the world from 1650-1965.\nWe wish to model the data with an exponential growth function of the form;\n\\(y=y_0e^{kx}\\)\nwhere\n\ny is the population in the year x,\ny0 is the population in 1650 and\nk is the rate constant.\n\n\n\nProvide an initial estimate of y0.\n\nThe parameter k can be estimated from a linear model fitted to loge population against year.\nRather than formally fitting a linear model you could estimate the slope approximately by using the smallest and largest value to estimate the slope and therefore k.\n\nUse this approach to estimate k.\n\nHint: \\[\nslope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}\n\\]\n\nFor an exponential growth model the doubling time of a population can be estimated by loge2 /k.\n\nExamine the graph and/or table to estimate the doubling time and use this to estimate k. You will have to make k the subject in the equation for estimating the doubling time.\n\nHow similar were the estimates of k?\n\n\n\n\nExercise 4 : Exponential growth models\nThis data is from Jenkins & Adams (2010) who studied soil respiration rates against temperature for different vegetation communities in the Snowy Mountains. They fitted an exponential growth model to the data.\nThe purpose of this exercise is to illustrate the dangers of using Excel‚Äôs in-built functions for statistics more complex than calculating means and fitting simple models.\nPlot the data in Excel and using the Add Trendline‚Ä¶ option. Make sure tick the option for displaying the equation in the graph.\n\nThe researchers performed the experiment up to a temperature of 40 degrees C, would you expect exponential growth in the respiration rate to continue if high temperatures were considered? Is there a better model?\n\nNow fit the same model in R using the nls function. Code to get you started is:\n\ntemp&lt;-c(5,10,20,25,30,35,40)\nrespiration&lt;-c(1,2,4,6,8,11,18)\n\n##Initial parameters\nexp.mod&lt;-c(Bp=1.0,Cp=0.1)\n\n##Fits exponential model\nres.exp&lt;-nls(respiration~Bp * exp(temp*Cp),start=exp.mod,trace=T)\n\n##Summarise model\nsummary(res.exp)\n\n\nNow you can fit a line to the plot. Does this look similar to your trendline in Excel?\n\n\n#Plots raw data\nplot(temp,respiration,xlab='Temperature',ylab='Respiration')\n\n#Creates new dataset for predictions ( 5 to 40 at an interval of 1)\ntemp.new&lt;-seq(5,40,1)\n\n#Makes predictions onto temp.new\npred.exp&lt;-predict(res.exp,list(temp=temp.new))\n\n#Adds model fit to existing plots\nlines (temp.new,pred.exp)\n\nCompare the parameters values between Excel and R. You can extract the RSS value from an nls object by using the code below:\n\ndeviance(res.exp)\n\n\nCalculate the RSS value for the Excel exponential model. Based on this, which is the better model?\n\nWhen faced with the need to fit an exponential function, one approach that was used before computing power became readily accessible was to log the y values which linearises the relationship with x, enabling the modeller to use a simple linear model.\nIf we linearise, model would be \\(log_e(y) = b_0 + b_1x\\), where \\(e^{b_0}\\) is the y0 parameter in an exponential model, and b1 is the k parameter in the exponential model. This is similar to what was demonstrated in the lecture this week.\nIn Excel, log the soil respiration data and fit a linear model. You will see that the fitted model gives the same values as the exponential model fitted to the untransformed data.\nIf you compare the r2 values for both you will see they are the same. This means that Excel reports the r2 of the linear model fitted to logged respiration as the r2 of the exponential model fitted to the raw data. This is naughty of Excel.\n\nFor the dataset used here the exponential model fits it so well the Excel approach is only slightly different to the correct approach used in R.\nIn cases where the model does not fit the data so well the differences would be larger. Logarithm makes smaller values larger and larger values smaller.\nWHY IS THIS SUB-OPTIMAL?\n\nRegression modelling assumes that the residuals are normally distributed so logging normally distributed data will change the distribution to a non-normal one ‚Äì it is best to analyse the data without transformation.\nModelling data on the logged scale reduces the impact that larger values have on minimising the RSS but when you plot the fitted model with the original data you may observe large discrepancies for larger values. In other words, using a linear model on the log(data) can result in a higher discrepancy for larger values when plotting the fitted model on the original data. Therefore, the model fitted to the logged data is not necessarily the best on the original data.\nThe reporting of the R-squared on the logged data on a model purported to be fitted to untransformed data is just wrong as the R-squared on the log scale will be better as the variation in the data has been reduced but we really want to know how well an exponential model fits the raw data.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#logistic-function",
    "href": "labs/Lab12.html#logistic-function",
    "title": "Lab 12 - Non-linear models",
    "section": "Logistic function",
    "text": "Logistic function\n\n\\(y = A + \\frac{C}{1+e^{-B(t-M)}}\\)\nwhere the parameters are A which is the starting point or initial condition, C which is the value of y where the functions flattens out (the horizontal asymptote), M which is the value of x where the change in y is largest (always occurs at y = A + 0.5C), and 0.5B is growth rate of individual when t = M.\nCommonly used to model growth that has a sigmoid shape, i.e.¬†where growth is initially slow, then picks up to a maximum, then slows down again as the system reaches a maximum.\nLimitations:\nHarder to fit than polynomials.\n\n\n\nExercise 5: Logistic models\nIn this exercise you will model the yield of pasture over time (since sowing) using a logistic function. Since the yield at sowing = 0, the A parameter will be set to 0 and the form of the logistic function that you will use is:\n\\(y = A + \\frac{C}{1+e^{-B(t-M)}}\\)\n\nFit the model in Excel using SOLVER and in R using the nls function. Refer to previous exercises for nls function structure and implement the logistic function. You will need to specify credible starting values for the parameters (see Box below).\n\n\nPlot the fitted model with the observations.\n\n\nCompare the parameters estimated by R and Excel.\n\n\nSTARTING VALUES\nWhen fitting non-linear functions (i.e.¬†logistic or exponential) using iterative procedures such as nls or SOLVER the starting estimates of the parameters have to be approximately correct to find a solution.\nThe best way to ensure that you have suitable starting values is to plot the data with the predictions overlaid for your starting parameters. You can then see how close your initial model is to the data.\nFor the logistic model the parameters have clear meanings so suitable starting values are:\n\nC is the maximum value of y so just use the maximum value of y in the raw data as the starting value.\nM is the value of t when y = 0.5C, this can be read approximately off the graph.\nB is harder to estimate but good starting point is 0.1.\n\n\n\nThat‚Äôs it for Module 3! Great work exploring non-linear models!\nThank you all (students and demonstrators!) for your hard work and enthusiasm throughout this Module. Good luck with Project 3 and the final exam!",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab03.html",
    "href": "labs/Lab03.html",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "",
    "text": "Tip\n\n\n\nLearning Outcomes\nAt the end of this computer practical, students should be able to:\n\nCalculate Binomial probabilities\n\nUsing your calculator;\nR commands;\nR simulations.\n\nCalculate Poisson probabilities\n\nUsing your calculator;\nR commands;\nR simulations.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#notation",
    "href": "labs/Lab03.html#notation",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Notation",
    "text": "Notation\nFactorials: \\(n! = n(n - 1)(n - 2)...(3)(2)(1)\\) for \\(n \\ge 1\\) and \\(0! = 1\\).\nBinomial coefficients: \\(\\left(\\begin{matrix}n\\\\x\\end{matrix}\\right)=\\frac{n!}{x!(n-x)!}\\) for \\(x=1,2,3,...,n\\)\nThe Binomial distribution models a context in which we have a fixed number n of independent Binary trials and a fixed likelihood of a success at each trial \\(p = P(success)\\).\nX = the number of successes in n trials \\(\\sim Bin(n,p)\\)\nProbability distribution function: \\(P(X = x) = \\left(\\begin{matrix}n\\\\x\\end{matrix}\\right)p^x(1-p)^{n-x}\\) for \\(x=1,2,3,...,n\\)\nCumulative distribution function (CDF): \\(F(x) = P(X \\le x)\\).",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-1---walk-through",
    "href": "labs/Lab03.html#exercise-1---walk-through",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 1 - Walk through",
    "text": "Exercise 1 - Walk through\nWith your neighbour, discuss how coin tossing is related to the Binomial distribution?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-2---walk-through",
    "href": "labs/Lab03.html#exercise-2---walk-through",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 2 - Walk through",
    "text": "Exercise 2 - Walk through\nPracticing using the Binomial distribution formula\n\nCalculate 4!, 3!, 2! and 1!.\n\n\\[4!=4 \\times 3 \\times 2 \\times 1\\]\nNow you calculate the rest\n\nShow that\n\n\\(\\left(\\begin{matrix}4\\\\0\\end{matrix}\\right)=1\\), \\(\\left(\\begin{matrix}4\\\\1\\end{matrix}\\right)=4\\), \\(\\left(\\begin{matrix}4\\\\2\\end{matrix}\\right)=6\\), \\(\\left(\\begin{matrix}4\\\\3\\end{matrix}\\right)=4\\), \\(\\left(\\begin{matrix}4\\\\4\\end{matrix}\\right)=1\\)\n\\(\\left(\\begin{matrix}4\\\\0\\end{matrix}\\right)=\\frac{4!}{0!(4-0)!}=\\frac{4!}{4!}=1\\)\n\\(\\left(\\begin{matrix}4\\\\1\\end{matrix}\\right)=\\frac{4!}{1!(4-1)!}=\\frac{4!}{1!\\times3!}=\\frac{4\\times3\\times2\\times1}{1\\times3\\times2\\times1}=\\frac{4}{1}=4\\)\n\nNow you calculate the rest!\nNote you can also calculate this using the nCr option on your calculator - watch this YouTube video which gives a nice demonstration with some elevator music in the background to relax you.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-3---walk-through",
    "href": "labs/Lab03.html#exercise-3---walk-through",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 3 - Walk through",
    "text": "Exercise 3 - Walk through\nCollaboration - Simulate the Binomial Distribution by Coin Tossing\n\nChoose a partner and together toss a coin 4 times counting the number of heads (use Flip a coin if you don‚Äôt have a coin). Record the number of heads in an excel spreadsheet called (Simulation 1) and set out a table in excel similar to below. Now repeat this process 19 more times.\n\n\n\n\n\nSimulation\nToss 1\nToss 2\nToss 3\nToss 4\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n\n\n3\n\n\n\n\n\n\n‚Ä¶\n\n\n\n\n\n\n‚Ä¶\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\nTally up the frequency of each number of heads and fill in a table similar to the following in excel.\n\n\n\n\nNumber heads \\(x\\)\n0\n1\n2\n3\n4\nTotal\n\n\n\n\nFrequency\n\n\n\n\n\n20\n\n\n\n\nNow fill in the probability distribution in excel = the proportion of number of heads in all simulations i.e.¬†number of simulations ith 0 heads/20, number of simulations with 1 head/20, number of simulations with 2 heads/20,‚Ä¶.\n\n\n\n\nNumber heads \\(x\\)\n0\n1\n2\n3\n4\nTotal\n\n\n\n\nProbability \\(P(X=x)\\)\n\n\n\n\n\n1\n\n\n\n\nnow see if you can make a barplot in Excel of the above table Excel instructinos\n\n\n\n\nBarchart 1\n\n\n\n\n\nBarchart 2\n\n\nQuestion: With your stats partner, discuss the shape of the distribution, is it what you would expect to see and compare this to what the barplot you made in excel, are they similar?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-4---walk-through",
    "href": "labs/Lab03.html#exercise-4---walk-through",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 4 - Walk through",
    "text": "Exercise 4 - Walk through\nNow we will use R to simulate 4 coin tosses representing 1 as heads and 0 as tails. Note that I have withheld the output to avoid ‚Äúspoiling‚Äù the suprise.\n\ncoin &lt;- c(0, 1) # vector representing coin tosses c(tails, heads)\nset.seed(1) # makes sure we all get the same answer i.e. we use the same randomly generated numbers\ntosses &lt;- sample(coin, size = 4 * 20, replace = T, prob = c(0.5, 0.5)) # this randomly picks a 0 or 1 from coin and there is an equal probability of sampling both. It creates a vector of results i.e. bo tosses in total\ndim(tosses) &lt;- c(20, 4) # This creates a data frame with 4 columns and 20 rows that represents the 20 trials of 4 coin tosses\ntosses ## this prints the result\nNumHeads &lt;- rowSums(tosses) ## this sums the number of heads in each trial\nNumHeads # this prints the results\n\n\nThe table function tallies each of the numbers of heads for each simulation. In each trial (4 tosses) How many times did you toss no heads, how many times did you toss 1 head, how many times did you toss 2 heads, how many times did you toss 3 heads and how many times did you toss 4 heads?\n\n\nFreq_table &lt;- table(NumHeads)\nFreq_table\n\nThe prob.table function divides each frequency by 20 i.e.\\(P(X=0)=2/20\\)\n\nprop.table(Freq_table)\n\n\nNow we can plot this. Notice how we customise the x-axis to ensure that 0 is displayed as even though there may be no instances where we got zero heads, it is still a possibility even if the probability is low. for this we use the argument , xaxt = ‚Äún‚Äù and then draw customised axis labels using the axis function.\n\n\nplot(prop.table(Freq_table), ylab = \"P(X=X)\", xlab = \"x\", xlim = c(0, 4), xaxt = \"n\")\naxis(1, at = 0:4)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-5",
    "href": "labs/Lab03.html#exercise-5",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 5",
    "text": "Exercise 5\nRepeat exercise 4, but this time change the ‚Äòprob = c(0.5, 0.5)‚Äô to ‚Äòprob = c(0.3, 0.7)‚Äô.\nQuestion: What happens to the shape of the distribution? What type of coin does this change in probability represent?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-6",
    "href": "labs/Lab03.html#exercise-6",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 6",
    "text": "Exercise 6\nRepeat exercise 4, but this time change the set.seed(1) to set.seed(123).\nQuestion: What happens to the frequency table ‚ÄòFreq_table‚Äô why do you think it might change?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-7",
    "href": "labs/Lab03.html#exercise-7",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 7",
    "text": "Exercise 7\nSix calves were born after artificial insemination (AI) with regular semen. Assuming that the probability for either being male or female is 0.5,\nQuestion: What is the probability that all 6 are male?\n\n\\(X=\\) number of male calves \\(\\sim Bin(n=6,p=0.5)\\) \\(P(X=6)=\\left(\\begin{matrix}6\\\\6\\end{matrix}\\right)0.5^6(1-0.5)^{6-6} = 0.015625\\)\nor in R we can calculate using the dbinom function which calculates the exact probability of having 6 male calfs born\n\ndbinom(6, 6, 0.5)\n\nQuestion: What is the probability that more than 4 calves are female?\nNOTE P(more than 4 calves are female) = P(less than 2 calcves are male) = P(X=0)+P(X=1)\n\\(=\\left(\\begin{matrix}6\\\\0\\end{matrix}\\right)0.5^0(1-0.5)^{6-0}+\\left(\\begin{matrix}6\\\\1\\end{matrix}\\right)0.5^1(1-0.5)^{6-1} = 0.109375\\)\nor in R\n\n1 - pbinom(4, 6, 0.5)\n\nQuestion: What are your assumptions? What is a more accurate estimate of the P(male calf)? Beef Article\nAssumptions: Each of the births is independent and P(male calf = 0.5). (In reality, it is 1.06 males per every female born in large populations of cattle, which gives p = 0.5145631. See article link.)\nUsing the updated probability p = 0.5145631 and using R recalculate\n\nQuestion: What is the probability that all 6 are male?\n\nQuestion: What is the probability that more than 4 calves are female?\n\nQuestion: Finally, calculate what is the probability that exactly 4 females are born?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-1---walk-through-1",
    "href": "labs/Lab03.html#exercise-1---walk-through-1",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 1 - Walk through",
    "text": "Exercise 1 - Walk through\nLets take a look at an example. Let us assume that on average 5 shoppers enter a store each hour. Say the shop is open for 10 hours per day, what would a typical day look like. Assuming that the number of visits follows a Poisson distribution i.e.¬†If \\(X \\sim Po(\\lambda)\\), and because we know \\(\\lambda=5\\) and \\(n=10\\), we can simulate this in R using the following:\n\nshoppers &lt;- rpois(10, 5)\nshoppers\n\nQuestion: Take a look at the minimum and maximum number of shoppers generated by the model and discuss with your neighbour how you, as a shop owner, might use this information?\n\nQuestion: Now run the simulation for a full 5 day week, is there any change?\n\nQuestion: Now suppose as a shop owner you are concerned that your shop assistant might get overwhelmed if more than 10 shoppers come in, in any one hour period, what is the probability of this?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-2",
    "href": "labs/Lab03.html#exercise-2",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe also often say that the Poisson Distribution is good for modeling rare events. For example, recent work in Drosophila suggests the spontaneous rate of deleterious mutations is 1.2 per diploid genome.\nOpen the following article and read the abstract and then search the word poisson and read the final two sentences of this paragraph.\nNature article\nAssume that X = the number of deleterious mutations \\(X \\sim Po(1.2)\\).\nQuestion: What is the probability that an individual has 0 mutations?\n\\(P(X=0)=\\frac{1.2^0e^{-1.2}}{0!}=0.3011942\\)\nOr in R\n\ndpois(0, 1.2)\n\nQuestion: What is the probability that an individual has less than or equal to 2 mutations?\n\\(P(X\\le2)=P(X=0)+P(X=1)+P(X=2)\\) \\(=\\frac{1.2^0e^{-1.2}}{0!}+\\frac{1.2^1e^{-1.2}}{1!}+\\frac{1.2^2e^{-1.2}}{2!}\\) \\(=0.8794871\\)\nOr in R\n\nppois(2, 1.2)\n\nMore recent research found that the average spontaneous rate of deleterious mutations was actually 1.4 per diploid genome. Using R recalculate the following.\n\nQuestion: What is the new probability that an individual has 0 mutations. How does this compare to the former probability, what does this suggest?\nQuestion: What is the new probability that an individual has less than or equal to 2 mutations How does this compare to the former probability, what does this suggest?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#advanced-exercise",
    "href": "labs/Lab03.html#advanced-exercise",
    "title": "Lab 03 ‚Äì Probability distributions",
    "section": "Advanced exercise",
    "text": "Advanced exercise\nFrom the tutorial we looked at Tomato germination. Use simulations to find approximate Binomial probabilities obtain an estimate of the probability of getting 7 or more germinating seeds, by doing simulations from a \\(Bin(8, 0.7)\\) in R.\nRemember that the exact probability is\n\n1 - pbinom(6, 8, 0.7)\n\n\nFirst try 10 simulations and fill out a similar table to below in excel. What is your estimate of the probability of 7 germinating seeds?\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 10, 8, 0.7) # randomly generates 10 values from Bin(8,0.7)\nseeds ## prints seeds\ntable(seeds) / 10 # turns counts into probabilities\nbarplot(table(seeds) / 10, xlab = \"Number Germinated x\", ylab = \"Probability\", col = \"green\")\n\n\nNext try 100 simulations.\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 100, 8, 0.7) # randomly generates 100 values from Bin(8,0.7)\nseeds ## prints seeds\ntable(seeds) / 100 # turns counts into probabilities\nbarplot(table(seeds) / 100, xlab = \"Number germinated (x)\", ylab = \"Probability\", col = \"green\")\n\n\nNow we will try 10000 simulations. What do you notice about the histogram?\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 10000, 8, 0.7) # randomly generates 100 values from Bin(8,0.7)\ntable(seeds) / 10000 # turns counts into probabilities\nbarplot(table(seeds) / 10000, xlab = \"Number germinated (x)\", ylab = \"Probability\", col = \"green\")\n\n\nNow we can compare the exact results and the simulated results by adding up the probabilities for 7 and 8 seeds germinating in each of the scenarios.\n\nNote that the numbers will may differ slightly each time as they a randomly generated unless you set the same seed.\n\n\n\nNumber germinated\n\\(P(X\\ge{7}\\))\n\n\n\n\nExact method using R\n\n\n\nSimulation (n=10)\n\n\n\nSimulation (n=100)\n\n\n\nSimulation (n=10000)\n\n\n\n\nQuestion: Which of the simulations gives the closest result to the exact method, why do you think this is? - try ask chatGPT or use the # q: What is ? prompt if you have enabled copilot.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab05.html",
    "href": "labs/Lab05.html",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 1-sample t-test\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#welcome",
    "href": "labs/Lab05.html#welcome",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 1-sample t-test\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#create-a-new-project",
    "href": "labs/Lab05.html#create-a-new-project",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "Create a new project",
    "text": "Create a new project\nReminder (skip to step 2 if you are going to use the directory you created in your tutorial)\nStep 1: Create a new project file for the practical put in your ENVX1002 Folder. File &gt; New Project &gt; New Directory &gt; New Project.\nStep 2: Download the data files from canvas or using above link and copy into your project directory.\nI recommend that you make a data folder in your project directory to keep things tidy! If you make a data folder in your project directory you will need to indicate this path before the file name.\nStep 3: Open a new Quarto file.\ni.e.¬†File &gt; New File &gt; Quarto Document and save it immediately i.e.¬†File &gt; Save.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#problems-with-your-personal-computer-and-r",
    "href": "labs/Lab05.html#problems-with-your-personal-computer-and-r",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "Problems with your personal computer and R",
    "text": "Problems with your personal computer and R\nNOTE: If you are having problems with R on your personal computer that cannot easily be solved by a demonstrator, please use the Lab PCs.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#installing-packages",
    "href": "labs/Lab05.html#installing-packages",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "Installing packages",
    "text": "Installing packages\nRemember All of the functions and data sets in R are organised into packages. There are the standard (or base) packages which are part of the source code - the functions and data sets that make up these packages are automatically available when R is opened. There are also many contributed packages. These have been written by many different authors, often to implement methods that are not available in the base packages. If you are unable to find a method in the base packages, you might be able to find it in a contributed package. The Comprehensive R Archive Network (CRAN) site (http://cran.r-project.org/) is where many contributed packages can be downloaded. Click on packages on the left hand side. We will download two packages in this class using the install.packages command and we then load the package into R using the library command.\nAlternatively, in RStudio click on the Packages tab &gt; Install &gt; type in package name &gt; click install.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#normally-you-choose-0.05-as-a-level-of-significance",
    "href": "labs/Lab05.html#normally-you-choose-0.05-as-a-level-of-significance",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "1. Normally you choose 0.05 as a level of significance:",
    "text": "1. Normally you choose 0.05 as a level of significance:\nThis value is generally accepted in the scientific community and is also linked to type 2 errors where choosing a lower significance increases the likelihood of a type 2 error occurring.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#write-null-and-alternative-hypotheses",
    "href": "labs/Lab05.html#write-null-and-alternative-hypotheses",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "2. Write null and alternative hypotheses:",
    "text": "2. Write null and alternative hypotheses:\n\n\nQuestion: Write down the null hypothesis and alternative hypotheses:\nH0: &lt; Type your answer here &gt;\nH1: &lt; Type your answer here &gt;",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#check-assumptions-normality",
    "href": "labs/Lab05.html#check-assumptions-normality",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "3. Check assumptions (normality):",
    "text": "3. Check assumptions (normality):\n\na. load data:\nMake sure you set your working directory first\n\n# Type your R code here\n\nIt is always good practice to look at the data first to make sure you have the correct data, it loaded in correctly and know what the names of the columns are. This can be done by typing the name of the data Milk or for large datasets, use str() to show the first 6 lines:\n\n# Type your R code here\n\n\n\nb. Tests for normality:\nqqplots:\n\n# Type your R code here\n\nHistogram and boxplots:\n\n# Type your R code here\n\n\n\nQuestion: Do the plots indicate the data are normally distributed?\nAnswer: &lt; Type your answer here &gt;\n\n\nShapiro-Wilk test of normality:\n\n# Type your R code here\n\n\n\nQuestion: Does the Shapiro-Wilk test indicate the data are normally distributed? Explain your answer.\nAnswer: &lt; Type your answer here &gt;",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#calculate-the-test-statistic",
    "href": "labs/Lab05.html#calculate-the-test-statistic",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "4. Calculate the test statistic",
    "text": "4. Calculate the test statistic\nIn R we achieve this via the command t.test(milk$Yield, mu = ‚Ä¶) The R output first gives us the calculated t value, the degrees of freedom, and then the p-value, it then provides the 95% CI and the mean of the sample. Were mu = ‚Ä¶ is written enter in the hypothesised mean.\n\n# write your R code here\n\n\n5. Obtain P-value or critical value\n\n\nQuestion: Does the hypothesised economic threshold lie within the confidence intervals?\nAnswer: &lt; Type your answer here &gt;\n\n\n\n\n6. Make statistical conclusion\n\n\nQuestion:: Based on the P-value, do we accept or reject the null hypothesis?\nAnswer: &lt; Type your answer here &gt;\n\n\n\n\n7. Write a scientific (biological) conclusion\n\n\nQuestion:: Now write a scientific (biological) conclusion based on the outcome in 6.\nAnswer: &lt; Type your answer here &gt;",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#thanks",
    "href": "labs/Lab05.html#thanks",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "Thanks!",
    "text": "Thanks!",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#exercise-1-carrots",
    "href": "labs/Lab05.html#exercise-1-carrots",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "Exercise 1: Carrots",
    "text": "Exercise 1: Carrots\nA farmer is growing carrots for a restaurant. The restaraunt wants their carrots to be 10 cm long, so the farmer wants to check if the carrots in their field differ significantly from the needed length.\n\n#Read in data\n\ncarrots &lt;- c(7, 7, 13, 5, 13, 10, 11, 12, 10,  9)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#exercise-2-penguins",
    "href": "labs/Lab05.html#exercise-2-penguins",
    "title": "Lab 05 ‚Äì Hypothesis Testing",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nRey has just landed on earth and notived that penguins look really similar to porgs. Using weight as the point of comparison, she wants to know if two different penguin species weigh the same as her pet Porg Stevie, who weighs 4000g.\nWe will be using the Palmer penguin dataset to test if chinstrap and gentoo penguins weigh the same as Stevie.\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n\n2.1 Chinstrap\n\nchinstrap &lt;-  penguins%&gt;%\n  filter(species == \"Chinstrap\")%&gt;%\n  na.omit()\n\n\n\n2.2 Gentoo\n\ngentoo &lt;-penguins%&gt;%\n  filter(species == \"Gentoo\")%&gt;%\n  na.omit() \n\n\n\nAttribution\nThis lab was developed using resources that are available under a Creative Commons Attribution 4.0 International license, made available on the SOLES Open Educational Resources repository.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab11.html",
    "href": "labs/Lab11.html",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to perform MLR and interpret the results using R;\nUndertake hypothesis testing to determine if model is significant\nUndertake hypothesis testing to determine if the true partial regression slope \\(\\neq\\) 0\nCheck assumptions are filled prior to assessing model output\nAssess model summary in terms of fit and P-values\nConsider more parsimonious models",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#before-you-begin",
    "href": "labs/Lab11.html#before-you-begin",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-11.Rmd or similar. The following data files are required:\n\nENVX1002_wk11_practical_data_Regression.xlsx\n\nLast week you explored simple linear regression and assessed the output of your models.\nThis week we will build upon this and venture into multiple linear regression.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon‚Äôt forget to save as you go!",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-1-corn-yields",
    "href": "labs/Lab11.html#exercise-1-corn-yields",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 1: Corn yields",
    "text": "Exercise 1: Corn yields\nData: Corn spreadsheet\nIn this data\n\ny = P content of corn\n\nx1 = inorganic P content of soil\n\nx2 = organic P content of soil\n\nn = 17 sites (The original data had 18 sites, one is removed here.)\n\nAim of our investigation: Understand the relationship between Organic and Inorganic phosphorous contents in the soil, and the phosphorous content of corn. This will allow us to see which type of phosphorous is being taken up by the corn.\n\nlibrary(readxl)\nCorn &lt;- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Corn\")\nhead(Corn)\n\n\n\n1.1 Examine the correlations\nSome people find it difficult to visually interpret graphical summaries of data in more than 2 dimensions; however, 3-dimensional surface plots are reasonably common in statistics although not usually in descriptive statistics.\nInitially we will examine the pairwise correlations to ‚Äúget a feel‚Äù for the data. We will then make a 3-dimensional surface plot using the lattice package .\nUsing R, we can calculate the correlation matrix quite easily.\nNote the use of round() to limit the number of significant digits.\n\nround(cor(Corn),3)\n\n\nWhat do the results of the correlation matrix tell you?\n\n\nBased on the correlation matrix, if we were to fit a single predictor model involving EITHER InorgP OR OrgP, then which model would be more successful?\n\n¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(Hint, the r2 is exactly that for a single predictor regression, the square of the correlation, r).\nThe pairs plot creates scatterplots between each possible pair of variables. Like a single scatterplot the pairs plot allows us to visually observe any trends.\n\nObserving the pairs plot below, do you see any strong trends? How well does this link to your correlation matrix?\n\n\npairs(Corn)\n\n\n\nSimple 3-D plot\nUnlike a simple plot we can creat for a simple linear regression, it is a bit more complex to visualise a model with more predictors. One way we can visualise the relationship is with a 3-D plot, which can be made using the function levelplot() in lattice.\nHere we plot the OrgP and InorgP in the axes and the levels in the plot are CornP.\nNote the package Viridis has been called, this is through personal choice.\nThe Viridis package has a range of assembled colour ramps which are easier for the reader to differentiate the colours, especially when printed in grayscale, or if the reader is colourblind.\n\nlibrary(lattice, quiet = T)\nlibrary(viridis, quiet = T) # will need to install.packages(\"viridis\")   \n\nlevelplot(CornP ~ InorgP + OrgP, data = Corn\n          , col.regions = viridis(100))\n\nThe level plot shows us the x (InorgP) and y variable (OrgP), with the colour scale representing the z variable, which in this case is Phosphorous being taken up by the corn (cornP). From the plot we can see that with higher levels of orgP and inorgP in the soil, the Phosphorous content in the corn is generally higher.\nIt is clear that the 3-D surface plot does not have colours everywhere, but this relates of course to the underlying data. In this case we don‚Äôt have continuous data in both directions, so the response (the colour) is only plotted where we have input variables.\nIf we did have continuous data in both directions the plot would look more like a heatmap, here are some examples.\n\n\n\n\n1.2 Fit the model\nWe will now use regression to estimate the joint effects of both inorganic phosphorus and organic phosphorus on the phosphorus content of corn.\n\\(CornP = \\beta_0 + \\beta_1 InorgP + \\beta_2 OrgP + error\\)\nThis is fairly simple and follows the same structure as simple linear regression and uses lm().\n\nMLR.Corn &lt;- lm(CornP ~ InorgP + OrgP, data=Corn)\n\n\n\n\n1.3 Check assumptions\nLet‚Äôs check the assumptions of regression are met via residual diagnostics.\n\nAre there any apparent problems with normality of CornP residuals or equality of variance for this small data set?\n\n\npar(mfrow=c(2,2))\nplot(MLR.Corn)\n\n\n\n\n1.4 Model output\nAfter checking our assumptions and we are happy with them, we can interpret our model output.\n\nIncorporating the partial regression coefficient estimates, what is the model equation?\n\n\nsummary(MLR.Corn)\n\n\nSimple linear regression allowed us to describe the relationship incorporating our regression coefficient estimate. We would interpret it as follows:\n*‚ÄúAs* \\(x\\) increases by 1, \\(y\\) decreases by \\(b_1\\) units‚Äù (depending on the direction of the relationship).\nThis week it is a bit different because we are dealing with partial regression coefficients instead.\nInstead, we would say:\n*‚Äúas* \\(x_1\\) increases by 1, \\(y\\) decreases by \\(b_1\\) units given all other partial regression coefficients are held constant‚Äù.\nApplied to our model, if we wanted to describe the relationship between InorgP and CornP, we would say:\n‚ÄúAs InorgP increases by 1, CornP increases by 1.2902, given OrgP is held constant.‚Äù\n\nGiven the above, how would you interpret the relationship between OrgP and CornP?\n\n\n\n\n1.5 Is the model useful?\nRemember now that the F-test and T-test are testing slightly different things.\n\n\nF-test\n\\(H_0:\\) all \\(\\beta_k = 0\\), i.e.¬†\\(\\beta_1 = \\beta_2 = 0\\)\n\\(H_1:\\) at least 1 \\(\\beta_k \\neq 0\\), i.e.¬†our model is significant\nWe will find the P-value for this test at the end of the summary output.\n\n\n\nt-test\n\\(H_0: \\beta_k = 0\\)\n\\(H_1: \\beta_k \\neq 0\\)\nWhere \\(\\beta_k\\) refers to one of the model partial regression coefficients. In the case of our model we only have 2: \\(b_1\\) (InorgP) and \\(b_2\\) (OrgP).\n\nNow it is your turn:\n\nLooking at the summary() output, is our overall model significant?\n\n\nWhich independent variable is a significant predictor of corn yield?\n\n\n\n\n\n1.6 How good is the model?\n\nHow much of the variation in CornP content is explained by the two independent variables?\n\n\nRun the model again but this time with only the significant independent variable. How do the model performance criteria (r2-adj, P-values, Residual Standard Error) change?\n\n\n\n\n1.7 Our conclusions\nWriting up conclusions for multiple linear regression are similar to simple linear regression, just with a couple of extra P-values to state.\nWe would first mention that our overall model is significant as we rejected the null hypothesis (P = 0.05). We could then describe the hypothesis test results for our predictor variables. Finally, we would describe the model fit and our adjusted-r2.\nRemember the scientific conclusion then relates our findings back to the context, answering aims.\n\nWhat would our statistical conclusion be?\n\n\nWhat would our Scientific conclusion be?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-2-water-quality",
    "href": "labs/Lab11.html#exercise-2-water-quality",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 2: Water quality",
    "text": "Exercise 2: Water quality\nData: Microbes spreadsheet\nThis exercise will use data from the NOAA Fisheries data portal. The dataset contains the results of microbial study of Pudget Sound, an estuary in Seattle, U.S.A.\nThe dataset contains the following variables:\n\ntotal_bacteria = Total bacteria (cells per ml) ‚Äì&gt; this will be our response variable\nwater_temp = Water temperature (¬∞C)\ncarbon_L_h = microbial production (¬µg carbon per L per hour)\nNO3 = Nitrate (¬µm)\n\nFirst thing to do, is read in the data. This time we will be using the Microbes spreadsheet:\n\nmic &lt;- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Microbes\")\n\n# Check the structure\nstr(mic)\n\n\n\n2.1 Examine the correlations\nFor this dataset we may expect to see some correlations;\n\nWarmer water temperature we would expect to see a higher amount of bacterial growth\nCarbon is a proxy for microbial production, so if we see a higher rate of carbon production, we would expect to see higher levels of bacteria\nNO3 (Nitrate) is an essential nutrient for plants and some bacteria species metabolise this\n\n\nLet‚Äôs test this. Observe the correlation matrix and pairs plots. Do you notice any strong correlations?\n\n\ncor(mic)\n\npairs(mic)\n\n\n\n\n2.2 Fit the model\nWe can now fit the model to see how much these predictors account for the variation in total bacteria.\n\\(total bacteria = \\beta_0 + \\beta_1 watertemp + \\beta_2 carbon + \\beta_3 NO3 + error\\)\nThere are two forms the lm code can take; you can either specify which variables you want to include by naming each one, or if only your desired variables are within your dataset, you can use the ~. to specify all columns.\n\nnames(mic) # tells us column names within the dataset\n\n# Form 1:\nmic.lm &lt;- lm(total_bacteria ~ water_temp + carbon_L_h + NO3, data = mic)\n\n# Form 2\nmic.lm &lt;- lm(total_bacteria ~ ., data = mic)\n\n\n\n\n2.3 Check assumptions\nLet‚Äôs check the assumptions of regression are met via residual diagnostics.\n\nAre there any apparent problems with normality of total_bacteria residuals or equality of variance for this data set?\n\n\npar(mfrow=c(2,2))\nplot(mic.lm)\n\n\n\n\n2.4 Model output\nAfter investigating the assumptions, they seem to be ok, so we can move onto the model summary.\n\nsummary(mic.lm)\n\n\nIncorporating the partial regression coefficient estimates, what is the equation for this model?\n\n\nLike you did in Exercise 1.4, how would you interpret the relationship between total_bacteria and water_temp?\n\n\n\n\n2.5 Is the model useful?\n\nObserving the P-value of the F-statistic in the summary, can we say our model is significant?\n\n\nAre any predictors significant?\n\n\n\n\n2.6 How good is the model?\n\nHow much of the variation in total bacteria is explained by the three independent variables?\n\n\nRun the model again but this time excluding the variable with the largest P-value. How do the model performance criteria (r2-adj, P-values, Residual Standard Error) change?\n\n\nsummary(lm(total_bacteria ~ water_temp + NO3, data = mic))\n\n\n\n\n2.7 Conclusions\n\nWhat would the statistical conclusion be for this model?\n\n\nWhat would our scientific conclusion be?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-3-dippers",
    "href": "labs/Lab11.html#exercise-3-dippers",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 3: Dippers",
    "text": "Exercise 3: Dippers\nData: Dippers spreadsheet\nWe will revisit the Dippers dataset from last week, but now incorporating other factors which may be influencing the distribution.\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\nTwenty-two sites were included in the survey. Variables are as follows\n\nwater hardness\nriver-bed slope\nthe numbers of caddis fly larvae\nthe numbers of stonefly larvae\nthe number of breeding pairs of dippers per 10 km of river\n\nIn the analyses, the four invertebrate variables were transformed using a Log(Number+1) transformation.\n\nNow it is your turn to work through the steps as above. What other factors are influencing the number of breeding pairs of Dippers?\n\nRead in the data from today‚Äôs Excel sheet, the corresponding sheet name is ‚ÄúDippers‚Äù\n\n\nInvestigate a correlation matrix and pairs plot of the dataset, are there signs of a relationship between breeding pair density and other independent variables?\n\n\npairs(Dippers)\ncor(Dippers)\n\n\nLet‚Äôs investigate further. Run the model incorporating all of our predictors, but before looking at our model output, are the assumptions ok?\n\n\n# Run model\ndipper.lm &lt;- lm(Br_Dens ~ ., data=Dippers)\n\n# Check assumptions\npar(mfrow = c(2,2))\nplot(dipper.lm)\n\n\nOnce you are happy assumptions are good, you can interpret the model output using summary().\n\nWhat is the equation for our model, incorporating our partial regression coefficients?\n\n\nBased on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd this time?\n\n\nIs LogCadd still a significant predictor of Dipper breeding pair density?\n\n\nWhat are the significant predictors of this model?\n\n\nHow good is the fit of our model?\n\n\nWhat might we do to improve the model fit?\n\n\nWhat statistical and scientific conclusions can we make from this model output?\n\n\nAnother week of linear models done! Great work fitting Multiple Linear Regression! Next week we break away and explore non-linear functions.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-1-house-prices",
    "href": "labs/Lab11.html#exercise-1-house-prices",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 1: House Prices",
    "text": "Exercise 1: House Prices\nData: - Housing\nThis exercise will use a dataset from kaggleto explore the variables affecting house prices.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-2-energy-use",
    "href": "labs/Lab11.html#exercise-2-energy-use",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 2: Energy use",
    "text": "Exercise 2: Energy use\nData:\n\nenergy\n\nUse the dataset from kaggle to explore which variables affect energy consumption.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-3-sales-vs-advertising-budget",
    "href": "labs/Lab11.html#exercise-3-sales-vs-advertising-budget",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 3: Sales vs advertising budget",
    "text": "Exercise 3: Sales vs advertising budget\nData:\n\nadvertising budget\n\nUse the dataset from kaggle to explore how different advertising budgets affect sales.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab10.html",
    "href": "labs/Lab10.html",
    "title": "Lab 10 - Linear Functions",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nFit simple linear models and obtain associated model summaries in R\nOverlay fitted models onto scatterplots in R\nUndertake hypothesis testing to determine if slope \\(\\neq\\) 0\nCheck assumptions are met prior to assessing model output\nAssess model summary in terms of fit and P-values",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#before-you-begin",
    "href": "labs/Lab10.html#before-you-begin",
    "title": "Lab 10 - Linear Functions",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-10.Rmd or similar. The following data files are required:\n\nENVX1002_wk10_practical_data_Regression.xlsx\n\nLast week you fitted models in R, now it is time to understand what the output means.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon‚Äôt forget to save as you go!",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-1-walkthrough---fertiliser",
    "href": "labs/Lab10.html#exercise-1-walkthrough---fertiliser",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 1: Walkthrough - Fertiliser",
    "text": "Exercise 1: Walkthrough - Fertiliser\nLike last week, we will start off our R modelling journey by fitting a model to the fertiliser data.\n\nRead the following code into R:\n\n\n# add the data to R Studio\nfert &lt;- c(0, 1, 2, 3, 4, 5)\nyield &lt;- c(2, 13, 19, 18, 25, 33)\n\n\n1.1 Scatterplot and correlation\nTo visually identify any trends or relationships, last week we created a scatterplot of the data. This is helps us visually understand our points so we know what we might expect from the model, and possibly identify if the relationship is looking non-linear.\n\n# Create a scatterplot\nplot(fert, yield)\n\nRemembering back to last week, we then calculated the correlation coefficient to numerically determine whether there was a relationship between fertiliser and yield.\nUsing the code below, we found there was quite a strong relationship between fertiliser and yield (0.964):\n\n# Correlation coefficient\ncor(fert, yield)\n\n\n\n1.2 State hypotheses\nRemembering back to the lecture and tutorial, the general equations for our hypotheses are:\n\\[\nH_0 : \\beta_1 = 0\n\\]\n\\[\nH_1 : \\beta_1 \\neq 0\n\\]\nIn the context of our data, the hypotheses would be:\n\\(H_0\\): Slope = 0; fertiliser is not a significant predictor of yield.\n\\(H_1\\): Slope \\(\\neq\\) 0; fertiliser is a significant predictor of yield.\nIf P &gt; 0.05, we fail to reject the null hypothesis that the true slope (\\(\\beta_1\\)) is equal to 0. If this is the case, it means our model does not predict better than the mean of our observations, and so there is no advantage to using our model over the mean of y (\\(\\bar{y}\\)).\nIf we find there is a high probability of the slope not being equal to 0 (P &lt; 0.05), we can reject the null hypothesis and conclude our model is better at predicting than the mean of our observations.\nNow we understand what we are testing for, we can fit the model.\n\n\n1.3 Fit the model\nAfter checking the correlations and scatterplot, we need to fit the model using the lm() function. Remember to tell R the name of the object you want to store it as (in this case, model.lm &lt;-), then state the name of the function. The arguments within the function (i.e.¬†between the brackets) will be yield ~ fert, with yield being the response variable and fert being the predictor.\n\n# Run your model\n## yield = response variable (x)\n## fert = predictor variable (y)\nmodel.lm &lt;- lm(yield ~ fert)\n\n\n\n1.4 Check assumptions\nThis time, before obtaining our model summary, we need to check our assumptions.\nSmaller sample size (n = 6) makes it harder to check whether the assumptions have been met, but we will still run through the check.\nLooking at each plot, we can see that the residual plots don‚Äôt look the best;\n\nResiduals vs fitted: Will tell us if the relationship is linear. We are looking for an even scatter around the mean, and red line should be reasonably straight. In this case the red line is not too straight, but the scatter seems even.\nNormal Q-Q: If the residuals are normally distributed, most of the points should lie along the dotted line. Our points follow the line, but do not lie on it.\nScale-Location: This is for testing whether the variance is equal in the residuals at each value of x. If the variance is equal, then we would expect to see an even scatter and no fanning. In this case, there is no fanning.\nResiduals vs Leverage: This will help us identify whether there are any single points influencing the slope or intercept of the model. We can see in the output plot there is a point sitting in the bottom-right corner, outside the dotted line, indicating that it may be having an influence on the model.\n\nThese plots are only useful as an example of how to obtain and interpret output. If we wanted to obtain a more reliable check of our assumptions (and a more reliable model), we would need a larger sample size (n &gt; 10).\n\n# Check your assumptions!!\npar(mfrow = c(2, 2)) # sets plots to show as 2x2 grid\nplot(model.lm)\n\nIn this case, we will assume the assumptions have been met and continue to assess the model output.\n\n\n1.5 Model output\nUse the summary() function to obtain output for your model:\n\n# Obtain model summary\nsummary(model.lm)\n\nIn the model output obtained from summary(model.lm) the model parameters will be listed under ‚ÄòEstimate‚Äô for the intercept and ‚Äòfert‚Äô. Last week we concluded the equation to be:\n\\[\nYield = 4.7619 + 5.4286*fert\n\\]\nFurthermore, from our model estimate, we can say that as fertiliser increases by 1, yield will increase by 5.4286.\n\n\n1.6 Is the model useful?\nWhen looking at the model summary output, we obtain the p-value from the coefficients table. We are interested in the P-value for fert and not the intercept.\nThe significance of the intercept P-value depends on our scientific question. We only really look at our intercept P-value when we want to extrapolate our line to the intercept, and know if the intercept is equal to zero (\\(H_0\\)) or not (\\(H_1\\)). This depends on your dataset and whether it makes sense to do so.\nAlso notice how the p-values for the F-test at the bottom of the summary output, and the t-test p-values we are using are the same. The F-test gives us an idea whether our overall model is significant and in this case, as we are only using a single predictor, the P-values will be the same.\nTherefore we can say the following:\nObserving the model output, we can see that the P-value for fert is significant (P = 0.00196) and we can say that as P &lt; 0.05, we reject the null hypothesis. We can conclude our slope is not 0 and our model is a better way to predict yield than the mean of our observations.\n\n\n1.7 How good is the model?\nTo assess how well the model fits the data, we need to look at the Residual standard error (3.147) and the r-squared value (0.9287).\n\nWe can say that our residual standard error is relatively low in terms of our response variable.\nOur r-squared indicates that fertiliser accounts for 92.9% of variation in yield. That‚Äôs pretty good!\n\nNote how Multiple R-squared and Adjusted R-squared are similar. For simple linear models we can opt for the multiple r-squared, but when using multiple predictors we need to use adjusted r-squared.\nFinally, to visually present our results, we can provide a scatterplot with the model overlaid.\n\n# Add the linear model to your scatterplot\nplot(fert, yield, xlab = \"fertiliser applied\", ylab = \"Yield\")\nabline(model.lm, col = \"red\")\n\n\n\n1.8 Our conclusions\nNow we can put our interpretations together to form the conclusion:\nObserving the model output, we can see that the P-value for fert is significant (P = 0.00196) and we can say that as P &lt; 0.05, we reject the null hypothesis. We can conclude our slope is not 0 and our model is a better way to predict yield than the mean of our observations.\nWe can therefore conclude that fertiliser is a significant predictor of crop yield as the slope is not equal to zero (P &lt; 0.05), and it accounts for 92.9% of the variation in yield.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-2-toxicity-in-peanuts",
    "href": "labs/Lab10.html#exercise-2-toxicity-in-peanuts",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 2: Toxicity in peanuts",
    "text": "Exercise 2: Toxicity in peanuts\nData: Peanuts spreadsheet\nThe data comprise of, for 34 batches, the average level of the fungal contaminant aflatoxin in a sample of 120 pounds of peanuts and the percentage of non-contaminated peanuts in the whole batch.\nThe data were collected with the aim of being able to predict the percentage of non-contaminated peanuts (Peanuts$percent) from the aflatoxin level (Peanuts$toxin) in a sample. We will now investigate whether this is the case.\nFirst thing‚Äôs first! Let‚Äôs read in the data using read_xlsx command:\n\nlibrary(readxl)\nPeanuts &lt;- read_xlsx(\"data/ENVX1002_wk10_practical_data_Regression.xlsx\", sheet = \"Peanuts\")\nhead(Peanuts)\n\n\n2.1 Scatter plot\nMake a scatter plot of the data.\n\nplot(Percent ~ Toxin, data = Peanuts)\n#Alternate syntax:\n#plot(Peanuts$Percent, Peanuts$Toxin)\n\n\nDescribe the relationship between the two variables.\n\n\nWould you say that the percentage of non-contaminated peanuts in a batch could be predicted accurately from the level of aflatoxin in a sample via a linear relationship?\n\n\n\n2.2 State Hypotheses\n\nWhat are the hypotheses we are testing? State them as the formulae and in the context of the study.\n\n\n\n2.3 Fit a linear model\nUse fit a linear model (lm()) to the Peanut data.\n\n# fit a linear model using lm()\nmod &lt;- lm(Percent ~ Toxin, data = Peanuts)\n\n\n\n2.4 Check assumptions\n\nInspect and comment on the residual plots- have the assumptions been met?\n\n\npar(mfrow = c(2, 2))\nplot(mod)\n\n\n\n2.5 Observe model output\nOnce you are certain the assumptions are met, you can proceed to look at the regression output.\n\nComment on the overall fit of the regression, i.e.¬†Is the model fit good? Is the model significant, and how much variation in percentage of non-contaminated peanuts does aflatoxin level account for?\n\n\n# Look at output with summary\nsummary(mod)\n\n\nIs toxin a significant predictor of percentage non-contaminated peanuts? If so, how can we tell?\n\n\nInterpret the slope parameter in terms of quantifying the relationship between toxin and percent.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-3-dippers",
    "href": "labs/Lab10.html#exercise-3-dippers",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 3: Dippers",
    "text": "Exercise 3: Dippers\nData: Dippers spreadsheet\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\nTwenty-two sites were included in the survey. For the purpose of fitting a simple linear model, the dataset has been reduced to two variables:\n\nThe number of breeding pairs of Dippers per 10 km of river\nThe numbers of caddis fly larvae (Log(Number+1) transformed)\n\nNow it is your turn to work through the steps as above. Does the number of caddis fly larvae influence the number of breeding pairs of Dippers?\n\nRead in the data from today‚Äôs Excel sheet, the corresponding sheet name is ‚ÄúDippers‚Äù\n\n\nObtain a scatterplot, are there signs of a relationship between breeding pair density and caddis fly larvae?\n\n\nWhat are the hypotheses we are testing? State them as the formulae and in the context of the study.\n\n\nLet‚Äôs investigate further. Run the model, but before looking at our model output, are the assumptions ok?\n\n\n# Run model\ndipper.lm &lt;- lm(Br_Dens ~ LogCadd, data = Dippers)\n\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(dipper.lm)\n\n\nOnce you are happy assumptions are good, you can use summary() to interpret the model output.\n\n\nWhat is the equation for our model, incorporating our coefficients?\n\n\nBased on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd?\n\n\nIs LogCadd a significant predictor of Dipper breeding pair density? How can we tell?\n\n\nHow good is the fit of our model?\n\n\nWhat conclusions can we make from this model output?\n\n\nA final thought; Does our result make sense within the context? i.e.¬†why might the Dipper breeding pair density be related to LogCadd?\n\nGreat work fitting simple linear models! Next week we step it up with multiple linear regression.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-1-cars-stopping-distance",
    "href": "labs/Lab10.html#exercise-1-cars-stopping-distance",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 1: Cars stopping distance",
    "text": "Exercise 1: Cars stopping distance\nUse the cars dataset from last week to test if speed (mph) is a predictor of stopping distance (fft).\n\nhead(cars)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-2-penguins",
    "href": "labs/Lab10.html#exercise-2-penguins",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nUse the palmer penguins dataset to test if flipper length is a significant predictor of bill length.\n\n#Load libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n#Clean data\npenguins &lt;- penguins%&gt;%\n  na.omit()#remove missing data\n\nhead(penguins)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-3-old-faithful-geyser-data",
    "href": "labs/Lab10.html#exercise-3-old-faithful-geyser-data",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 3: Old Faithful Geyser Data",
    "text": "Exercise 3: Old Faithful Geyser Data\nUsing the inbuilt faithful dataset, test whether waiting time is a significant predictor of eruption time.\n\nhead(faithful)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab09.html#before-you-begin",
    "href": "labs/Lab09.html#before-you-begin",
    "title": "Lab 9 - Describing relationships",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-09.Rmd or similar. The following data files are required:\n\nENVX1002_practical_data_Regression.xlsx",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-1-linear-modelling-in-excel",
    "href": "labs/Lab09.html#exercise-1-linear-modelling-in-excel",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 1: Linear Modelling in Excel",
    "text": "Exercise 1: Linear Modelling in Excel\nThis exercise focusses on fitting the model parameters and demonstrating two ways a model can be fitted - numerical or analytical;\n\nAnalytical: equation(s) are used directly to find solution, e.g.¬†estimate parameters that minimise residual sum of squares\nNumerical: computer uses ‚Äúrandom guesses‚Äù to find set of parameters to that minimises objective function, in this case residual sum of squares\n\nWe mostly use R for modelling, but R does everything automatically. It is important to know what is going on ‚Äòbehind the scenes‚Äô, which is why we are starting in Excel. Similar to the tutorial, you will be calculating each component of the model parameter step by step in the exercises that follow.\n\n\n1.1 Horses\nThis is our example of Analytical fitting method.\nThe number of horses on Canadian farms appeared to decrease after the war:\n\nTo see whether this is likely to be true, fit a model to the above data ‚Äòby hand‚Äô in Excel. To aid the calculation it is recommended to fill out the Excel table provided ENVX1002_practical_data_Regression.xlsx, you can find it in the spreadsheet labelled Horses.\n\nThe table we have provided in Excel has broken the regression parameter equations (b0, b1) into smaller components so you can understand the underlying mechanisms and where these values come from.\n\nPlot the two variables in Excel and fit a line. You can fit a number of models in Excel simply by right clicking on the scatter of points clicking Add Trendline ‚Ä¶. Within the add Tendline window (see screenshot below), a number of options are given, here we want Linear and we want to tick display the Equation and Display r-squared on the chart.\n\n\n\n\n\nScreenshot: Format Trendline\n\n\n\nThe R-squared value is a measure of how well the model fits the data where 1.0 is a perfect fit; we will discuss this more in Week 10. The values which appear in the model equation should be the same as those obtained in your earlier calculations.\n\nAlthough it is important for the model equation, do you think the intercept provides a realistic value in this particular case? What does it mean?\n\n\nCalculate the correlation coefficient using the =CORREL function in Excel. Type =CORREL( and highlight the Year column, and then after a comma highlight the Horses column and close the brackets.\n\n\nIf the relationship was non-linear would this would be a good statistic to use to describe the relationship between horse and years? Explain your answer.\n\n\n\n\n1.2 Fertiliser data\nThis is our example of numerical fitting of a model.\nFigure 1 shows a plot of yield against fertiliser where a linear model is fitted through the scatterplot of raw observations. Intuitively you would draw this as a line that comes as close to possible to all observations which you may have come across as a ‚Äòline of best fit‚Äô. In this exercise we will explore how models can be fitted automatically based on least-squares estimation.\n\n\n\n\nFigure 1: Plot of Yield-response to fertiliser\n\n\n\nIn Figure 1 you will notice that the line does not fit the data perfectly which is typical of biological and environmental data. A measure of how far the model is from the data is the residual.\n\\[\\begin{equation}\nresidual = y_i - \\hat{y}_1\n\\label{1}\n\\end{equation}\\]\nWhere \\(y_i\\) is the observed value for the ith observation and \\(\\hat{y}_1\\) is the predicted value for the ith observation. In this case the predicted value is based on the linear model.\nIf we add up the square of the residuals for the n observations we get something called the Residual Sum of Squares (\\(SS_{res}\\)):\n\\[\\begin{equation}\nSS_{res} =\\sum_{i = 1}^{n} (y - \\hat{y})^2 \\label{2}\n\\end{equation}\\]\nThe best fitting model will have the smallest RSS. The general method is called least-squared estimation. We will now use Excel to find the optimal model.\nEnter values of 2 for the y-intercept (\\(b_0\\)) and 3 for the slope (\\(b_1\\)) in cells H2:H3. These are the initial guess values.\n\nNow use these parameter values to create predictions for each value of fertiliser in the Predicted column.\n\nMake sure that rather than writing in the value ‚Äò2‚Äô and ‚Äò3‚Äô for your predicted column, you refer to cells H2 and H3 (write as $H$2 and $H$3, see screenshot below). Once you have completed the equation, you can apply the equation to the other rows by clicking on the small box at the bottom right corner of the cell and drag it down the rows. Writing dollar signs into your references to H2 and H3 prevents your equation from moving down the row column.\n\n\n\n\nScreenshot of Predicted column input\n\n\n\n\nUse this information to calculate (i) residuals (ii) residuals2 (iii) RSS.\nCreate a plot similar to Figure 1 where the observations are plotted as symbols and the model predictions are a line. You should have your spreadsheet set up so that if you change the values of the parameters the plotted line changes as well. Try to fit the line manually. This can be difficult, especially for non-linear models.\nFollow instructions provided in the Tutorial, or in the file How to install Solver to ensure you have Solver ready to use in Excel.\n\nOnce you have added Solver, click on the tab Data &gt;&gt; Solver, and you will see the following (see screenshot below). For Set Objective, you need to select the cell where your RSS value has been calculated. We wish to minimize this so we click on Min, and we do this by Changing Cells where the parameters of the model are found, in this case the y-intercept and slope. Before clicking Solve, make sure you can see your calculated values so you can see how your how it all changes.\n\n\n\n\nScreenshot of solver with input values\n\n\n\nWhen ready, click on Solve and it should find a solution for the minimum RSS. Solver uses an iterative procedure to find the minimum RSS which means it successively guesses values until it finds the optimal value. This is a numerical solution to the problem of model fitting.\nYour ‚ÄòSOLVED‚Äô parameters should be the same as what appears in your trendline equation.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-2-fitting-a-model-in-r",
    "href": "labs/Lab09.html#exercise-2-fitting-a-model-in-r",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 2: Fitting a model in R",
    "text": "Exercise 2: Fitting a model in R\nNow we have a deeper understanding of what is going on behind the scenes, we can fit linear models in R.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon‚Äôt forget to save as you go!\n\n\n2.1 Have a go - Fertiliser data\nYou will use the fertiliser data to fit a linear model in R. As we covered fitting linear models in the Tutorial, it is now your turn to have a go at fitting the models (with some hints along the way).\n\nRead the following code into R:\n\n\n# add the data to R Studio\nfert &lt;- c(0, 1, 2, 3, 4, 5)\nyield &lt;- c(2, 13, 19, 18, 25, 33)\n\n\nTo visually identify any trends or relationships, create a scatterplot of fertiliser vs yield. From the scatterplot you see, are there any relationships or trends evident?\n\n\n# Create a scatterplot\nplot(fert, yield)\n\n\nTo numerically determine whether there is a relationship, calculate the correlation coefficient. (assume data is normally distributed). Does the correlation coefficient indicate a relationship between fertiliser and yield?\n\n\n# To calculate the correlation coefficient:\ncor(fert, yield)\n\n\nYou can now fit the model in R using the lm() function. Remember to tell R the name of the object you want to store it as (in this case, model.lm &lt;-), then state the name of the function. The arguments within the function (i.e.¬†between the brackets) will be yield ~ fert, with yield being the response variable and fert being the predictor.\n\n\n# Run your model\n## yield = response variable (x)\n## fert = predictor variable (y)\nmodel.lm &lt;- lm(yield ~ fert)\n\n# Obtain model summary - In here you can obtain the model parameters\n# Look for Intercept Estimate and fert Estimate\nsummary(model.lm)\n\n\nIn the model output obtained from summary(model.lm) the model parameters will be listed under ‚ÄòEstimate‚Äô for the intercept and ‚Äòfert‚Äô. Compare these values to what you have calculated in Excel.\n\n\nBased on this output, what would the model equation be? Does it match your findings in Excel?\n\n\nYou can now fit your model to the scatterplot you created previously using the abline() function. Make sure you run the plot function and the abline function in one go. If the lines are run separately, an error may appear saying ‚Äúplot.new hasn‚Äôt been called yet‚Äù; this is because the abline function requires a current plot on which it can overlay the line.\n\nAlso remember, when presenting plots (e.g.¬†in a report), they should be able to stand alone and be self-explanatory. We therefore need to make sure there are clear axis labels. This can be done using ‚Äòxlab‚Äô and ‚Äòylab‚Äô arguments.\n\n# Add the linear model to your scatterplot\nplot(fert, yield, xlab = \"fertiliser applied\", ylab = \"Yield\")\nabline(model.lm, col = \"red\")\n\n\n\n\n2.2 ABARES data\nIn this final example we will be using a dataset obtained from the Australian Bureau of Agricultural and Resource Economics and Sciences (ABARES). The dataset provides a measure of productivity growth (TFP; Total Factor Productivity) in the Australian dairy industry from the years 1978 to 2018.\nMore information about the ABARES dataset and productivity can be found here.\n\nRead in the data from the Excel file for today‚Äôs practical.\n\nBecause we have such a large dataset this time, it is better to read the data straight from Excel than read in each individual value. Reading straight from the source file in Excel saves time and reduces chance of input error.\n\nlibrary(readxl)\n\nABARES &lt;- read_excel(\"data/ENVX1002_practical_data_Regression.xlsx\", sheet = \"ABARES\")\n\n\nCreate a scatterplot of Year against TFP. Dont forget the format will be different now - instead of only mentioning the object name, e.g.¬†plot(yield, fert), you will need to refer to the specific columns within the ABARES dataset. (i.e.¬†ABARES$Year).\n\n\nCan you see a trend between TFP and Year? Or are the points evenly scattered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary. Year will be our predictor and TFP will be our response variable. What are the model parameters (i.e.¬†\\(b_0\\) and \\(b_1\\))?\n\n\nWhat would the equation for this model be?\n\n\nOverlay your model onto the scatterplot you produced earlier. When plotting make sure you refer to the column names as you did for the model (e.g.¬†ABARES$Year).\n\n\nThat‚Äôs it! Great work today. Next week: interpreting linear models!",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-1-cars-stopping-distance",
    "href": "labs/Lab09.html#exercise-1-cars-stopping-distance",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 1: Cars stopping distance",
    "text": "Exercise 1: Cars stopping distance\nFor this exercise we willuse the inbuilt dataset cars to see if there is a relationship between a cars speed (mph) and stopping distance (fft).\n\nhead(cars)\n\n\nCreate a scatterplot of speed vs distance\n\n\nIs there are trend? Or are the point evenly scatered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary.\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-2-penguins",
    "href": "labs/Lab09.html#exercise-2-penguins",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nFor this exercise, we will be using the palmer penguin data set to see if there is a relationship between bill and flipper length.\n\n#Load libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n#Clean data\npenguins &lt;- penguins%&gt;%\n  na.omit()#remove missing data\n\nhead(penguins)\n\n\nCreate a scatterplot of bill length vs flipper legth\n\n\nIs there are trend? Or are the point evenly scatered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary. What are the parameters?\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-3-old-faithful-geyser-data",
    "href": "labs/Lab09.html#exercise-3-old-faithful-geyser-data",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 3: Old Faithful Geyser Data",
    "text": "Exercise 3: Old Faithful Geyser Data\nFor this exercise, we will be looking at the relationship between geyser eruption time and time between erupttions, using the inbuilt data set faithful\n\nhead(faithful)\n\n\nCreate a scatterplot of eruptions vs waiting time\n\n\nIs there are trend? Or are the point evenly scatered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary.\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "CONTRIBUTING/quick-start.html",
    "href": "CONTRIBUTING/quick-start.html",
    "title": "Contributing",
    "section": "",
    "text": "To contribute to this project you will need to first ensure that you have been properly configured. This includes setting up your workflow, installing the necessary dependencies, and understanding the project‚Äôs structure.\n\n\nThese are basic instructions but will get you up and running quickly. For more detailed instructions, see the relevant documentation (when suggested). 1. Clone the repository to your local machine. 2. Create a new branch with a descriptive name (e.g., lab-11). 3. Make your changes, and save/commit your changes frequently. 5. Render the entire project to ensure that your changes have not broken anything. 6. Push your branch to the GitHub repository. 7. Create a pull request in GitHub or GitKraken for review.\n\n\n\nHere are some best practices for effective branching and pull requests:\n\nAlways create a new branch for each piece of work.\nUse descriptive branch names that reflect the work being done.\nKeep branches focused on a single task or feature.\nMake small, frequent commits with clear messages.\nUpdate your branch regularly with changes from main.\nTest your changes thoroughly before creating a pull request.\nWrite clear pull request descriptions explaining your changes.\nDelete branches after they are merged.\n\nFinally, as long as the main branch is always deployable (i.e.¬†it renders without errors), you can be confident that your changes will not break the project."
  },
  {
    "objectID": "CONTRIBUTING/quick-start.html#quick-start",
    "href": "CONTRIBUTING/quick-start.html#quick-start",
    "title": "Contributing",
    "section": "",
    "text": "These are basic instructions but will get you up and running quickly. For more detailed instructions, see the relevant documentation (when suggested). 1. Clone the repository to your local machine. 2. Create a new branch with a descriptive name (e.g., lab-11). 3. Make your changes, and save/commit your changes frequently. 5. Render the entire project to ensure that your changes have not broken anything. 6. Push your branch to the GitHub repository. 7. Create a pull request in GitHub or GitKraken for review."
  },
  {
    "objectID": "CONTRIBUTING/quick-start.html#tips",
    "href": "CONTRIBUTING/quick-start.html#tips",
    "title": "Contributing",
    "section": "",
    "text": "Here are some best practices for effective branching and pull requests:\n\nAlways create a new branch for each piece of work.\nUse descriptive branch names that reflect the work being done.\nKeep branches focused on a single task or feature.\nMake small, frequent commits with clear messages.\nUpdate your branch regularly with changes from main.\nTest your changes thoroughly before creating a pull request.\nWrite clear pull request descriptions explaining your changes.\nDelete branches after they are merged.\n\nFinally, as long as the main branch is always deployable (i.e.¬†it renders without errors), you can be confident that your changes will not break the project."
  },
  {
    "objectID": "module03/043-nonlinear.html",
    "href": "module03/043-nonlinear.html",
    "title": "Non-linear regression",
    "section": "",
    "text": "Linear relationships are simple to interpret since the rate of change is constant ‚Äì i.e.¬†as \\(x\\) changes, \\(y\\) changes at a constant rate. For non-linear relationships ‚Äì as \\(x\\) changes, \\(y\\) changes at an unproportional rate. To simplify interpretation and enable the use of linear models, it is often recommended to transform non-linear data to make it approximately linear.\nTransformation is usually possible for monotonic relationships (i.e.¬†relationships that are always increasing or decreasing) such as exponential growth curves. The most common transformations are:\n\n\n\nName\nTransformation\n\n\n\n\nInverse\n\\(y = \\frac{1}{x}\\)\n\n\nRoot\n\\(y = \\sqrt[a]{x}\\)\n\n\nExponential\n\\(y = e^x\\)\n\n\nLogarithmic\n\\(y = \\log_a(x)\\)\n\n\n\nHowever, in the case of non-monotonic relationships (e.g.¬†polynomials, asymptotic, logistic) transformation may not be enough to make the data meet the assumptions. In this section, we look at fitting non-linear models.\n\\[ Y_i = f(x_i, \\beta) + \\epsilon_i \\] where \\(f(x_i, \\beta)\\) is a nonlinear function of the parameters \\(\\beta\\).\nThe assumptions for non-linear regression are INE (Independence, Normality, Equal variance).\n\n\nA polynomial equation is an extension to linear regression and can still be fitted using least squares. Typically, a polynomial equation has multiple terms of the same predictor (i.e.¬†one \\(x\\)). The more terms in the polynomial, the more complex the model and the less likely it follows an actual biological relationship. A complex model may ‚Äòfit‚Äô the data well, but fail to represent the true relationship between the variables (overfitting).\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i \\] where \\(k\\) is the degree of the polynomial.\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nEach level increases the power of the predictor by 1.\n\n\n\nTo fit a polynomial, we still use the lm() function. To create the polynomial terms, we use the poly(x, n) function, where x is the predictor and n is the degree of the polynomial.\nBelow is an example with asymptotic data ‚Äì it increases rapidly to a certain point and then levels off. The linear model (blue) does not capture the complexity of the relationship. The polynomial model with 10 terms (green) fits the data well, but it is too complex between \\(x\\) = 5-10. In this case, the polynomial model with 2 terms (red), aka the quadratic model, fits the data the best.\n\n\nCode\n# Generate some data with an asymptotic relationship\nset.seed(442) # set seed\n\nasymptotic = tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100*(1-exp(-0.5*predictor)) + rnorm(length(predictor), mean = 0, sd = 10))\n\n\n\nmod_lin &lt;- lm(response ~ predictor, asymptotic) # blue\nmod_poly2 &lt;- lm(response ~ poly(predictor, 2), asymptotic) # red\nmod_poly10 &lt;- lm(response ~ poly(predictor, 10), asymptotic) #green\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point(size = 2) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_lin)), color = \"slateblue\", size = 1.5, linetype = 2) +\n  geom_line(aes(y = predict(mod_poly2)), color = \"brown\", size = 1, linetype = 1) +\n  geom_line(aes(y = predict(mod_poly10)), color = \"seagreen\", size = 1.5, linetype = 2) +\n  theme_classic()\n\n\nThis is also evident when comparing the summary() of the models. The linear model explains the least amount of variation in \\(y\\) (Multiple R-squared = 57.01%). The 2-term polynomial model explains 81.95% of variation (Adjusted R-squared), whereas the 10-term polynomial model explains 86.21% of variation (Adjusted R-squared). The first three terms of the polynomial model are significant, the fourth is almost significant, but the remainder are not. Comparing the 2 and 10-term polynomial models, are the extra 8 terms worth an extra 4.26% of variation explained? Considering the 10-term polynomial is very complex and overfitted ‚Äì the principle of parsimony would say ‚Äòno‚Äô.\n\nsummary(mod_lin)\nsummary(mod_poly2)\nsummary(mod_poly10)\n\nThe hypothesis of a polynomial model is similar to that of multiple linear regression. A significance test is done for each individual coefficient, and the overall model hypothesis is below:\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\] \\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0\\] In the case that all coefficients are zero, the model is better represented by the mean of the data.\nAs for interpretation of the quadratic model, we can write the equation, but the coefficients do not have much meaning.\n\\[\\text{response} = 79.82 + 159.37 \\cdot \\text{predictor} + -106.94 \\cdot \\text{predictor}^2\\] Given it is a quadratic equation, we could calculate the peak of the curve (\\(x_{peak} = -\\frac{b}{2a}\\) and substitute to get \\(y_{peak} = c - \\frac{b^2}{4a}\\)). For a polynomial of a higher degree, it would not be meaningful.\nAlthough a little out of order, we can also check the assumptions. Recall that linearity is not an assumption for a non-linear model ‚Äì we can disregard the shape of the line for the Residuals vs Fitted plot, and focus on the distribution of the point around the line (which is even). The residuals fall on the line for the Normal-QQ plot indicating residuals are normally distributed, and the Scale-Location plot shows the residuals evenly distributed (equal varince), and there are no extreme outliers in the Residuals vs Leverage plot.\n\npar(mfrow=c(2,2))\nplot(mod_poly2)",
    "crumbs": [
      "**üìò Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#polynomial-regression",
    "href": "module03/043-nonlinear.html#polynomial-regression",
    "title": "Non-linear regression",
    "section": "",
    "text": "A polynomial equation is an extension to linear regression and can still be fitted using least squares. Typically, a polynomial equation has multiple terms of the same predictor (i.e.¬†one \\(x\\)). The more terms in the polynomial, the more complex the model and the less likely it follows an actual biological relationship. A complex model may ‚Äòfit‚Äô the data well, but fail to represent the true relationship between the variables (overfitting).\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i \\] where \\(k\\) is the degree of the polynomial.\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nEach level increases the power of the predictor by 1.\n\n\n\nTo fit a polynomial, we still use the lm() function. To create the polynomial terms, we use the poly(x, n) function, where x is the predictor and n is the degree of the polynomial.\nBelow is an example with asymptotic data ‚Äì it increases rapidly to a certain point and then levels off. The linear model (blue) does not capture the complexity of the relationship. The polynomial model with 10 terms (green) fits the data well, but it is too complex between \\(x\\) = 5-10. In this case, the polynomial model with 2 terms (red), aka the quadratic model, fits the data the best.\n\n\nCode\n# Generate some data with an asymptotic relationship\nset.seed(442) # set seed\n\nasymptotic = tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100*(1-exp(-0.5*predictor)) + rnorm(length(predictor), mean = 0, sd = 10))\n\n\n\nmod_lin &lt;- lm(response ~ predictor, asymptotic) # blue\nmod_poly2 &lt;- lm(response ~ poly(predictor, 2), asymptotic) # red\nmod_poly10 &lt;- lm(response ~ poly(predictor, 10), asymptotic) #green\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point(size = 2) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_lin)), color = \"slateblue\", size = 1.5, linetype = 2) +\n  geom_line(aes(y = predict(mod_poly2)), color = \"brown\", size = 1, linetype = 1) +\n  geom_line(aes(y = predict(mod_poly10)), color = \"seagreen\", size = 1.5, linetype = 2) +\n  theme_classic()\n\n\nThis is also evident when comparing the summary() of the models. The linear model explains the least amount of variation in \\(y\\) (Multiple R-squared = 57.01%). The 2-term polynomial model explains 81.95% of variation (Adjusted R-squared), whereas the 10-term polynomial model explains 86.21% of variation (Adjusted R-squared). The first three terms of the polynomial model are significant, the fourth is almost significant, but the remainder are not. Comparing the 2 and 10-term polynomial models, are the extra 8 terms worth an extra 4.26% of variation explained? Considering the 10-term polynomial is very complex and overfitted ‚Äì the principle of parsimony would say ‚Äòno‚Äô.\n\nsummary(mod_lin)\nsummary(mod_poly2)\nsummary(mod_poly10)\n\nThe hypothesis of a polynomial model is similar to that of multiple linear regression. A significance test is done for each individual coefficient, and the overall model hypothesis is below:\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\] \\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0\\] In the case that all coefficients are zero, the model is better represented by the mean of the data.\nAs for interpretation of the quadratic model, we can write the equation, but the coefficients do not have much meaning.\n\\[\\text{response} = 79.82 + 159.37 \\cdot \\text{predictor} + -106.94 \\cdot \\text{predictor}^2\\] Given it is a quadratic equation, we could calculate the peak of the curve (\\(x_{peak} = -\\frac{b}{2a}\\) and substitute to get \\(y_{peak} = c - \\frac{b^2}{4a}\\)). For a polynomial of a higher degree, it would not be meaningful.\nAlthough a little out of order, we can also check the assumptions. Recall that linearity is not an assumption for a non-linear model ‚Äì we can disregard the shape of the line for the Residuals vs Fitted plot, and focus on the distribution of the point around the line (which is even). The residuals fall on the line for the Normal-QQ plot indicating residuals are normally distributed, and the Scale-Location plot shows the residuals evenly distributed (equal varince), and there are no extreme outliers in the Residuals vs Leverage plot.\n\npar(mfrow=c(2,2))\nplot(mod_poly2)",
    "crumbs": [
      "**üìò Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#horizontal-asymptotic-regression",
    "href": "module03/043-nonlinear.html#horizontal-asymptotic-regression",
    "title": "Non-linear regression",
    "section": "Horizontal asymptotic regression",
    "text": "Horizontal asymptotic regression\nAn asymptotic relationship is a type of non-linear relationship where the response variable approaches a limit as the predictor variable increases. This is common in growth curves, where growth is rapid at first and then slows down as it approaches a maximum (e.g.¬†age vs height, population growth, diminishing returns). The equation for an asymptotic relationship is:\n\\[ y = a + b(1 - e^{-cx}) \\]\n\n\\(a\\) is value of \\(y\\) when \\(x = 0\\).\n\\(b\\) is the upper limit: the maximum value of \\(y\\).\n\\(c\\) is the rate of change \\(y\\) approaches the upper limit.",
    "crumbs": [
      "**üìò Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#in-practice-1",
    "href": "module03/043-nonlinear.html#in-practice-1",
    "title": "Non-linear regression",
    "section": "In practice",
    "text": "In practice\nWith the nls() function, we first need to estimate our initial parameters for \\(a\\), \\(b\\) and \\(c\\). We can do this by plotting the data and making an educated guess. For the asymptotic data, we can see that the lower limit (\\(a\\)) is around 0 and the upper limit (\\(b\\)) is around 100. The rate of change (\\(c\\)) is a little harder to estimate, so we will guess a value of 0.8. If the model returns an error (singular gradient matrix at initial parameter estimates), we can try different values ‚Äì the most likely culprit will be the rate of change (\\(c\\)).\n\n\nCode\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() + \n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100), \n               arrow = arrow(length = unit(0.5, \"cm\")), \n               color = \"red\") +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n\n\n\n# Fit the model\nmod_asymptotic &lt;- nls(response ~ a + b*(1-exp(-c*predictor)), data = asymptotic, \n  start = list(a = 0, b = 100, c = 0.8))\n\nAfter we fit out model, we assess the assumptions plots and check our residuals for INE ‚Äì they look fine.\n\n# Assumption plots\nlibrary(nlstools)\nresids &lt;- nlsResiduals(mod_asymptotic)\nplot(resids)\n\nFinally, we evaluate the model. There is not much to interpret - except the Residual standard error. We can compare models directly with this error term. R2 is not calculated for non-linear models.\nThe Number of iterations to convergence is the number of times the computer changed the parameters to try and get a better fit.\nThe resulting model equation would be \\(y = -14.52 + 113.04(1 - e^{-0.63x})\\).\n\nsummary(mod_asymptotic)\n\nWe can then plot the original data and the line of best fit.\n\n\nCode\nggplot(asymptotic, aes(predictor, response)) +\n  geom_point(color = \"black\") +  # Original data points\n  geom_line(y = predict(mod_asymptotic), color = \"red\", size = 1) +  # Fitted line\n  theme_minimal() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n\n\n\nPoor estimation\nIf the starting values are too far (most likely \\(c\\) term), the model will not run, and there will be an error. However, there is some flexibility allowed. We use very inaccurate initial estimates - and the model still gets to the same result, it just takes more tries (iterations) to get there.\n\n# Fit the model\nmod_asymptotic &lt;- nls(response ~ a + b*(1-exp(-c*predictor)), data = asymptotic, \n  start = list(a = -100, b = 200, c = 15))\n\nsummary(mod_asymptotic)",
    "crumbs": [
      "**üìò Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#in-practice-2",
    "href": "module03/043-nonlinear.html#in-practice-2",
    "title": "Non-linear regression",
    "section": "In practice",
    "text": "In practice\nSo if we were to estimate the initial parameters‚Ä¶\n\nmod_logistic &lt;- nls(\n  response ~ c + (d - c) / (1 + exp(-b * (predictor - a))),\n  data = logistic,\n  start = list(c = 20, d = 300, b = 5, a = 5)\n)\n\nsummary(mod_logistic)\n\nAs above, we can only interpret the Residual standard error. The resulting model equation would be \\(y = 16.20 + 293.25 / (1 + e^{-0.81(x - 5.06)})\\).\n\n\nCode\nggplot(logistic, aes(predictor, response)) +\n  geom_point(color = \"black\") +  # Original data points\n  geom_line(y = predict(mod_logistic), color = \"red\", size = 1) +  # Fitted line\n  theme_minimal() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()",
    "crumbs": [
      "**üìò Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html",
    "href": "module03/041-linear_functions.html",
    "title": "Why do we create models?",
    "section": "",
    "text": "Often in a scientific experiment, we collect data for a response variable (\\(y\\)) and one or more predictor variables (\\(x\\)).\nSome interchangeable terms:\n\n\\(y\\) ‚Äì independent variable, response variable, target, outcome, etc.\n\\(x\\) ‚Äì dependent variable, predictor variable, feature, input etc.\n\nThere are several reasons why we would ‚Äòmodel‚Äô or ‚Äòcreate a model‚Äô for the data we have collected:\n\nTo describe the relationship between \\(x\\) and \\(y\\) (e.g.¬†weak/moderate/strong, positive/negative, linear/non-linear, etc.)\nTo explain the relationship between \\(x\\) and \\(y\\) in terms of an equation\nTo predict the value of \\(y\\) for a given value of \\(x\\)\n\nThe simplest form of a model is a linear model.",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#analytical-method",
    "href": "module03/041-linear_functions.html#analytical-method",
    "title": "Why do we create models?",
    "section": "Analytical method",
    "text": "Analytical method\nFor simple linear regression, we can calculate the values of \\(\\beta_0\\) and \\(\\beta_1\\) with equations\nFirst we calculate the slope \\(\\beta_1\\):\n\\[ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} \\] Which we then substitute into the equation below to get the intercept \\(\\beta_0\\):\n\\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\]\nUsing R to do our calculations:\n\ncov_xy &lt;- sum((x - mean(x)) * (y - mean(y)))\nvar_x &lt;- sum((x - mean(x))^2)\nb1 &lt;- cov_xy / var_x\n\nb0 &lt;- mean(y) - b1 * mean(x)\n\nSo the analytical method determines that our linear model is \\(y = -0.363076 + 0.415755 \\cdot x\\).",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#numerical-method",
    "href": "module03/041-linear_functions.html#numerical-method",
    "title": "Why do we create models?",
    "section": "Numerical method",
    "text": "Numerical method\nThe numerical method is a trial and error method. We start with some initial values of \\(\\beta_0\\) and \\(\\beta_1\\), and then update the values to minimize the sum of squares. This is the most common method that computer programs (e.g.¬†Excel, R, Python, etc.) will use as it is more computationally efficient with very large dataset (e.g.¬†millions of rows).\nFitting the model in R is very simple:\n\nmod &lt;- lm(y ~ x) # fit a linear model between x and y\nsummary(mod)     # model output in a neat summary table\n\nThe estimate of our parameters (or coefficients) are in the Estimate column. The Intercept Estimate is our y-intercept \\(\\beta_0\\) and the x Estimate is our slope \\(\\beta_1\\).\nSo the numerical method run by R determines that our linear model is \\(y = -0.363076 + 0.415755 \\cdot x\\). This is exactly the same result as the analytical method (at least to 6 decimal places).",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#exploratory-data-analysis",
    "href": "module03/041-linear_functions.html#exploratory-data-analysis",
    "title": "Why do we create models?",
    "section": "1. Exploratory data analysis",
    "text": "1. Exploratory data analysis\nThis is a basic step in any data analysis. We need to understand the data we are working with. As with previous modules, we can look at summary statistics, distributions, and visualise the data.\nFor linear regression, we also need to look at the relationship between the predictor and response. We first look at a scatter plot to determine if we have linear data, and then we choose a suitable correlation coefficient.\n\nggplot() +\n  geom_point(aes(x = x, y = y)) +\n  labs(title = \"Petal length vs Petal width\",\n       x = \"Petal length\",\n       y = \"Petal width\")\n\nThere appears to be a strong, positive linear relationship between petal length and petal width.\n\ncor(x, y) |&gt; round(2) # calculate correlation |&gt; round to 2 decimal places\n\nThe correlation coefficient is 0.96, which is very close to 1. This is almost a perfect positive linear relationship!",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#assumptions",
    "href": "module03/041-linear_functions.html#assumptions",
    "title": "Why do we create models?",
    "section": "2. Assumptions",
    "text": "2. Assumptions\nTo use linear regression, there are several assumptions (LINE) that need to be met:\n\nLinearity ‚Äì is there a linear relationship?\nIndependence ‚Äì are the residuals independent of each other?\nNormality ‚Äì are the residuals normally distributed?\nEqual variance ‚Äì is the variance of the residuals constant?\n\nIf assumptions are met, then we can be confident that the model is a good fit for the data. If they are not met, then the hypothesis test results are unreliable, the standard error estimate is unreliable, and the estimates of the coefficients will not fit the model well.\nIf the assumptions are not met, then we either need to transform our data (e.g.¬†\\(x\\), \\(y\\), or both) with a function (e.g.¬†square root, natural log etc.) or use a non-linear model.\nAn important point to remember is that the assumptions are about the residuals, not the data itself. The equation for a linear model is:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\] where \\(\\epsilon\\) is the error term, or residual. This is the only term which adds variation to an otherwise straight line, so this is what we need to check our assumptions with.\n\nLinearity\nLinear regression assumes that there is a linear relationship between \\(x\\) and \\(y\\). It does not make logical or statistical sense to fit a linear model to data that does not have a linear relationship. With non-linear data, the other assumptions will not be met either. The easiest method to check for linearity with a scatter plot.\n\n\nIndependence\nThe independence of errors is the assumption that the residuals for one observation are independent of another observation. This is a difficult assumption to check, but it is often assumed that the data is collected in a way that the residuals are independent.\nFor example, if we are measuring the height of children in a class, the height of one child should not affect the height of another child. However, if all the children were siblings or identical octuplets, then the residuals would not be independent. Another case where this assumption could be broken is in time series data ‚Äì the height of a child this year is not independent from the height last year.\n\n\nNormality\nA linear model assumes the residuals are normally distributed around the line of best fit. This is important for hypothesis testing and confidence intervals. We can check this assumption with a Q-Q plot or a histogram of the residuals.\n\n\n\n\n\n\n\nEqual variance\nThis is also known as the assumption of constant variance or homoscedasticity. It assumes that the variance of the residuals is constant across all levels of the predictor variable (i.e.¬†no fanning). Again, this is important for hypothesis testing and confidence intervals. We can check this assumption with a scatter plot of Residuals vs Fitted values and the Scale-Location plot.\n\n\nAssumption Plots\nIn R, plots of the residuals can be made with the plot() function and the model object (in this case mod) as the input. The four plots produced are 1) Residuals vs Fitted (linearity, equal variances), 2) the Normal Q-Q plot (normality), 3) a Scale-Location plot (equal variances), and 4) the Residuals vs Leverage plot (extreme outliers).\n\nmod &lt;- lm(y ~ x)  # fit a linear model between x and y\n\npar(mfrow=c(2,2)) # set up a 2x2 grid of plots\nplot(mod)         # create the plots\n\nThese same plots can also be made with autoplot() function (from the ggfortify package). It does not require the par() function to set up the grid and is more aesthetically customisable.\n\nlibrary(ggfortify)\nggplot2::autoplot(mod)\n\n\nThe Residuals vs Fitted plots the residuals against the ‚Äòpredicted‚Äô values (i.e.¬†points on the line). If met, the points will be randomly scattered around the horizontal 0 line. If there is a pattern (e.g.¬†a curve, a quadratic), then the assumption of linearity is not met. If there is a fan shape, then the assumption of equal variance is not met.\nThe Normal Q-Q plot compares the residuals to a normal distribution. If the residuals are normally distributed, the points will fall on the straight line. If the points deviate from the line, then the residuals are not normally distributed.\nThe Scale-Location plot is used to check the assumption of equal variance. If the line is essentially horizontal and points are randomly scattered, then the assumption is met.\nThe Residuals vs Leverage plot is used to identify extreme outliers. Although not an official assumption, a very extreme outlier has the potential to skew the line of best fit. These influential points should be kept track of or removed from the dataset.\n\nThe examples above are for the iris dataset with petal length and petal width. The assumptions are met.\nBelow is an example of plots from data that has an exponential relationship with unequal variances (it fans in) with a couple of extreme outliers (in red).\n\nmod_exp &lt;- lm(y_exp ~ x_exp)\npar(mfrow=c(2,2))\nplot(mod_exp)\n\nWe can see that:\n\nThe Residuals vs Fitted plot looks curved, indicating unequal variances. The scatter of points is also not very even because of the extreme outliers.\nThe Normal Q-Q plot shows the points are not on the line at the tails and hence the residuals are not normally distributed.\nThe Scale-Location plot shows a ‚ÄòW‚Äô shape, indicating unequal variances.\nThe Residuals vs Leverage plot shows the two extreme outliers past the dotted lines, which confirms they are influential points.",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#model-evaluation",
    "href": "module03/041-linear_functions.html#model-evaluation",
    "title": "Why do we create models?",
    "section": "Model evaluation",
    "text": "Model evaluation\nSo now that we have confirmed the assumptions are met, the next step is to determine how good the model actually is. If the model was not a good fit to the data, then we wouldn‚Äôt want to interpret it ‚Äì we‚Äôd try and improve it first.\nLet‚Äôs break down the summary(mod) output. The first few lines are the model formula lm(formula = y ~ x), so this is our linear model. The Residuals section gives some summary statistics of the residuals.\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\n\nHypothesis Test\nThe null hypothesis for a simple linear regression is that the slope is zero (\\(\\beta_1 = 0\\)), and the model does not perform better than just using the mean. There is thus no relationship between \\(x\\) and \\(y\\). The alternative hypothesis is that the slope is not zero (\\(\\beta_1 \\neq 0\\)), which means there is a relationship between \\(x\\) and \\(y\\), and the model is better than using the mean.\nThe Coefficients section gives the estimates of our y-intercept and slope, as well as the standard error, the t-value, and the p-value when determining the estimate. The p-value is the most easily interpreted ‚Äì we care the p-value of the (\\(x\\)). If the p-value is less than 0.05, then we can reject the null hypothesis that the slope \\(\\beta_1\\) is zero.\nThe Signif. codes section gives a visual representation of the p-value. The more stars, the smaller the p-value. The *** means the p-value is less than 0.001, ** means less than 0.01, and * means less than 0.05. This is useful when we have many predictors.\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.363076   0.039762  -9.131  4.7e-16 ***\nx            0.415755   0.009582  43.387  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n\n\nModel Fit\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\nThe Residual standard error is the standard deviation of the residuals. The smaller the value, the better the model fits the data. It is in the same units as the response variable. Considering the \\(y\\) ranges between 0.1 and 2.5, a standard error of 0.2065 is quite small.\nThe Multiple R-squared is the proportion of the variance in the response variable that is explained by the model. The more variation explained by the model, the better the fit. A value of ‚Äò1‚Äô indicates a perfect fit, and a value of ‚Äò0‚Äô suggests the model explained no variation at all. A value of 92.71% is very good. In simple linear regression, the R2 is actually equivalent to the correlation coefficient squared - which is also where the term comes from.\nThe Adjusted R-squared is the same as the Multiple R-squared, but adjusted for the number of predictors in the model. We use the Adjusted R-squared in multiple linear regression, and the Multiple R-squared in simple linear regression.\n\\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n\\[ R^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} \\] Lastly, the F-statistic is a statistical test of the overall significance of the model. In simple linear regression (1 predictor, 1 response) it is the same as the p-value in the Coefficients table. The F-statistic value is 1882 in the example, and the degrees of freedom (1, 148) are the number of predictors (1) and the number of observations minus two (150-2=148) because there are two parameters. This is covered in more detail in ENVX2001.",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#interpretation",
    "href": "module03/041-linear_functions.html#interpretation",
    "title": "Why do we create models?",
    "section": "Interpretation",
    "text": "Interpretation\nThe assumptions for simple linear regression were met. The model explains 92.71% of the variation in the petal width with petal length with a residual standard error of 0.2065, and petal length is a significant predictor (p &lt; 2e-16).\nThe equation of the model is \\(y = -0.36 + 0.42 \\cdot x\\). This means that for every unit increase in petal length, petal width increases by 0.42 cm.\nThe y-intercept is -0.36, which is the value of petal width when petal length is zero. This is not a meaningful value in this context, as petal length cannot be zero. This is often the case with the y-intercept.\nLast but not least, the result can be plotted on a scatterplot with the line of best fit.\n\n### Base R\nplot(x, y, pch = 19,\n     xlab = \"Petal Length\", ylab = \"Petal Width\", main = \"Base R\"\n)\nabline(mod, col = \"red\") # using our model `mod` object\n# abline(a = -0.363076, b = 0.415755, col = \"red\") # manually inputting coefficients\n\n### ggplot\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") + # ggplot will fit the model for us\n  # geom_abline(intercept = -0.363076, slope = 0.415755, color = \"red\") + # manually inputting coefficients\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = \"ggplot2\")",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#explaining-backtransformation",
    "href": "module03/041-linear_functions.html#explaining-backtransformation",
    "title": "Why do we create models?",
    "section": "Explaining backtransformation",
    "text": "Explaining backtransformation\n\nSquare root\nThe backtransformation of the coefficients is not as straightforward as the interpretation of the coefficients. For example, if we had used a square root transformation sqrt(Ozone), then if we wanted to get the effect of increasing Temp by one unit:\n\\[ \\sqrt{Ozone} = \\beta_0 + \\beta_1 \\cdot Temp \\] \\[ Ozone = (\\beta_0 + \\beta_1 \\cdot Temp)^2 \\] \\[ Ozone = \\beta_0^2 + 2 \\beta_0 \\beta_1 Temp + \\beta_1^2 Temp^2 \\] So if Temp increases by one unit, the increase in Ozone is not just \\(\\beta_1^2\\)! We end up with \\(2\\beta_1(\\beta_0+\\beta_1 \\cdot Temp) + \\beta_1^2\\).\n\n\nNatural log\nFor natural logs, however, the backtransformation is much simpler because of log laws.\n\\[ log(Ozone) = \\beta_0 + \\beta_1 \\cdot Temp \\] \\[ Ozone = e^{\\beta_0 + \\beta_1 \\cdot Temp} = e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp} \\] If Temp increases by one unit, then the increase in Ozone is:\n\\[ Ozone = e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot (Temp+1)} = e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp} \\cdot e^{\\beta_1} \\] The ratio between the two is:\n\\[ \\frac{e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp} \\cdot e^{\\beta_1}}{e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp}} = e^{\\beta_1} \\] So for a one unit increase in Temp, Ozone increases by \\(e^{\\beta_1}\\) times.",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#percentage-change",
    "href": "module03/041-linear_functions.html#percentage-change",
    "title": "Why do we create models?",
    "section": "Percentage Change",
    "text": "Percentage Change\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute \\(e\\) below for 10 or any other base), but the quick approximation only works with natural log transformations.\nIf \\(y\\) has been transformed with a natural log (log(y)), for a one-unit increase in \\(x\\) the percent change in \\(y\\) (not log(y)) is calculated with:\n\\[\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)\\] If \\(\\beta_1\\) is small (i.e.¬†\\(-0.25 &lt; \\beta_1 &lt; 0.25\\)), then: \\(e^{\\beta_1} \\approx 1 + \\beta_1\\). So \\(\\Delta y \\% \\approx 100 \\cdot \\beta_1\\). Below are some examples ‚Äì when \\(\\beta_1\\) is 2, the ‚Äòquick estimate‚Äô is off by 439%!\n\n\n\n\n\n\n\n\nŒ≤\nExact \\((e^{\\beta} - 1)\\)%\nApproximate \\(100 \\cdot \\beta\\)\n\n\n\n\n-0.25\n-22.13\n-25\n\n\n-0.1\n-9.52\n-10\n\n\n0.01\n1.01\n1\n\n\n0.1\n10.52\n10\n\n\n0.25\n28.41\n25\n\n\n0.5\n64.87\n50\n\n\n2\n638.91\n200\n\n\n\nSo for our linear model \\(log(\\text{Ozone}) = -1.849 + 0.068 \\cdot \\text{Temp}\\), a one unit increase in Temp results in approximately a 6.8% increase in Ozone.",
    "crumbs": [
      "**üìò Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module02/03-nonparametric1.html",
    "href": "module02/03-nonparametric1.html",
    "title": "Chi-squared tests",
    "section": "",
    "text": "Chi-squared tests\nThe chi-squared distribution (where chi is pronounced ‚Äòky‚Äô) is a very widely used distribution in statistics. Its symbol is ÔÅ£2. It has MANY applications. Here we will consider only two of these applications ‚Äì tests of agreement with expected outcomes, and contingency tables.\n7.1 Notes on the ÔÅ£2 Distribution\nThe density function for a ÔÅ£2 distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the ÔÅ£2 density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g.¬†1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the ÔÅ£2 distribution approaches the shape (and properties) of a normal distribution.\nFigure 7.1 The shape of the ÔÅ£2 density function for various degrees of freedom.\nThe mean and variance of a ÔÅ£2 variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as ÔÅÆ (Greek n), then\nMean of   variable = ÔÅÆ  (i.e. mean = df)\nVariance of   variable = 2ÔÅÆ (i.e. variance = twice the df)\nCritical values of a ÔÅ£2 distribution are given in the ÔÅ£2 probability table that appears as Appendix A.5.\n7.2 Testing Agreement of Frequency Data with Expectation Models\n7.2.1 Steps in Chi-Squared Tests of Agreement\nThe process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\nChoose an appropriate hypothesis test for the type of data you have, and the type of question you‚Äôre asking.\nChoose the level of significance for the test.\nWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\nCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\nCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic ‚Äì however there are alternative tests designed to accommodate these situations.\nCalculate the test statistic and degrees of freedom.\nObtain the P-value.\nDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n7.2.2 Examples: Testing Whether Outcomes are Equally Probable\nEXAMPLE 1\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a ‚Äúfair‚Äù coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a ‚Äúfair‚Äù coin.\nEXAMPLE 2 (from Mead et al, 2003)\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\nCheese First preference A 5 B 7 C 18 D 10 Total 40\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n =  =9.80.\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the value of 9.80 is greater than the value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since P &lt; 0.05.)\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with\n =  =‚Ä¶      etc.\n7.2.3 Example: Testing Whether Outcomes are in Expected Proportions (from Mead et al, 2003)\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen).\nThe figures obtained for the primula plants follow. Normal eye Primrose Queen eye Total Flat leaves 328 122 450 Crimped leaves 77 33 110 Total 405 155 560\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, ¬æ of the total 560 observation in the first class of the characteristic and the remaining ¬º in the second class. Further, this model predicts that ¬æ of the flat-leaved plants should have normal eyes, resulting in ¬æ √ó ¬æ of all the plants or 9/16 with flat leaves and normal eyes; the remaining ¬º of the flat-leaved plants, which is ¬º √ó ¬æ or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes.\nThe calculation of these expected or predicted proportions is shown below.\nNormal eye  Primrose Queen eye\nFlat leaves ¬æ √ó ¬æ = 9/16 ¬º √ó ¬æ = 3/16 Crimped leaves ¬æ √ó ¬º = 3/16 ¬º √ó ¬º = 1/16\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35.\nThe observed and expected frequencies are summarized in the table below.\nNormal eye  Primrose Queen eye\nFlat leaves 328 (315) 122 (105) Crimped leaves 77 (105) 33 (35)\n    =  \n    = 0.54 + 2.75 + 7.47 + 0.11 = 10.77. \nWe compare 10.77 with the 5% point of the distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable.\nSee pp.¬†332-333 of Mead et al, 2003 for what to do next‚Ä¶ after rejecting the model.\n7.3 Contingency Tables\n7.3.1 Example: (2 x 2) Contingency Table\nConsider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\nRecovered   \nYes No  Total\nProcedure 1 14 6 20 Procedure 2 8 12 20 Total 22 18 40\nThis is one form of a 2ÔÇ¥2 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\nSolution: We will perform a statistical hypothesis test:\nH0: There is no difference in the true recovery rates for animals on either procedure H1: The recovery rates do differ.\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\nH0: p1 = p2\nH1: p1 ‚â† p2\nEstimates of individual recovery rates are = 14/20 = 0.7 and = 8/20 = 0.4. Is this difference due to chance?\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n  =  .\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be 20 22/40 = 20 0.55 = 11 animals. In general this can be written as:\nSo the expected frequencies for the cells in the table are:\n‚ÄÉ Expected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\nRecovered   \nYes No  Total\nProcedure 1 14 (11) 6 (9) 20 Procedure 2 8 (11) 12 (9) 20 Total 22 18 40\nThe table shows observed (and expected) frequencies. The ÔÅ£2 test statistic is then calculated using:\nLarge values of ÔÅ£2 indicate discrepancies between observed and expected frequencies, i.e.¬†large values indicate that H0 should be rejected in favour of H1.\nThe df of this ÔÅ£2 test is 1 for a 2ÔÇ¥2 contingency table. In general,\nIf H0 is true, the observed ÔÅ£2 is just one observation from a ÔÅ£2 distribution with 1 df:\nSince , there is (just) not sufficient evidence to reject H0. Thus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n‚ÄÉ 7.3.2 Example: (4 x 3) Contingency Table\nThe second example is a 4ÔÇ¥3 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n    Disease Status      \nVaccine No Mild Severe Total Control 100 (137.3) 71 (42.6) 29 (20.1) 200 A 146 (133.9) 32 (41.6) 17 (19.6) 195 B 149 (132.5) 28 (41.2) 16 (19.3) 193 C 146 (137.3) 37 (42.6) 17 (20.1) 200 Total 541 168 79 788\nThe table shows observed (and expected) frequencies.\nWe test H0 that there is no association between disease status and vaccination given, i.e.¬†all vaccinations have equal effectiveness.\nAssuming H0 is true, the expected frequencies are calculated as follows:\ne.g.¬†Expected frequency for an animal in the Control, No disease group:\n.\nAs before, the test statistic is and this will have (4-1)ÔÇ¥(3-1) = 6 df.\nThis is how to do the test in the Stats menu in GenStat:\nStats &gt; Statistical Tests &gt; Contingency Tables‚Ä¶\nGenStat requires the data to be set up as a 4ÔÇ¥3 Table type of spreadsheet (as opposed to a Variate or Matrix). It then reports the X2 test statistic and P-Value by selecting the Pearson method.\nPearson chi-squared value is 45.22 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\nLikelihood chi-squared value is 43.34 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001 ‚ÄÉ The ML ÔÅ£2 is calculated as follows:\nThe degrees of freedom are (r ‚Äì 1)(c ‚Äì 1) as before, where r and c are the numbers of rows and columns respectively.\nIn general, the ÔÅ£2 approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\nDisease Status  \nVaccine No Mild Severe Total Control 10 7 3 20 A 15 3 2 20 B 15 3 2 20 C 15 4 2 21 Total 55 17 9 81",
    "crumbs": [
      "**üìó Module 2**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html",
    "href": "module02/01-ttest1.html",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as ‚Äúconfidence limits‚Äù. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI‚Äôs for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g.¬†(3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g.¬†99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI‚Äôs in literature.\n\n\n\n(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)‚Äôs are not all the same due to sampling variability. Their scatter depends on both the variability of the y‚Äôs, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ¬± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)‚Äôs if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‚Äòsamples‚Äô of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let‚Äôs assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci\n\n\n\n\nThere are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)‚Äôs sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student‚Äôs T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n ‚Äì 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g.¬†T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the ‚Äúold school‚Äù t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student‚Äôs T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\nqt(0.975, 24)\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).\n\n\n\n\nIf an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated ‚Äúsample‚Äù, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e.¬†when we make the CI wide.\n\n\n\nAs you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a ‚Äútransformation‚Äù is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\nkurtosis(bcc_df$BloodCellCount)\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\nObservations:\n\nThis is count data. From statistical theory, we don‚Äôt expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e.¬†n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\nkurtosis(bcc_df$log_BloodCellCount)\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\nkurtosis(tccb_df$log_TcCB_ppb)\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is ‚Äì0.598. So the back-transformed mean is \\(e^{‚Äì0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won‚Äôt be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.\n\n\n\n\nSometimes research questions are framed not as ‚ÄúWhat is a plausible range of values for such and such a parameter?‚Äù but rather ‚ÄúAre the data consistent with this particular value for the parameter?‚Äù. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the ‚Äúhealth‚Äù of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don‚Äôt - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause ÔÅ°, the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g.¬†in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou‚Äôll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).\n\n\n\n\n\n\nThe liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e.¬†(population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e.¬†more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let‚Äôs say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).\n\n\n\n\nOften researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case‚Ä¶\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!\n\n\n\n\n\nFor the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e.¬†we assume we don‚Äôt know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs.¬†\\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the ‚Äúold school‚Äù t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R‚Äôs t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has ‚Äúheavier‚Äù tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).\n\n\n\n\nHypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI‚Äôs.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance ‚Äì the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI‚Äôs as they are more informative than the p-value alone. For example, the width of the CI‚Äôs says something about the precision of the estimate.\n\n\n\nExample\nThere is evidence that total nitrogen levels in the river ‚Äì like many other environmental quality data ‚Äì are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical ‚Äúsmiley‚Äù shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n\nshapiro.test(tn$TN)\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\nWe can use the mean of this transformed data i.e.¬†the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nshapiro.test(tn$log10_TN)\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet‚Äôs test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-intervals-for-mu",
    "href": "module02/01-ttest1.html#confidence-intervals-for-mu",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as ‚Äúconfidence limits‚Äù. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI‚Äôs for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g.¬†(3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g.¬†99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI‚Äôs in literature.",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "href": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)‚Äôs are not all the same due to sampling variability. Their scatter depends on both the variability of the y‚Äôs, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ¬± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)‚Äôs if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‚Äòsamples‚Äô of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let‚Äôs assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "href": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "There are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)‚Äôs sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student‚Äôs T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n ‚Äì 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g.¬†T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the ‚Äúold school‚Äù t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student‚Äôs T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\nqt(0.975, 24)\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#what-is-meant-by-confidence-interval",
    "href": "module02/01-ttest1.html#what-is-meant-by-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "If an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated ‚Äúsample‚Äù, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e.¬†when we make the CI wide.",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "href": "module02/01-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "As you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a ‚Äútransformation‚Äù is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\nkurtosis(bcc_df$BloodCellCount)\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\nObservations:\n\nThis is count data. From statistical theory, we don‚Äôt expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e.¬†n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\nkurtosis(bcc_df$log_BloodCellCount)\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\nkurtosis(tccb_df$log_TcCB_ppb)\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is ‚Äì0.598. So the back-transformed mean is \\(e^{‚Äì0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won‚Äôt be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#hypothesis-testing",
    "href": "module02/01-ttest1.html#hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Sometimes research questions are framed not as ‚ÄúWhat is a plausible range of values for such and such a parameter?‚Äù but rather ‚ÄúAre the data consistent with this particular value for the parameter?‚Äù. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the ‚Äúhealth‚Äù of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don‚Äôt - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause ÔÅ°, the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g.¬†in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou‚Äôll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-z-tests",
    "href": "module02/01-ttest1.html#one-sample-z-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "The liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e.¬†(population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e.¬†more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let‚Äôs say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#general-notes-on-hypothesis-testing",
    "href": "module02/01-ttest1.html#general-notes-on-hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Often researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case‚Ä¶\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-t-tests",
    "href": "module02/01-ttest1.html#one-sample-t-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "For the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e.¬†we assume we don‚Äôt know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs.¬†\\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the ‚Äúold school‚Äù t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R‚Äôs t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has ‚Äúheavier‚Äù tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "href": "module02/01-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Hypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI‚Äôs.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance ‚Äì the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI‚Äôs as they are more informative than the p-value alone. For example, the width of the CI‚Äôs says something about the precision of the estimate.",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-t-test-with-data-transformation",
    "href": "module02/01-ttest1.html#one-sample-t-test-with-data-transformation",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Example\nThere is evidence that total nitrogen levels in the river ‚Äì like many other environmental quality data ‚Äì are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical ‚Äúsmiley‚Äù shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n\nshapiro.test(tn$TN)\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\nWe can use the mean of this transformed data i.e.¬†the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nshapiro.test(tn$log10_TN)\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet‚Äôs test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**üìó Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html",
    "href": "module02/02-ttest2.html",
    "title": "Two-sample \\(t\\)-test",
    "section": "",
    "text": "In the one-sample tests just considered, we compared a population mean, \\(\\mu\\) (where our experimental sample represents the population) with a fixed numeric value of interest. In practice, this is fairly rare. More frequently we have samples drawn from two (or more) populations and we compare the two means to see if the treatments produce similar results.\nThe null hypothesis is that the two samples come from a population with the same true mean, \\(\\mu\\). This is commonly expressed as\n\\[H_0: \\mu_1 = \\mu_2\\]\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are the means of the two populations. The alternative hypothesis is that the two means are different, i.e.\n\\[H_1: \\mu_1 \\neq \\mu_2\\]",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#assumptions",
    "href": "module02/02-ttest2.html#assumptions",
    "title": "Two-sample \\(t\\)-test",
    "section": "Assumptions",
    "text": "Assumptions\nThe two-sample \\(t\\)-test assumes that the data are :\n\nContinuous,\nat least approximately normally distributed, and\nthe variances of the two sets are homogeneous (i.e.¬†the same).\n\nIdeally these assumptions should be tested before you carry about the two-sample \\(t\\)-test.",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#what-if-assumptions-are-not-met",
    "href": "module02/02-ttest2.html#what-if-assumptions-are-not-met",
    "title": "Two-sample \\(t\\)-test",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\nIf the data do not meet either of these assumptions, then you may choose an appropriate transformation or look for another technique to use e.g.¬†a non-parametric method (see non-parametric sections of this book).\nIf the variances cannot be assumed to be equal (but the data are normally distributed), there is a way of adjusting the two-sample \\(t\\)-test to compensate for this ‚Äì it‚Äôs called the Satterthwaite‚Äôs approximation ‚Äì more on this later!",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#three-variants-of-the-two-sample-t-test",
    "href": "module02/02-ttest2.html#three-variants-of-the-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Three Variants of the Two-Sample \\(t\\)-test",
    "text": "Three Variants of the Two-Sample \\(t\\)-test\nThere are 3 different ways in which a two-sample \\(t\\)-test can be used. They ALL assume that the data is approximately normally distributed.\n\nIndependent samples, equal variance - run a two-sample \\(t\\)-test assuming equal variances.\nIndependent samples, unequal variance - run a two-sample \\(t\\)-test assuming unequal variances.\nPaired samples - run a paired \\(t\\)-test.\n\nIn choosing which variant of the \\(t\\)-test is applicable in your situation, you first need to decide whether your two samples are paired or unpaired (independent).",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#paired-data",
    "href": "module02/02-ttest2.html#paired-data",
    "title": "Two-sample \\(t\\)-test",
    "section": "Paired Data",
    "text": "Paired Data\nPaired samples arise when we measure pairs of similar experimental units. In this pair of experimental units, one unit receives Treatment 1 and the other receives Treatment 2.\nIn some cases, treatments are applied to the same experimental unit e.g.¬†one half of a piece of fruit receives Trt 1 and the other half Trt 2; two plants of different varieties are grown in the same pot (here variety is the treatment).\nIn recognising this pairing in our analysis we are taking into account the fact that biological variation between pairs is likely to be larger than within pairs. This way, we get a clearer picture of the difference that is due to the treatment factor.\n\nExamples of paired data\n\nObservations at two times on the same experimental unit\n\nBefore and after readings of particle matter in the air on 3 sites near a new power station (before the station was built, and after it became operational). (Dytham 2003, 80)\nMeasurements of water flow on two consecutive days at 6 sites along a river. (Dytham 2003, 83)\n\nObservations on 2 halves/parts of the same experimental unit\n\nOne half of each (uncut) grapefruit was exposed to sunlight, and the other half was shaded (McConway et al 1999, p.¬†198).\nA standard (recommended) variety of wheat is compared with a new variety via 2 similar plots on each of 8 farms (Clewer and Scarisbrick 2001, p.¬†46).",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#two-sample-t-test-for-independent-samples",
    "href": "module02/02-ttest2.html#two-sample-t-test-for-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Two-Sample t-Test for Independent Samples",
    "text": "Two-Sample t-Test for Independent Samples\nProcedure for the test:\n\nSet up the null and alternate hypotheses\nDecide on the level of significance, 5%, 1%, 0.01% etc.\nCheck the assumptions of normality and equal variance. We use an F-test to formally test for equality of variance.\nCalculate the test statistic \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{SED}\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means, and \\(SED\\) is the standard error of the difference between the means.\nCalculate the degrees of freedom (df).\nFind the P-value in printed statistical tables or via GenStat or Excel.\nMake a statistical conclusion by comparing this P-value to your chosen level of significance (if P &lt;Œ±, then reject null hypothesis).\nCalculate the confidence interval.\nInterpret your results biologically.\n\n\n\n\n\n\n\nCalculating the test statistic\n\n\n\nWhen calculating the test statistic, use \\(y_1\\) as the larger mean. This will give a positive value for \\(t\\).",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "href": "module02/02-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)",
    "text": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)\nA two-sample t-test shows whether there is evidence of a difference in population means. The magnitude of this difference can be estimated with a confidence interval.\nA 95% confidence interval for the true difference \\(\\mu_1 - \\mu_2\\) is given by \\[\\bar{y}_1 - \\bar{y}_2 \\pm t^{\\alpha/2}_{df} \\times SED\\]\nwhere \\(\\bar{y}_1 - \\bar{y}_2\\) is the difference between the sample means, \\(t^{\\alpha/2}_{df}\\) is the critical value from the t-distribution for the chosen level of significance and degrees of freedom, and \\(SED\\) is the standard error of the difference between the means.\nThe df and SED need to take into account whether or not you are assuming equal variances.",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "href": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with EQUAL Variances",
    "text": "SED and df for Independent Samples with EQUAL Variances",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "href": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with UNequal Variances",
    "text": "SED and df for Independent Samples with UNequal Variances\nWhat do you do if when you‚Äôre checking your assumptions for a two- sample t-test you find that the variances are not equal? You can go ahead with a modified t-test or you can choose a different test.\nThis modified t-test used in the case of unequal variances is often called Satterthwaite‚Äôs approximate t-test.\n\nSatterthwaite‚Äôs Approximate t-Test\nThe null and alternate hypotheses remain the same i.e.¬†\\[H_0: \\mu1 = \\mu2\\] or \\[H_0: \\mu1 - \\mu2 = 0\\]\nThe formula for the test statistic, \\(t\\), in Satterthwaite‚Äôs approximate test is a little different to that for the t-test with equal variances. The s.e.d. changes because we can no longer use a pooled estimate of the variance (since the variances cannot be assumed equal).\nA correction for unequal variance is made to the degrees of freedom.\nThen proceed as usual through the rest of the test - i.e.¬†find the P-value; draw a statistical conclusion about your hypothesis; interpret your results biologically.",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#f-test-for-equality-of-variances",
    "href": "module02/02-ttest2.html#f-test-for-equality-of-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "F-Test for Equality of Variances",
    "text": "F-Test for Equality of Variances\nOne of the conditions for the independent sample t-test to be valid is that the population variances œÉ12 and œÉ22 are equal.\nTo test the null hypothesis that œÉ12 = œÉ22 divide the larger s2 value by the smaller s2 to obtain the variance ratio, v.r.:\nTo undertake this two-tailed test at the 5% level you need to carry out the one-tailed test at the 2.5% level. You can use the 2.5% F table of critical values, or you can use GenStat via the menus Data&gt;Probability Calculations‚Ä¶ to find the P-value.\nIf you use the printed statistical table, you will need to compare the critical value you find there with the variance ratio you calculated. If the calculated variance ratio &gt; critical value, reject H0 and conclude that the variances are significantly different.",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#example-two-sample-t-test",
    "href": "module02/02-ttest2.html#example-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Example: Two-Sample t-Test",
    "text": "Example: Two-Sample t-Test\nWeights of two breeds of cattle are to be compared: 15 cattle from Breed 1 and 12 cattle from Breed 2 were randomly sampled. Their recorded weights (kg) are shown.\nBreed 1 Breed 2 148.1 187.6 146.2 180.3 152.8 198.6 135.3 190.7 151.2 196.3 146.3 203.8 163.5 190.2 146.6 201.0 162.4 194.7 140.2 221.1 159.4 186.7 181.8 203.1 165.1\n165.0\n141.6\nThe following descriptive statistics are obtained from these data.\nBreed 1 Breed2\nSample mean (kg) 153.700 196.175 Sample s.d. (kg) 12.301 10.616\nIs there any systematic difference in their weights? The question, ‚ÄúIs there any systematic difference in their weights?‚Äù, is asking whether or not there is any significant difference between the 2 samples ‚Äì and in effect if there is any significant difference between the 2 breeds because the factor that distinguishes the 2 samples is breed.\nWe could answer this question by testing whether or not the population mean weights for the 2 breeds can be assumed to be equal:\nH0: Œºbreed 1 = ŒºBreed 2 versus H1: ŒºBreed 1 ‚â† ŒºBreed 2\nTo test this particular hypothesis about the equality of the means (as an avenue for answering our broader question), we would use a two-sample t-test.\nHowever, it is REALLY important to remember that there are other tests and hypotheses that we could use to answer our broader question (about whether or not there is any statistical difference between the weights of the 2 breeds). We‚Äôll consider some non-parametric alternatives in Biometry 2.\nTo proceed with a two-sample t-test, there are 2 assumptions that we need to check: ‚Ä¢ Normality of Breed 1 data and normality of Breed 2 data ‚Ä¢ Equality of variances of the 2 samples\nBoth of these checks are hypothesis tests in their own right and contain the usual elements of a hypothesis test i.e.¬†null & alternate hypothesis; test statistic; df; P-value or critical value; conclusion.\nTesting the Assumption of Normality If you are doing the test by hand, you would need to assume that the data are normally distributed (and hope this is true). At least the data in this example is continuous‚Ä¶ which is one small step towards normality.\nTesting the Assumption of Equality of Variance H0: œÉ2Breed 1 = œÉ2Breed 2 versus H1: œÉ2Breed 1 ‚â† œÉ2Breed 2\nTest statistic:\nThere are 2 degrees of freedom to calculate for an F test. They are called the numerator df, ÔÅÆ1 and the denominator df, ÔÅÆ2. (Remember from school days, numerator is the top half of a fraction, denominator is the bottom half of a fraction.) The ÔÅÆ that looks like a curly ‚Äòv‚Äô is the Greek letter ‚Äònu‚Äô.\nFor an F test used to test equality of variance, df are: ÔÅÆ1 = n1 ‚Äì 1, ÔÅÆ2 = n2 ‚Äì 1.\nHere, ÔÅÆ1 = nBreed 1 -1 = 15 - 1 = 14 and ÔÅÆ2 = nBreed 2 - 1 = 12 ‚Äì 1 = 11.\nUsing the 2.5% F table, we can compare (at the upper tail) the Fcritical and Fobserved values. If Fobs &gt; Fcrit, we reject H0 and conclude that the variances of the 2 samples are NOT equal.\nFrom these tables, it is not possible to find exactly, so we will make do with the closest possible value .\nWe find Fcrit ‚âà 3.33 and Fobs = 1.34. Since Fcrit &gt; Fobs ,we CAN assume that the variances are equal.\nSo‚Ä¶ assuming normality and equal variances, we can complete a ‚Äúpooled‚Äù two-sample t-test where\nTesting the null hypothesis that H0: ŒºBreed 1 = ŒºBreed 2, we find that there is a significant difference in the mean weights of the two breeds of cattle (T = 9.46, df = 25, P &lt; 0.001). The mean weight of Breed 2 is significantly higher and we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1.\nThis last piece of information, ‚Äúwe are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1‚Äù, is obtained from the 95% confidence interval for the true difference (Œº1 ‚Äì Œº2).\nNB. Recall that a 95% confidence interval for the true difference (Œº1 ‚Äì Œº2) is .\n6.10 Paired t-Test\nWe can re-write the generic null hypothesis for a two-sample test of means, H0: Œº1 = Œº2 as‚Ä¶ H0: Œº1 ‚Äì Œº2 = 0, or H0: Œºd = 0.\nThe paired t-test is actually a one-sample t-test of the differences between pairs of observations (from 2 different samples).\nAssumption: ‚Ä¢ Data is approximately normally distributed.\nSince we are doing a one-sample t-test on the differences then the assumption of equal variances is not relevant. Firstly, create a single column of data to use in the t test. Each value in the column corresponds to the difference between the 2 values for a particular matched pair.\nFarm Yield Variety A (kg) Yield Variety B (kg) Difference 1 17.8 14.7 3.1 2 18.5 15.2 3.3 3 12.2 12.9 -0.7 4 19.7 18.3 1.4 5 10.8 10.1 0.7 6 11.9 12.2 -0.3 7 15.6 13.5 2.1 8 12.5 9.9 2.6\nThe difference for Farm 1 = 17.8 ‚Äì 14.7 = 3.1.\nData Source: Clewer and Scarisbrick (2001, 46)\nSecondly, find some summary statistics of the differences so you can complete the t test:\nNo. of values, n = 8        Sum = 12.2          Mean = 1.525\nVariance = 2.299            Std Dev = 1.516\nand df = n -1, where ÔÅ≠d is usually 0.\n6.11 Confidence Interval for ÔÅ≠1 ‚Äì ÔÅ≠2 (Paired Samples)\nA 95% confidence interval for the mean difference is",
    "crumbs": [
      "**üìó Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html",
    "href": "module03/040-describing_relationships.html",
    "title": "Describing relationships",
    "section": "",
    "text": "In statistics, a relationship between two or more numerical variables means when one variable changes, the other variable/s will also change. We can describe these relationships in terms of direction, strength, and form.\nTo determine the relationship between two variables qualitatively with by visualising our data with scatter plots.\n\n\nDirection\nA relationship can be positive, negative, or non-existent.\n\nPositive relationship: when one variable increases, the other variable also increases, e.g.¬†the amount of rainfall received and the resulting crop yield.\nNegative relationship: when one variable increases, the other variable decreases, e.g.¬†the levels of insulin and glucose in blood.\nNo relationship: when one variable changes, the other variable does not change, e.g.¬†shoe size and IQ.\n\nStrength\nThe strength of a relationship refers to how closely the two variables are related - weak, moderate, strong, very strong and perfect. A weak relationship will have more scatter than a strong relationship.\nForm\nThe form of a relationship refers to the shape of the relationship. The simplest form is a straight line or linear relationship. Some examples of non-linear forms include polynomials (e.g.¬†quadratic, cubic), exponential, and logistic.\n\n\n\n\n\nTo measure the relationship quantitatively, we use correlation coefficients. These are numbers between -1 and 1, where -1 indicates a perfect negative relationship, 0 indicates no relationship, and 1 indicates a perfect positive relationship.\nThe most common correlation coefficient is the Pearson correlation coefficient (\\(r\\)). It measures linear relationships between two numerical variables.\nPearson‚Äôs Correlation (r) Formula:\n\\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \\]\nIn essence, it is the covariance divided by the product of the standard deviations.\nIn R, we use the function cor() to calculate the correlation coefficient. By default, it calculates the Pearson correlation coefficient.\n\ncor(x, y) \n\nDescribing the correlation coefficient in terms of strength can be a little subjective. Below are some approximate ranges for the common terms.\n\n|0 - 0.1| : no relationship\n|0.1 - 0.3| : weak\n|0.4 - 0.6| : moderate\n|0.7 - 1| : strong\n|0.9 - 1| : very strong\n\n\n\n\nIn the case of non-linear relationships, it is best not to use the Pearson‚Äôs correlation coefficient. Instead, we can use the Spearman‚Äôs rank correlation coefficient (\\(r_{s}\\)) or Kendall‚Äôs tau (\\(\\tau\\)). Note, these relationships must still be monotonic.\n\nMonotonic: a relationship that is consistently increasing or decreasing\nLinear: a relationship that is increasing or decreasing at a constant rate\n\nTo use either Spearman‚Äôs rank correlation coefficient or Kendall‚Äôs tau, we can use the cor() function with the method argument set to either \"spearman\" or \"kendall\".\n\ncor(x, y, method = \"spearman\")\ncor(x, y, method = \"kendall\")\n\nSpearman‚Äôs rank correlation coefficient essentially ranks the data (e.g.¬†the smallest value will be ranked 1, the second smallest 2, etc.) for both the x and y axis, and then calculates the Pearson correlation coefficient for the ranks. Thus it works for any monotonic relationship.\nKendall‚Äôs tau looks at all possible x and y pairs, and determines if they are concordant. e.g.¬†one pair of points \\((x_{i}, y_{i})\\) is concordant with another pair \\((x_{j}, y_{j})\\) if \\(x_{i} &gt; x_{j}\\) and \\(y_{i} &gt; y_{j}\\) or \\(x_{i} &lt; x_{j}\\) and \\(y_{i} &lt; y_{j}\\). Kendall‚Äôs tau is then calculated with:\n\\[ \\tau = \\frac{\\text{(number of concordant pairs)} - \\text{(number of discordant pairs)}}{\\text{(total number of pairs)}}\\]\n\n\n\n\nCorrelation does not imply causation. Just because two variables are correlated does not mean that one causes the other to change. Some example of spurious (i.e.¬†ridiculous) correlations can be found at Spurious Correlations.\nBefore conducting an experiment to collect data (or analysing existing data), it is important to have a hypothesis about the relationship between the variables. Is there reason to believe the two variables should have a relationship? If not, any ‚Äòrelationship‚Äô found via scatter plots or correlations are unlikely meaningful.",
    "crumbs": [
      "**üìò Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html#scatter-plots",
    "href": "module03/040-describing_relationships.html#scatter-plots",
    "title": "Describing relationships",
    "section": "",
    "text": "Direction\nA relationship can be positive, negative, or non-existent.\n\nPositive relationship: when one variable increases, the other variable also increases, e.g.¬†the amount of rainfall received and the resulting crop yield.\nNegative relationship: when one variable increases, the other variable decreases, e.g.¬†the levels of insulin and glucose in blood.\nNo relationship: when one variable changes, the other variable does not change, e.g.¬†shoe size and IQ.\n\nStrength\nThe strength of a relationship refers to how closely the two variables are related - weak, moderate, strong, very strong and perfect. A weak relationship will have more scatter than a strong relationship.\nForm\nThe form of a relationship refers to the shape of the relationship. The simplest form is a straight line or linear relationship. Some examples of non-linear forms include polynomials (e.g.¬†quadratic, cubic), exponential, and logistic.",
    "crumbs": [
      "**üìò Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html#correlation",
    "href": "module03/040-describing_relationships.html#correlation",
    "title": "Describing relationships",
    "section": "",
    "text": "To measure the relationship quantitatively, we use correlation coefficients. These are numbers between -1 and 1, where -1 indicates a perfect negative relationship, 0 indicates no relationship, and 1 indicates a perfect positive relationship.\nThe most common correlation coefficient is the Pearson correlation coefficient (\\(r\\)). It measures linear relationships between two numerical variables.\nPearson‚Äôs Correlation (r) Formula:\n\\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \\]\nIn essence, it is the covariance divided by the product of the standard deviations.\nIn R, we use the function cor() to calculate the correlation coefficient. By default, it calculates the Pearson correlation coefficient.\n\ncor(x, y) \n\nDescribing the correlation coefficient in terms of strength can be a little subjective. Below are some approximate ranges for the common terms.\n\n|0 - 0.1| : no relationship\n|0.1 - 0.3| : weak\n|0.4 - 0.6| : moderate\n|0.7 - 1| : strong\n|0.9 - 1| : very strong\n\n\n\n\nIn the case of non-linear relationships, it is best not to use the Pearson‚Äôs correlation coefficient. Instead, we can use the Spearman‚Äôs rank correlation coefficient (\\(r_{s}\\)) or Kendall‚Äôs tau (\\(\\tau\\)). Note, these relationships must still be monotonic.\n\nMonotonic: a relationship that is consistently increasing or decreasing\nLinear: a relationship that is increasing or decreasing at a constant rate\n\nTo use either Spearman‚Äôs rank correlation coefficient or Kendall‚Äôs tau, we can use the cor() function with the method argument set to either \"spearman\" or \"kendall\".\n\ncor(x, y, method = \"spearman\")\ncor(x, y, method = \"kendall\")\n\nSpearman‚Äôs rank correlation coefficient essentially ranks the data (e.g.¬†the smallest value will be ranked 1, the second smallest 2, etc.) for both the x and y axis, and then calculates the Pearson correlation coefficient for the ranks. Thus it works for any monotonic relationship.\nKendall‚Äôs tau looks at all possible x and y pairs, and determines if they are concordant. e.g.¬†one pair of points \\((x_{i}, y_{i})\\) is concordant with another pair \\((x_{j}, y_{j})\\) if \\(x_{i} &gt; x_{j}\\) and \\(y_{i} &gt; y_{j}\\) or \\(x_{i} &lt; x_{j}\\) and \\(y_{i} &lt; y_{j}\\). Kendall‚Äôs tau is then calculated with:\n\\[ \\tau = \\frac{\\text{(number of concordant pairs)} - \\text{(number of discordant pairs)}}{\\text{(total number of pairs)}}\\]",
    "crumbs": [
      "**üìò Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html#causation",
    "href": "module03/040-describing_relationships.html#causation",
    "title": "Describing relationships",
    "section": "",
    "text": "Correlation does not imply causation. Just because two variables are correlated does not mean that one causes the other to change. Some example of spurious (i.e.¬†ridiculous) correlations can be found at Spurious Correlations.\nBefore conducting an experiment to collect data (or analysing existing data), it is important to have a hypothesis about the relationship between the variables. Is there reason to believe the two variables should have a relationship? If not, any ‚Äòrelationship‚Äô found via scatter plots or correlations are unlikely meaningful.",
    "crumbs": [
      "**üìò Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html",
    "href": "module03/042-linear_functions_multi.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The formula or function for multiple linear regression is:\n\\[ y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i \\]\nTherefore, estimating the model involves estimating the values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), ‚Ä¶, \\(\\beta_k\\).\n\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) to \\(\\beta_k\\) are the partial regression coefficients\n\\(\\epsilon\\) is the error term (aka residuals)\n\nFor simple linear regression (response and one predictor), we can visualise this with a scatterplot (response on the y-axis, predictor on the x-axis) and a line of best fit. Two variables, two dimensions, a 2D plot. When we have more than one predictor - this becomes more difficult to visualise.\nLikewise, interpretation becomes more complex. If \\(x_1\\) increases by one unit, \\(y_i\\) increases by \\(\\beta_1\\) units, if all other variables are held constant. This is why the coefficients in multiple linear regression are referred to as partial regression coefficients.\n\n\nThe principle of parsimony is the idea that the simplest explanation is the best explanation. In the context of multiple linear regression, this means we need to consider whether adding more variables is actually useful. This is because the more variables we include, the more complex the model becomes, and the more likely it is to overfit the data. If we have too few variables, we could underfit the data (low % variance explained).\n\nIf a model with 3 variables has the same predictive power as a model with 10 variables, it is better to use the model with 3 variables. A model with 3 variables is easier to fit and interpret, and it means the other 7 variables were not adding much to the model and could be culled.\nThe process of selecting the correct variables is called variable selection. There are methods to do this (e.g.¬†stepwise regression with step()) but they are beyond the scope of this unit. A simple method is to remove predictors that do not appear significant in the model (ideally one by one) until all remaining variables are significant. This should be done in conjunction with checking the adjust R2 to ensure the model does not decline drastically in predictive capability.",
    "crumbs": [
      "**üìò Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#principle-of-parsimony",
    "href": "module03/042-linear_functions_multi.html#principle-of-parsimony",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The principle of parsimony is the idea that the simplest explanation is the best explanation. In the context of multiple linear regression, this means we need to consider whether adding more variables is actually useful. This is because the more variables we include, the more complex the model becomes, and the more likely it is to overfit the data. If we have too few variables, we could underfit the data (low % variance explained).\n\nIf a model with 3 variables has the same predictive power as a model with 10 variables, it is better to use the model with 3 variables. A model with 3 variables is easier to fit and interpret, and it means the other 7 variables were not adding much to the model and could be culled.\nThe process of selecting the correct variables is called variable selection. There are methods to do this (e.g.¬†stepwise regression with step()) but they are beyond the scope of this unit. A simple method is to remove predictors that do not appear significant in the model (ideally one by one) until all remaining variables are significant. This should be done in conjunction with checking the adjust R2 to ensure the model does not decline drastically in predictive capability.",
    "crumbs": [
      "**üìò Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#exploratory-data-analysis",
    "href": "module03/042-linear_functions_multi.html#exploratory-data-analysis",
    "title": "Multiple Linear Regression",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nFirst we will need to remove the missing values in the dataset.\n\nsummary(airquality)\ndata &lt;- na.omit(airquality) # remove NA and rename as new object (to not change original dataset)\n\nNext we will look at the relationships between Ozone and the other variables in the dataset. Below are some potential functions that can visualise the data and correlations ‚Äì choose the one that works best visually for your data.\n\n# ### Base R\n# pairs(data)\n# cor(data) |&gt; round(2)\n# \n# ### corrplot\n# library(corrplot)\n# corrplot::corrplot(cor(data), method = \"circle\")\n\n### psych\npsych::pairs.panels(data)\n\nThe key thing to check is the plots and correlations between Ozone and the predictors (first column, first row). The best correlated variables are Temp (\\(r = 0.7\\)), Wind (\\(r = -0.61\\)), and Solar.R (\\(r = 0.35\\)). Month is negligibly correlated (\\(r = 0.14\\)) and Day has no relationship (\\(r = -0.01\\)).\nWe can see that Ozone does not have a linear relationship with Temp, Solar.R or Wind, so we do a pre-emptive natural log transformation.\n\ndata &lt;- data %&gt;%\n  mutate(Ozone_log = log(Ozone)) %&gt;%  # create the log-transformed column\n  select(-Ozone) %&gt;%                  # remove the original Ozone column\n  select(Ozone_log, everything())     # reorder so that Ozone_log is first\n\npsych::pairs.panels(data)\n\nThe linearity between Ozone_log and both Temp and Solar.R are higher, and for Wind it is slightly lower. The scatterplots however are looking more linear.",
    "crumbs": [
      "**üìò Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#assumptions",
    "href": "module03/042-linear_functions_multi.html#assumptions",
    "title": "Multiple Linear Regression",
    "section": "Assumptions",
    "text": "Assumptions\nThe assumptions for multiple linear regression are the same as simple linear regression - except one additional condition, called collinearity. This is when two or more predictors are highly correlated with each other. For the assumption to be broken, it requires a perfect relationship (\\(r\\) = -1 or 1), but strong and very correlations will still have an effect. This can cause problems with the model, as the model cannot distinguish between the two predictors and the resulting relationships are unreliable. Judging from the correlations above, this is not an issue for this dataset.\nWe fit the model with all variables and create our assumption plots.\n\nmod &lt;- lm(Ozone_log ~ Temp + Wind + Solar.R + Month + Day, data = data)\npar(mfrow = c(2,2))\nplot(mod)\n\nThe residuals vs fitted plot is fairly evenly distributed around the line, and the red line is mostly flat. The normal QQ sticks to the line well except one outlier - we cross-check with the residuals vs leverage plot, but the point does not exceed the dotted Cook‚Äôs distance line so it is not extreme enough to need action. Scale-location is slightly tilted but the line is fairly straight, and the points are evenly distributed so variances are equal.\nAll assumptions are met.",
    "crumbs": [
      "**üìò Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#model-evaluation",
    "href": "module03/042-linear_functions_multi.html#model-evaluation",
    "title": "Multiple Linear Regression",
    "section": "Model evaluation",
    "text": "Model evaluation\nLike simple linear regression, there is a hypothesis for multiple linear regression. The null hypothesis is that all partial regression coefficients are equal to zero, and the mean is a better predictor or line of best fit. The alternate hypothesis is that at least one of the partial regression coefficients is not equal to zero, and there is merit in the multiple linear regression model.\n\\[ H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0 \\] \\[ H_1: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0 \\]\n\nsummary(mod)\n\nWe can see that the p-value for the F-statistic is very low, so we fail to reject the alternate hypothesis, and the model is better than using the mean of the data. The Adjusted R-squared is 65.36%, which means that 65.36% of the variance in Ozone_log can be explained by the predictors. The Residual standard error is 0.5096.\nTemp, Wind and Solar.R are all significant predictors of Ozone_log, but Month and Day are not. So we try removing them from the model.\n\nmod &lt;- lm(Ozone_log ~ Temp + Wind + Solar.R, data = data)\nsummary(mod)\n\nTemp, Wind and Solar.R are all still significant predictors of Ozone_log. The Adjusted R-squared is 65.5% (which is higher) and the Residual standard error is 0.5086 (which is lower). This means that the model is slightly better without Month and Day.\nFollowing the principle of parsimony, the model with just Temp, Wind and Solar.R is the more parsimonious model, and the model we will use to predict Ozone_log.",
    "crumbs": [
      "**üìò Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#interpretation",
    "href": "module03/042-linear_functions_multi.html#interpretation",
    "title": "Multiple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\nThe equation for our model is \\(log(\\text{Ozone}) = -0.026 + 0.049 \\cdot \\text{Temp} - 0.062 \\cdot \\text{Wind} + 0.003 \\cdot \\text{Solar.R}\\). For a unit increase in Temp, log(Ozone) increases by 0.049 units, and Ozone increases by \\(e^0.049 = 1.05\\) times or approximately 4.9%, given all other predictors are held constant. The most useful interpretation here is the percentage increase. Given all partial regression coefficient are \\(&lt; |0.25|\\), we can use a quick approximation (\\(\\times 100 \\%\\)).\n\nFor every one unit increase in Temp, Ozone increases by 4.9%, given all other predictors are held constant\nFor every one unit increase in Wind, Ozone decreases by 6.2%, given all other predictors are held constant\nFor every one unit increase in Solar.R, Ozone increases by 0.3%, given all other predictors are held constant\n\nThe variation in Ozone concentration in 1973 New York could be mostly (adjusted R2 = 65.5%) explained by the weather, i.e.¬†temperature, wind speed, and solar radiation.",
    "crumbs": [
      "**üìò Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX1002 Handbook",
    "section": "",
    "text": "Welcome to the ENVX1002 lab handbook. This handbook is designed to be an optional companion to the lectures, labs and assessments for the course."
  },
  {
    "objectID": "index.html#how-to-use-this-handbook",
    "href": "index.html#how-to-use-this-handbook",
    "title": "ENVX1002 Handbook",
    "section": "How to use this handbook",
    "text": "How to use this handbook\nRead this handbook before, or after each lecture to better understand certain concepts. Lab exercises are also included in this handbook, which you can use to prepare for the lab sessions.\nTo begin, we strongly suggest that you read our introduction chapter, which will provide you with an overview of why ENVX1002 is important for your degree and includes some introductory statistical concepts.\n\n\n\n\n\n\nAcknowledgements\n\n\n\n\n\nWe would like to acknowledge the work of previous generations of teaching staff who created the bulk of the teaching material within this manual. In particular, the work of:\n\nAssoc. Prof.¬†Mick O‚ÄôNeill\nDr.¬†Kathryn Aufflick\nAssoc. Prof.¬†Peter Thomson\nProf.¬†Thomas Bishop\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\n\n\nThis handbook is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nWhen you are ready, check the sidebar to get started."
  },
  {
    "objectID": "labs/Lab07.html",
    "href": "labs/Lab07.html",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nLearn to use R to calculate a chi-squared test for:\n\nTest of proportions\nTest of independence\n\nLearn how to interpret statistical output.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#before-we-begin",
    "href": "labs/Lab07.html#before-we-begin",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Before we begin",
    "text": "Before we begin\nCreate your Quarto document and save it as Lab-07.qmd or similar. There are no data files to download for this lab.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#quick-introduction",
    "href": "labs/Lab07.html#quick-introduction",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Quick introduction",
    "text": "Quick introduction\nThe chi-square test is used to compare the observed distribution to an expected distribution, in a situation where we have two or more categories in a discrete data. In other words, it compares multiple observed proportions to expected probabilities.\nThe formula is:\n\\[\\chi^2 = \\sum_{i=1}^{k}\\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) is the observed frequency, \\(E_i\\) is the expected frequency for each categorym and \\(k\\) is the number of categories.\nFor more information about the technique, consult your lecture slides and tutorial 7.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-1-wild-tulips-walk-through",
    "href": "labs/Lab07.html#exercise-1-wild-tulips-walk-through",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Exercise 1: wild tulips (walk-through)",
    "text": "Exercise 1: wild tulips (walk-through)\n\nBackground\nSuppose we collected wild tulips and found that 81 were red, 50 were yellow and 27 were white. Are these colours equally common?\nIf these colours were equally distributed, the expected proportion would be 1/3 for each of the colour. Therefore, we want to test if the observed proportions are significantly different from the expected proportions.\nThe data is below.\n\ntulip &lt;- c(81, 50, 27)\n\n\n\nInstructions\nUtilise the HATPC process and test the hypothesis that the proportion of flower colours of tulips are equally common, assuming that the samples are independent. We can explore the data as we check the assumptions of the test.\n\n\nHATPC:\n\nHypothesis\nAssumptions\nTest (statistic)\nP-value\nConclusion\n\n\n\n\n\n\n\nLevel of significance\n\n\n\nThe level of significance is usually set at 0.05. This value is generally accepted in the scientific community and is also linked to Type 2 errors, where choosing a lower significance increases the likelihood of failing to reject the null hypothesis when it is false.\n\n\n\n\nHypotheses\nWhat are the null hypothesis and alternative hypotheses?\n\n\nClick here to view answer\n\n\nH0: There is no significant difference between the observed and the expected proportions of flower colours.\n\nH1: There is a significant difference between the observed and the expected proportions of flower colours.\n\n\n\n\n\nAssumptions\nRecall that the assumptions of the \\(\\chi^2\\) test are:\n\nNo cell has expected frequencies less than 1\nNo more than 20% of cells have expected frequencies less than 5\n\n\n\n\n\n\n\nNote\n\n\n\nIn the case that the above assumptions are violated then the probability of a type 1 error occurring (rejecting the null hypothesis when it is true, i.e.¬†false positive) increases.\n\n\nTo calculate expected frequencies, we first calculate the total number of tulips and then divide by the number of categories.\n\nexpected &lt;- rep(sum(tulip) * 1 / 3, 3) #rep function replicated the value we're calculating inside the brackets\nexpected\n\nDoes the data satisfy the assumptions of a \\(\\chi^2\\) test?\n\n\nClick here to view answer\n\nYes, as expected frequencies &gt; 5.\n\n\n\nTest statistic\nThe chisq.test() function in R is used to calculate the chi-squared test.\n\nres &lt;- chisq.test(tulip, p = c(1 / 3, 1 / 3, 1 / 3))\nres\n\nNote that we coud check our assumptions post-analysis by checking the expected frequencies stored in the expected object of the output:\n\nres$expected\n\n\n\nP-value\nWrite down how you should report the critical value, p-value and df in a scientific paper?\n\n\nClick here to view answer\n\n\\(\\chi^2 = 27.9, d.f. =2, p &lt; 0.01\\)\n\n\n\nConclusions\nBased on the p-value, do we accept or reject the null hypothesis?\n\n\nClick here to view answer\n\nWe reject the null hypothesis as the p-value is less than 0.05.\n\nNow write a scientific (biological) conclusion based on the outcome.\n\n\nClick here to view answer\n\nThere is a significant difference in the proportion of flower colours of tulips (\\(\\chi^2 = 27.9, d.f. =2, p &lt; 0.01\\)).",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-2-hermit-crabs",
    "href": "labs/Lab07.html#exercise-2-hermit-crabs",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Exercise 2: hermit crabs",
    "text": "Exercise 2: hermit crabs\n\nBackground\nIn a study of hermit crab behaviour at Point Lookout, North Stradbroke Island, a random sample of 3 types of gastropod shells was collected. Each shell was then scored as being either occupied by a hermit crab or empty. Do hermit crabs prefer a certain shell?\n\n\n\nShell species\nOccupied\nEmpty\n\n\n\n\nAustrochochlea\n47\n42\n\n\nBembicium\n10\n41\n\n\nCirithid\n125\n49\n\n\n\nThe data is stored in a table object in R below. Note that it is different from a data.frame object. You can verify this by using the str() or class() functions.\n\ncrabs &lt;- as.table( #Make a table with values for each row\n  rbind(\n    Aus = c(47, 42),\n    Bem = c(10, 41),\n    Cir = c(125, 49)\n  )\n)\n\ncolnames(crabs) &lt;- c(\"Occupied\", \"Empty\") #Add column names to table\nstr(crabs)\ncrabs\n\n\nData exploration\nSince we have a multi-dimensional dataset, we can try to plot the data to visualise it.\nA mosaic plot is a graphical representation of the data in a two-way contingency table. It is a way of visualising the relationship between two categorical variables.\nTry the code below. Can you interpret the plot?\n\n# mosaic plot of crabs\nmosaicplot(crabs, main = \"Hermit crabs and shell species\")\n\n\n\nClick here to view interpretation\n\nThe plot shows that the distribution of hermit crabs in the different shell species is not equal. The majority of hermit crabs are found in the Cirithid shell species, followed by Austrochochlea and Bembicium. This can be observed by the width of the boxes in the plot.\nThere are also differences in the number of empty shells in the different shell species. The Bembicium shell species has the highest number of empty shells, followed by Austrochochlea and Cirithid. This can be observed by the height of the boxes in the plot.\n\n\n\n\nHATPC\nNow it is your turn to test the hypothesis that the three shell species are equally preferred by hermit crabs. Follow the HATPC process with the following questions in mind (but you don‚Äôt have to answer them individually):\n\nWhat are the null hypothesis and alternative hypotheses?\nDoes the data satisfy the assumptions of a \\(\\chi^2\\) test?\nHow should you report the critical value, p-value and df in a scientific paper?\nBased on the p-value, do we accept or reject the null hypothesis?\nWrite a scientific (biological) conclusion based on the outcome.\n\nUse the markdown structure below to answer the questions (if you wish):\n## Exercise 2: hermit crabs\n### Hypothesis\n### Assumptions\n### Test statistic\n### P-value\n### Conclusions\nTake your time, and when you are ready, check your answers with your demonstrators.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#done",
    "href": "labs/Lab07.html#done",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Done!",
    "text": "Done!\nThis is the end of the lab. Remember to save your work, render your document and ask your demonstrators for feedback if you are unsure about your answers.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-1-national-rspca-statistics",
    "href": "labs/Lab07.html#exercise-1-national-rspca-statistics",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Exercise 1: National RSPCA statistics",
    "text": "Exercise 1: National RSPCA statistics\nThe RSPCA releases statistics on the number of animals they receive, reclaim and rehome every year. In the 2023-24 financial year, the RSPCA received 17468 dogs, 26704 cats, and 37497 other animals. The ‚Äúother‚Äù category includes horses, small animals, livestock and wildlife.\nUsing the HATPC framework, test whether these animals were received in equal proportions.\n\nreceived &lt;- c(17468, 26704, 37497)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-2-uc-berkeley-admissions",
    "href": "labs/Lab07.html#exercise-2-uc-berkeley-admissions",
    "title": "Lab 07 ‚Äì Chi-squared test",
    "section": "Exercise 2: UC Berkeley Admissions",
    "text": "Exercise 2: UC Berkeley Admissions\nFor the two exercises we‚Äôre going to use simplified versions of the inbuilt dataset ‚ÄòUCBAdmissions‚Äô, which has data on student admissions for Berkeley. The dataset shows how many students were rejected and admitted to the university by both department and gender.\n\n2.1 Admissions by department\nDid every department at UC Berkeley admit students in equal proportions?\n\ndept_admissions &lt;- c(601, 370, 322, 269, 147, 46)\n\n\n\n2.2\nAre male and female students admitted and rejected in the same proportion?\n\ngender &lt;- as.table( #Make a table with values for each row\n  rbind(\n    admitted = c(1198, 557),\n    rejected = c(1493, 1278)\n  )\n)\n\ncolnames(gender) &lt;- c(\"Male\", \"Female\") #Add column names to table",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab06.html",
    "href": "labs/Lab06.html",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 2-sample t-test\n\nindependent samples with constant variance\nindependent samples with unequal variance\npaired samples\ndata transformations\n\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#before-you-begin",
    "href": "labs/Lab06.html#before-you-begin",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-06.Rmd or similar. The following data files are required:\n\nBarley.csv\nPlant_growth.csv\nTurbidity.csv\n\nThe following external packages are used in this lab. Install them if you have not done so already.\ninstall.packages(c(\"tidyverse\", \"car\"), \n  repo = \"https://cloud.r-project.org\")\nFinally, try to complete today‚Äôs lab exercises in pairs and try out pair programming, where one person writes the code and the other person reviews each line as it is written. You can swap roles every 10 minutes or so. This is a great way to learn from each other and to improve your coding skills.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-1-barley-walk-through",
    "href": "labs/Lab06.html#exercise-1-barley-walk-through",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Exercise 1: barley (walk-through)",
    "text": "Exercise 1: barley (walk-through)\n\nBackground\nAn experiment was designed to compare two varieties of spring barley. Thirty four plots were used, seventeen being randomly allocated to variety A and seventeen to variety B. Unfortunately five plots were destroyed. The yields (t/ha) from the remaining plots were as they appear in the file Barley.csv.\n\n\nInstructions\nFirst, quickly explore the data; then, utilise the HATPC process and test the hypothesis that the two varieties give equal yields, assuming that the samples are independent.\n\n\nHATPC:\n\nHypothesis\nAssumptions\nTest (statistic)\nP-value\nConclusion\n\n\n\n\n\n\n\nLevel of significance\n\n\n\nThe level of significance is usually set at 0.05. This value is generally accepted in the scientific community and is also linked to Type 2 errors, where choosing a lower significance increases the likelihood of failing to reject the null hypothesis when it is false.\n\n\n\n\nData exploration\nFirst we load the data and inspect its structure to see if it needs to be cleaned or transformed. The glimpse() function is a tidy version of str() that provides a quick overview of the data that focuses on the variables, ignoring data attributes.\n\n\nTry to compare str() and glimpse() to see what the differences are.\n\nbarley &lt;- readr::read_csv(\"data/Barley.csv\") # packagename:: before a function lets you access a function without having to load the whole library first\ndplyr::glimpse(barley)\n\nRows: 29\nColumns: 2\n$ Variety &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A‚Ä¶\n$ Yield   &lt;dbl&gt; 4.6, 4.3, 3.8, 3.4, 3.9, 3.9, 3.9, 4.4, 3.6, 3.6, 4.7, 3.9, 3.‚Ä¶\n\n\nThe Variety column is a factor with two levels, A and B, but it is defined as a character. We can convert it to a factor using the mutate() function from the dplyr package, but it is not necessary for the t-test since R will automatically convert it to a factor.\n\nlibrary(tidyverse)\nbarley &lt;- mutate(barley, Variety = as.factor(Variety))\n\nQuickly preview the data as a plot to see if there are any trends or unusual observations.\n\nbarley %&gt;%\n  ggplot(aes(x = Variety, y = Yield)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nA trained eye will anticipate that the data may not meet the assumption of equal variance; however, we will test this assumption later. Otherwise, there appear to be no unusual observations in the data.\n\n\nHypothesis\nWhat are the null and alternative hypotheses? We can use the following notation:\n\\[H_0: \\mu_A = \\mu_B\\] \\[H_1: \\mu_A \\neq \\mu_B\\]\nwhere \\(\\mu_A\\) and \\(\\mu_B\\) are the population means of varieties A and B, respectively.\n\n\nIt is important that when using mathematical symbols to denote the null and alternative hypotheses, you should always define what the symbols mean. Otherwise, the reader may not understand what you are referring to.\nThe equations above are written in LaTeX, a typesetting system that is commonly used in scientific writing. You can learn more about LaTeX here. The raw syntax used to write the equations are shown below:\n$$H_0: \\mu_A = \\mu_B$$\n$$H_1: \\mu_A \\neq \\mu_B$$\nWhy do we always define the null and alternative hypotheses? In complex research projects or when working in a team, it is important to ensure that everyone is on the same page. By defining the hypotheses, you can avoid misunderstandings and ensure that everyone is working towards the same goal as the mathematical notation is clear and unambiguous.\n\n\nAssumptions\n\nNormality\nThere are many ways to check for normality. Here we will use the QQ-plot. Use of ggplot2 is preferred (as a means of practice) but since we are just exploring data, base R functions are not a problem to use.\n\nUsing ggplot2Using base R\n\n\n\nggplot(barley, aes(sample = Yield)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Variety) #facet_wrap ensures there are separate plots for each variety rather than one plot with all the data in Yield \n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\nqqnorm(barley$Yield[barley$Variety == \"A\"], main = \"Variety A\") # square brackets to subset the data by variety\nqqline(barley$Yield[barley$Variety == \"A\"])\nqqnorm(barley$Yield[barley$Variety == \"B\"], main = \"Variety B\")\nqqline(barley$Yield[barley$Variety == \"B\"])\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Do the plots indicate the data are normally distributed?\nAnswer: Yes, the data appear to be normally distributed as the QQ-plot shows that the data points are close to the line.\n\n\nHomogeneity of variance\nFrom the boxplot, we can see that there is some indication that the variances are not equal. We can test this assumption using Bartlett‚Äôs test or Levene‚Äôs test; here we will just use Bartlett‚Äôs test.\n\nbartlett.test(Yield ~ Variety, data = barley)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Yield by Variety\nBartlett's K-squared = 14.616, df = 1, p-value = 0.0001318\n\n\nQuestion: Does the Bartlett‚Äôs test indicate the two groups have equal variances? What effect will that have on the analysis?\nAnswer: The two groups have unequal variance (Bartlett‚Äôs test: \\(X^2 = 14.6\\), \\(p &lt; 0.01\\)). This means that we will need to use the Welch‚Äôs t-test, which does not assume equal variances.\n\n\n\nTest statistic\nWe can now calculate the test statistic using the t.test() function in R. Since the variances are unequal, we do not have to specify the var.equal argument ‚Äì the default test for t.test() is the Welch‚Äôs t-test which does not assume equal variances.\n\nt.test(Yield ~ Variety, data = barley)\n\n\n    Welch Two Sample t-test\n\ndata:  Yield by Variety\nt = -4.9994, df = 19.441, p-value = 7.458e-05\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -0.9293569 -0.3814274\nsample estimates:\nmean in group A mean in group B \n       4.052941        4.708333 \n\n\n\n\nP-value\nSince the p-value is &lt; 0.05, we can reject the null hypothesis that the mean yield of both varieties is equal.\n\n\nConclusion\nThe conclusion needs to be brought into the context of the study. In a scientific report or paper, you would write something like this:\n\nThe mean yield of barley variety A was significantly different from that of variety B (\\(t = -5.0\\), \\(df = 19.4\\), \\(p &lt; 0.01\\)).",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-2-plant-growth",
    "href": "labs/Lab06.html#exercise-2-plant-growth",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Exercise 2: plant growth",
    "text": "Exercise 2: plant growth\n\nBackground\nIn a test of a particular treatment aimed at inducing growth, 20 plants were grouped into ten pairs so that the two members of each pair were as similar as possible. One plant of each pair was chosen randomly and treated; the other was left as a control. The increases in height (in centimetres) of plants over a two-week period are given in the file Two week plant heights. We wish to compare whether the treatment is actually inducing improved growth, as compared to the control.\n\n\nInstructions\nHere, we have two samples, and the samples are paired as it is a before-after experiment. So we‚Äôd like to conduct a paired t-test.\nFor paired t-tests the analysis is performed as a 1-sample t-test on the difference between each pair so the only assumption is the normality assumption.\nCopy the structure below and perform your analysis in your document.\n## Exercise 2: plant growth\n### Data exploration\n### Hypothesis\n### Assumptions\n#### Normality\n#### Homogeneity of variance\n### Test statistic\n### P-value\n### Conclusion\nNote that the data is not tidy. The code below will convert the data to the long format and assign it to tidy_plant.\n\nplant_growth &lt;- readr::read_csv(\"data/Plant_growth.csv\")\n\ntidy_plant &lt;- plant_growth %&gt;%\n  pivot_longer(cols = c(Treated, Control), names_to = \"Treatment\", values_to = \"Height\")\n\nYou may also need to perform a Shapiro-Wilk test to check for normality. To do this for each group, you can use the tapply() function.\n\ntapply(tidy_plant$Height, tidy_plant$Treatment, shapiro.test)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-3-turbidity",
    "href": "labs/Lab06.html#exercise-3-turbidity",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Exercise 3: turbidity",
    "text": "Exercise 3: turbidity\n\nBackground\nA new filtering process was installed at a dam which provided drinking water for a nearby town. To check on its success, a number of water samples were taken at random times and locations in the weeks before and after the process was installed. The following are the turbidity values (units = NTU) of the water samples.\n\n\nInstructions\nNow we consider further examples of a two-sample t-test, but where the assumption of equal variance and normality may not be met for the raw data. Sometimes after applying a data transformation the analysis can proceed assuming equal variances ‚Äì but always check after a data transformation.\nThe data can be read with the code below:\n\nturbidity &lt;-read_csv(\"data/Turbidity.csv\")\n\nFor data transformation, you may need to create a new variable in your dataset to store the transformed data. For example, to create a new variable TurbLog10 that stores the log10 transformed turbidity values, you can use the following code:\n\nturbidity$TurbLog10 &lt;- log10(turbidity$Turbidity)\n\nTo interpret the results for your conclusions, you may need to back-transform the mean and/or confidence interval values. To back transform log10 data you use:\n\\[10^{\\text{mean or CI}}\\]\nTo back-transform natural log, loge, you use:\n\\[e^{\\text{mean or CI}}\\]",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-1-tooth-growth",
    "href": "labs/Lab06.html#exercise-1-tooth-growth",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Exercise 1: Tooth growth",
    "text": "Exercise 1: Tooth growth\nTooth GRowth is an inbuilt dataset that shows the effect of vitamin c in Guinea pig tooth growth. It has three variables:\n\nlen = tooth length\nsupp = type of supplement (Orange juice or ascorbic acid)\ndose = mg/day gieven to the guinea pigs\n\n\nhead(ToothGrowth)\nstr(ToothGrowth)\n\nUsing the HATPC framework, test whether the type of supplent affects tooth length.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-2-adelie-penguin-bill-length",
    "href": "labs/Lab06.html#exercise-2-adelie-penguin-bill-length",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Exercise 2: Adelie penguin bill length",
    "text": "Exercise 2: Adelie penguin bill length\nFor this exercise, we will be using a subset of the palmer penguins dataset.\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nadelie &lt;-penguins%&gt;%\n  filter(species == \"Adelie\")%&gt;%\n  na.omit()%&gt;%\n  droplevels()\n\nhead(adelie)\n\nUsing the HATPC framework, test whether male and female penguins have the same length bill",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-3-penguin-body-mass",
    "href": "labs/Lab06.html#exercise-3-penguin-body-mass",
    "title": "Lab 06 ‚Äì Two-sample t-test",
    "section": "Exercise 3: Penguin body mass",
    "text": "Exercise 3: Penguin body mass\nFor this exercise, we will use a subset of the palmer penguins data set again. This time, we will be comparing two different penguin species.\n\nlibrary(palmerpenguins)\n\npenguins2&lt;- penguins%&gt;%\n  filter(species != \"Adelie\")%&gt;%\n  na.omit()%&gt;%\n  droplevels()\n\nUsing the HATPC framework, test whether chinstrap and gentoo penguins have different body masses.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab08.html",
    "href": "labs/Lab08.html",
    "title": "Lab 08 ‚Äì üö´ No exercises",
    "section": "",
    "text": "There are no exercises this week to make room for the evaluation task.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 08 -- üö´ No exercises"
    ]
  },
  {
    "objectID": "labs/Lab04.html",
    "href": "labs/Lab04.html",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "",
    "text": "Project 1 is due in week 5 and for many of you this may be your first university assignment; some may be nervous, while others may be more relaxed. Your demonstrators will use the start of this practical to discuss how you may be feeling for this first assessment and share ways you might want to approach and prepare for the assessment.\nThis is the last reflective activity for now, thank you all for contributing so far and we hope you have found some benefit in the activities. Now for some probability!\n\n\n\n\n\n\nLearning outcomes\n\n\n\nAt the end of this computer practical, students should be able to:\n\ncalculate tail, interval and inverse probabilities associated with the Normal distribution\ncalculate probabilities associated sampling distribution of the sample mean by using simulation in R and using R commands.\n\n\n\nLink to data is below:\n\nENVX1002_Data4.xlsx\nAlternatively download from Canvas",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "href": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "Exercise 1 - Class activity - How tall is ENVX1002??",
    "text": "Exercise 1 - Class activity - How tall is ENVX1002??\n\nSet up a new PROJECT for Lab 4 and create a quarto document called Lab_4.qmd and save it in your project directory.\nRecord the height of all male and female students in the class up on the board (we will try to provide a tape if you are unsure about your height)\nEnter the data into r, for example:\n\n\nfemale_height &lt;- c(176, 180, 187, 168)\nmale_height &lt;- c(175, 183, 163, 190)\n\n\nCalculate the mean and standard deviation using R and graph the distribution for both genders, for example\n\n\nf_mean &lt;- mean(female_height)\nf_sd &lt;- sd(female_height)\nhist(female_height)\n\n\nDiscuss with your neighbour or post on the zoom chat, which model (distribution function) do you think would fit the data\nHow does your class compare to the Australian statistics. For this we will look at the mean and standard deviation of measured heights for men and women aged 18 - 24 from the ABS for 1995 see page 13 of\n\nhttps://www.ausstats.abs.gov.au/Ausstats/subscriber.nsf/Lookup/CA25687100069892CA256889001F4A36/$File/43590_1995.pdf\nNote that both reported and measured heights are provided and not surprisingly reported heights are bigger that the measured :o)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-2---milkfat-example",
    "href": "labs/Lab04.html#exercise-2---milkfat-example",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "Exercise 2 - Milkfat example",
    "text": "Exercise 2 - Milkfat example\n\nPart 1\nThe milkfat content in milk (in %) for 120 cows are presented in the worksheet called ENVX1002_Data4.xlsx. Copy the file into your project directory and:\n\nImport the data into R.\n\n\nlibrary(readxl)\nmilkfat &lt;- read_excel(\"data/ENVX1002_Data4.xlsx\", sheet = \"Milkfat\")\n\n\nCalculate the summary statistics of Milkfat (mean, median and sd)\n\nNote that we use $ColumnName to select a column from the data\n\nmean(milkfat$Milkfat)\n\n\nWhat type of cows could they be? Compare your data to the table in the following link:\n\nhttps://lactalis.com.au/info-center/different-breeds-of-cows/\n\nWhat state could they be from? Check some of the recent Milk Production reports from Dairy Australia. The data can be found in the Average Milkfat & Protein (%) section of the PDF report: The reports can be found at the following link:\n\nhttps://www.dairyaustralia.com.au/resource-repository/2020/09/25/milk-production-report\n\nCould the data be normally distributed?\n\n\nCreate a histogram and boxplot of the milk fat data. Is the data Normally distributed?\n\n\nrequire(ggplot2)\nggplot(milkfat, aes(x = Milkfat)) +\n  geom_histogram(binwidth = 0.1, fill = \"lightblue\", color = \"black\") +\n  xlab(\"Milkfat (%)\")\n\n\nIn the UK, breakfast milk' (orChannel Island milk‚Äô) has 5.5% fat content. What percentage of the cows in this data set is yielding breakfast milk with \\(\\ge\\) 5.5%?\n\n\ns &lt;- sort(milkfat$Milkfat) # Sorts the data\ns # Look at the sorted data\nlength(s[s &gt;= 5.5]) # Counts how many are &gt;= 5.5\n\n\nIn Australia, full cream milk has greater than 3.2% milk fat content. What percentage of these cows is yielding full cream milk?\n\n\n## Your turn\n\n\n\nPart 2\nLet \\(X\\) represent the milk fat content for the population of this breed of cows.\n\nAssuming the population is normal, use the sample mean and standard deviation from the previous question as estimates of the population parameters. So \\(X\\sim (\\mu =..., \\sigma^2 = ...)\\).\nDraw a picture of the curve representing \\(X\\). The below example uses ggplot2 to draw the curve for \\(N(4.16,0.30^2)\\).\n\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(4.16 - 4 * 0.3, 4.16 + 4 * 0.3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.16, sd = 0.30)) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16, 0.30^2) ~ pdf))\n\n\nWhat is the probability that 1 cow has a fat content less than 4%? We will adapt the ggplot command above a picture of this probability and then use R to find the probability.\n\nHint: You may need to use the stat_function command to draw the curve and then use the pnorm command to find the probability.\n\nggplot(data.frame(x = c(4.16 - 4 * 0.3, 4.16 + 4 * 0.3)), aes(x = x)) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = 0.30),\n    geom = \"area\", fill = \"white\"\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = 0.30),\n    xlim = c(4.16 - 4 * 0.3, 4), geom = \"area\", fill = \"red\"\n  ) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16, 0.30^2) ~ pdf))\n\n\npnorm(4, 4.16, 0.30)\n\n\nWhat is the probability that 1 cow (randomly sampled) has a fat content greater than 4.5%? Try and adapt the ggplots above to draw a picture of this probability and then use R to find the probability.\nFor a sample of 10 cows (randomly sampled), what is the probability that the sample mean milk fat content is greater than 4.2%?\n\nHint: First find the distribution of the sample mean \\(\\overline{X}\\). Then find \\(P(\\overline{X}&gt;4.2)\\)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-3---skin-cancer",
    "href": "labs/Lab04.html#exercise-3---skin-cancer",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "Exercise 3 - Skin cancer",
    "text": "Exercise 3 - Skin cancer\nA dermatologist investigating a certain type of skin cancer induced the cancer in nine rats and then treated them with a new experimental drug. For each rat she recorded the number of hours until remission of the cancer. The rats had a mean remission time of 400 hours and a standard deviation of 30 hours. From this data, calculate the standard error of the mean.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-4---soil-carbon",
    "href": "labs/Lab04.html#exercise-4---soil-carbon",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "Exercise 4 - Soil carbon",
    "text": "Exercise 4 - Soil carbon\nAn initial soil carbon survey of a farm based on 12 observations found that the sample mean \\(\\overline{X}\\) was 1.2% and the standard deviation s was 0.4%. How many observations would be needed to estimate the mean carbon value with a standard error of 0.1%?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "href": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "Exercise 5 - What‚Äôs in the media - looming state election",
    "text": "Exercise 5 - What‚Äôs in the media - looming state election\nAn article was published in the Sydney Morning Herald on Saturday 20.3.2010 about statistics related to opinion polls. Read it and find the sentences related to (i) populations versus samples (ii) standard error formula (iv) the effect of sample size on standard errors.\nhttp://www.smh.com.au/national/demystifying-the-dark-art-of-polling-20100319-qmai.html",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-6---extra-practice",
    "href": "labs/Lab04.html#exercise-6---extra-practice",
    "title": "Lab 04 ‚Äì Sampling distributions",
    "section": "Exercise 6 - Extra practice",
    "text": "Exercise 6 - Extra practice\nThe average Australian woman has height (in cms) of 161.8 with a standard deviation of 6.\n\nThe Australian Institute of Sport ran a netball training camp for the best Australian young players. How tall were the goal position players? http://www.abc.net.au/news/2015-06-14/tall-athletes-get-support-at-ais-to-stand-as-proud-netballers/6544642\nWhat is the probability of finding an Australian woman of this height or taller?\n\nHints:\nStep 1: Using ggplot, draw a sketch of the Normal curve with the probability identified. You may need to draw a section of the right tail as the probability is small! We have provided the solution for the plotting to assist you.\nStep 2: Calculate the probability in R.\n\n1 - pnorm(189, 161.8, 6)\n\nggplot() +\n  stat_function(\n    fun = dnorm, args = list(mean = 161.8, sd = 6),\n    geom = \"area\", fill = \"white\", xlim = c(180, 161.8 + 4 * 6)\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 161.8, sd = 6),\n    geom = \"area\", fill = \"red\", xlim = c(161.8 + 4 * 6, 189)\n  ) +\n  xlab(\"x\") +\n  ylab(expression(N(161.8, 6^2) ~ pdf)) +\n  scale_x_continuous(breaks = 189)\n\n\nDharshani Sivalingam is the tallest netball player in the world. How tall is Dharshani? https://en.wikipedia.org/wiki/Tharjini_Sivalingam What is the probability of finding an Australian woman of Dharshani‚Äôs height?\nMadison Brown is one of the the shortest Australian International players. How tall is Madision? https://en.wikipedia.org/wiki/Madison_Browne What percentage of Australian women are between Madison and Dharshani‚Äôs heights?\nIf 80% of Australian women are above a certain height, what is that height?",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab02.html",
    "href": "labs/Lab02.html",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "",
    "text": "Learning Outcomes\n\n\n\nAt the end of this practical students should be able to:\n\nAcknowledge the importance of academic integrity\nImport, subset and inspect data in R\nCalculate simple summary statistics using both R and Excel\nGenerate dynamic reports in Quarto using inline R code\nWrite equations in both Quarto and MS Word\nunderstand how to debug R code",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#before-we-begin",
    "href": "labs/Lab02.html#before-we-begin",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Before we begin",
    "text": "Before we begin\nAt the beginnning of each lab, please create a new Quarto document in your project folder. This will be where you will practice R code, write notes and work on the exercises in this lab. We will go through the process together in the Exercise 1.\nThe following packages are required for this practical:\n\npacman ‚Äì a package for managing the loading of other packages (among other awesome features) -tidyverse ‚Äì a collection of R packages designed for data science\nreadxl ‚Äì a package for reading Excel files\nmodeest ‚Äì a package for estimating the mode of a distribution\n\nIf you have not already installed these packages, you can do so by adding the following code into a code chunk in your Quarto document:\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, readxl, modeest)\n\nFinally, please download the data file used in this lab here: soil.xlsx.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#academic-integrity",
    "href": "labs/Lab02.html#academic-integrity",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Academic integrity",
    "text": "Academic integrity\nThis exercise encourages students to discuss academic integrity, and in particular the grey areas often present. Your demonstrator will provide you with a number of scenarios to discuss with each other in smaller groups, and then with the class.\nIf you are interested in more information on Academic Integrity at the University of Sydney, see the following link: Academic Integrity. Also ensure you have completed the Academic Honesty Education Module (AHEM). This must be complete before your first assessment is due (next week for ENVX1002).",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-1-setting-up",
    "href": "labs/Lab02.html#exercise-1-setting-up",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Exercise 1: Setting up",
    "text": "Exercise 1: Setting up\nToday we will go through the process of getting into your RStudio project for labs and set up a new Quarto document. Follow these steps carefully:\n\nOpen RStudio Project\n\nLocate your RStudio project file (.Rproj) in your lab folder\nEither:\n\nOpen RStudio and use File &gt; Open Project to navigate to your .Rproj file\nDouble-click the .Rproj file directly from your file explorer\n\n\n\n\n\nLook at the top-right corner of your RStudio window. If you‚Äôve opened the project correctly, you‚Äôll see your project name displayed there. This confirms you‚Äôre working in the right project environment.\n\nCreate a New Quarto Document\n\nIn RStudio, click File &gt; New File &gt; Quarto Document‚Ä¶\nIn the dialog box that appears:\n\nChoose ‚ÄúHTML‚Äù as the output format\nGive your document a clear title (e.g., ‚ÄúLab 02‚Äù)\nClick ‚ÄúCreate‚Äù\n\n\nSave Your Document\n\nImmediately save your new document\nClick File &gt; Save or press Ctrl+S (Windows) / Cmd+S (Mac)\nName it something meaningful like lab02_practice.qmd\nEnsure you save it in your lab project folder\n\nRender the Document\n\nClick the ‚ÄúRender‚Äù button in the editor toolbar\nReview the output HTML file that appears\nLet us know if you encounter any issues\n\n\n\n\n\n\n\n\nPro tips\n\n\n\n\nLook for your project name in the top-right corner to confirm you‚Äôre in the right project\nUse Ctrl+S (Windows) / Cmd+S (Mac) to save your work frequently\nCheck for missing backticks or brackets if your document won‚Äôt render",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-2-exploring-water-quality",
    "href": "labs/Lab02.html#exercise-2-exploring-water-quality",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Exercise 2: exploring water quality",
    "text": "Exercise 2: exploring water quality\nSulphate (SO4) is an important indicator in water quality monitoring, typically measured to assess environmental impacts from industrial and agricultural activities. In this exercise we will calculate basic descriptive statistics for the SO4 concentration in water samples. The data is stored in an Excel file with a single sheet named ‚ÄúSO4‚Äù.\nFirst, let‚Äôs load the SO4 data from the Excel file:\n\nso4 &lt;- read_excel(\"data/soil.xlsx\", sheet = \"SO4\")\n\nWhen we load data into R for the first time, it is important to check what it looks like (and whether it loaded correctly). The str() function is a good way to quickly inspect the data:\n\nstr(so4)\n\nWhat does the output of str() tell us about the data? You may want to look at the documentation ?str or search online for more information, but ask your demonstrator if you‚Äôre still unsure about why we use this function.\nSince the data is a data frame object, we have a good idea of what functions we can use to explore it. Let‚Äôs examine the first few rows of our data:\n\nhead(so4)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-3-calculating-basic-statistics",
    "href": "labs/Lab02.html#exercise-3-calculating-basic-statistics",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Exercise 3: Calculating basic statistics",
    "text": "Exercise 3: Calculating basic statistics\nNow let‚Äôs calculate some basic descriptive statistics:\n\n# Calculate mean, median, and mode\nmean_so4 &lt;- mean(so4$SO4)\nmedian_so4 &lt;- median(so4$SO4)\nmode_so4 &lt;- mfv(so4$SO4)[1] # Most frequent value using modeest package\n\n# Calculate measures of spread\nrange_so4 &lt;- range(so4$SO4)\niqr_so4 &lt;- IQR(so4$SO4)\nvar_so4 &lt;- var(so4$SO4)\nsd_so4 &lt;- sd(so4$SO4)\n\nWhen reporting statistics in a scientific document, there are two approaches we could take:\n\n1. Basic R output (not recommended):\n\nmean_so4\n\n[1] 61.92308\n\nsd_so4\n\n[1] 5.241558\n\nmedian_so4\n\n[1] 62.1\n\n\n\n\n2. Inline reporting (recommended):\nConsider the following text:\n\nThe mean SO4 concentration in our samples is 61.92 mg/kg, with a standard deviation of 5.24 mg/kg. The median value is 62.1 mg/kg.\n\nUsing inline R code (approach 2) has several advantages:\n\nStatistics are seamlessly integrated into your text\nNumbers are automatically updated if your data changes\nResults are presented in a reader-friendly format\n\nTo create inline R code, use backticks with r, like this:\nThe mean SO~4~ concentration in our samples is 61.92 mg/kg\nNow try to recreate the paragraph above in your Quarto document.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-4-doing-it-in-excel",
    "href": "labs/Lab02.html#exercise-4-doing-it-in-excel",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Exercise 4: doing it in Excel",
    "text": "Exercise 4: doing it in Excel\nWhile R is powerful for data analysis, Microsoft Excel remains a widely used tool in just about any setting. Understanding how to perform basic statistical calculations in Excel is valuable for quick analyses and data exploration.\nGiven data, you can pick any cell to calculate statistics, but it‚Äôs common to use a new row or column to keep the data and results separate. For this exercise, we‚Äôll calculate the same statistical measures as we did in R, but using Excel‚Äôs built-in functions.\nExcel formulas always begin with an equals sign (=) followed by the function name and parentheses containing the data range.\n\nSelect the cell where you want the result to appear\nType = to begin the formula\nType the function name (e.g., AVERAGE)\nEnter the data range in parentheses\nPress Enter to calculate the result\n\n\n\n\nImported data in Excel.\n\n\nTo calculate these statistics in Excel:\n\nFor central tendency:\n\nMean: Use =AVERAGE()\nMedian: Use =MEDIAN()\nMode: Use =MODE()\n\n\n\n\n\nUsing the MEAN() formula in Excel.\n\n\n\nFor spread:\n\nRange: Use =MAX() and =MIN()\nIQR: Use =QUARTILE.INC() for Q3 and Q1\nVariance: Use =VAR.S()\nStandard Deviation: Use =STDEV.S()",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-5-soil",
    "href": "labs/Lab02.html#exercise-5-soil",
    "title": "Lab 02 ‚Äì Statistical programming",
    "section": "Exercise 5: soil",
    "text": "Exercise 5: soil\nIn this exercise, we‚Äôll explore different ways to subset data in R using the soil characteristics data. First, let‚Äôs load the data:\n\nsoil &lt;- read_excel(\"data/soil.xlsx\", sheet = \"soil\")\n\n\nUnderstanding data structure\nLet‚Äôs examine our data structure:\n\nstr(soil)\nhead(soil)\n\n\n\nBasic Subsetting in R\nThere are two main ways to subset data in R:\n\nUsing the $ operator to select columns\nUsing square brackets [] to select rows and columns\n\n\nUsing the $ Operator\nThe $ operator allows us to select a specific column from our data frame. For example:\n\n# Get the land use column\nsoil$land_use\n\n# Calculate the mean clay content at 60 cm depth\nmean(soil$clay60)\n\n\n\n\n\n\n\nNote\n\n\n\nThe $ operator is particularly useful when you want to:\n\nAccess a single column quickly\nUse column values in calculations\nCreate plots with specific variables\n\n\n\n\n\nUsing Square Brackets []\nSquare brackets allow more flexible subsetting using the format: dataframe[rows, columns]\n\n# First 5 rows, all columns\nsoil[1:5, ]\n\n# Clay columns\nsoil[, c(\"clay0\", \"clay60\")]\n\n# First 3 rows, electrical conductivity columns\nsoil[1:3, c(\"ec0\", \"ec60\")]\n\n\n\n\nPractice Questions\n\nUsing the $ operator:\n\nExtract all EC (Electrical Conductivity) values\nCalculate the mean EC\n\nUsing square brackets:\n\nSelect rows 10-15 of the dataset\nExtract pH values for samples 5-10\nCreate a subset containing only pH and OC (Organic Carbon) columns\n\nChallenge: Create a subset that contains:\n\nOnly samples where pH is greater than 7\nOnly the pH and EC columns\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember: - When using [], leaving the row or column section empty (with just a comma) means ‚Äúselect all‚Äù - You can use logical conditions inside [] to filter data",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab01.html",
    "href": "labs/Lab01.html",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "",
    "text": "Learning Outcomes\n\n\n\nAt the end of this practical students should be able to:\n\nInstall and set up R and RStudio on their computer\nUnderstand the relationship between R and RStudio\nCreate and manage R projects effectively\nWrite and render basic Quarto documents\nImport data from CSV and Excel files\nPerform basic operations in R",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-1-setting-up",
    "href": "labs/Lab01.html#exercise-1-setting-up",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "Exercise 1: setting up",
    "text": "Exercise 1: setting up\n\nInstalling R and RStudio\nBefore we begin working with data, we need to set up our statistical computing environment. We‚Äôll be using two main pieces of software:\n\nR: The statistical programming language (the engine)\nRStudio: The integrated development environment (IDE) that makes working with R easier (the interface)\n\n\n\nThink of R and RStudio like a car: R is the engine that does all the work, while RStudio is the dashboard and controls that make it easier to drive.\nThis week‚Äôs tutorial would have guided you through the installation process, but if you missed it, below are the steps to install R and RStudio on your personal computer.\n\n\n\n\n\n\nImportant\n\n\n\nYou must install R before installing RStudio, as RStudio needs R to function.\n\n\n\nInstalling R\n\nGo to the CRAN (Comprehensive R Archive Network) website\nClick on the link for your operating system\nFollow the installation instructions for your system\n\nFor Windows: Click ‚Äúbase‚Äù then download the latest version\nFor Mac: Choose the appropriate .pkg file for your system (Apple Silicon or Intel)\nFor Linux: Follow the instructions for your distribution\n\n\n\n\nInstalling RStudio\n\nVisit the Posit download page\nScroll down to ‚ÄúRStudio Desktop‚Äù\nClick the download button for your operating system\nRun the installer and follow the prompts\n\n\n\n\n\n\n\nTip\n\n\n\nIf you‚Äôre using a University computer, both R and RStudio should already be installed. However, it‚Äôs important to install them on your personal computer for working outside of class.\n\n\n\n\n\nCreating your first R project\nAn R project is like a container that keeps all your work organised and tidy. Think of it as a dedicated workspace for your course where everything stays together and works smoothly. Here‚Äôs why R projects are especially helpful for beginners:\n\n\nGood project organisation is crucial for reproducible research. It helps you and others find files easily and ensures your code works consistently.\n\nConsistent starting point: Every time you open your project, you‚Äôll be in the right place with all your files readily available\nNo more lost files: Your data, code, and outputs stay together in one organised location\nEasier file paths: You don‚Äôt need to worry about complex file locations - R projects make it simple to find and use your files\nCollaboration ready: When sharing your work, everything stays organised and works on other computers\nBetter workflow: As you learn more complex analyses, having an organised project structure will save you time and prevent headaches\n\nLet‚Äôs create a project for this course:\n\nOpen RStudio\nClick File ‚Üí New Project\nChoose ‚ÄúNew Directory‚Äù\nClick ‚ÄúNew Project‚Äù\nEnter ‚ÄúENVX1002‚Äù as the directory name\nChoose a location on your computer (preferably in a cloud-synced folder)\nClick ‚ÄúCreate Project‚Äù\n\nTo keep things simple but organised, let‚Äôs create one essential folder:\nENVX1002/\n‚îú‚îÄ‚îÄ ENVX1002.Rproj  # This file helps RStudio manage your project\n‚îî‚îÄ‚îÄ data/           # Store your datasets here\n\n\nAlways keep your raw data separate from your analysis files. This helps prevent accidental modifications to your original data.\nTo create the data folder, any of the following works:\n\nIn RStudio‚Äôs Files pane (bottom-right), click ‚ÄúNew Folder‚Äù, then name it ‚Äúdata‚Äù\nIn the console, run dir.create(\"data\") to create the folder\nManually create a folder named ‚Äúdata‚Äù in your project directory, using Finder(macOS), File Explorer(Windows), or similar\n\n\n\n\n\n\n\nTip\n\n\n\nAs you progress in the course, you can create more folders to organise your work. But for now, keeping it simple will help you focus on learning R without getting overwhelmed by complex folder structures.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-2-using-quarto",
    "href": "labs/Lab01.html#exercise-2-using-quarto",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "Exercise 2: using Quarto",
    "text": "Exercise 2: using Quarto\n\nWhat is Quarto?\nQuarto is a modern publishing system that allows you to:\n\n\nQuarto documents combine code, text, and output in one file, making your analysis reproducible and easy to share.\n\nCombine text, code, and output in one document\nCreate professional reports, presentations, and websites\nWork with multiple programming languages (including R)\nGenerate high-quality output in various formats (HTML, PDF, Word)\n\n\n\nCreating your first Quarto document\n\nIn RStudio, click File ‚Üí New File ‚Üí Quarto Document\nFill in the title and author\nClick ‚ÄúCreate‚Äù\nSave the document (File ‚Üí Save As‚Ä¶) with a .qmd extension\n\nTo render your document:\n\nClick the ‚ÄúRender‚Äù button (blue arrow) in the editor toolbar\nThe HTML output will automatically open in your default browser\n\n\nBasic markdown formatting\nQuarto uses markdown for text formatting:\n\n\nMarkdown is a simple way to format text that‚Äôs easy to read and write. The syntax is designed to be intuitive. Quarto‚Äôs documentation on markdown can be found here.\n\nBold text: **bold**\nItalic text: *italic*\nHeaders: # Level 1, ## Level 2, ### Level 3\nLists: Use - or 1. for bullet or numbered lists\nLinks: [text](URL)\nImages: ![alt text](image.png)\n\n\n\nCode chunks\nCode chunks in Quarto start with ```{r} and end with ```:\n\n\nCode chunks are where you write and execute R code. They keep your code separate from your text while showing both the code and its output.\n\n```{r}\n2 + 2\nmean(c(1, 2, 3, 4, 5))\n```\n\nYou can control chunk behavior with options:\n\necho: false - Hide the code but show results\neval: false - Show code but don‚Äôt run it\nwarning: false - Hide warning messages\nmessage: false - Hide messages\n\n\n10 / 5\n\n\n\n\nBasic R operations\nNow that we have our environment set up, let‚Äôs try some basic R operations:\n\n\nR uses standard mathematical operators. Remember that ^ means ‚Äúto the power of‚Äù and * means multiplication.\n\n# Basic arithmetic\n5 + 5\n10 - 3\n4 * 2\n8 / 2\n2^3 # Exponentiation\n\nThat‚Äôs it for now. We will look more into code chunks next week when we focus on statistical operations.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-3-r-packages",
    "href": "labs/Lab01.html#exercise-3-r-packages",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "Exercise 3: R packages",
    "text": "Exercise 3: R packages\n\nWhat are R packages?\nR packages are collections of functions, data, and documentation that extend R‚Äôs capabilities. Think of them as add-ons or extensions that provide additional functionality beyond what comes with base R. They are essential tools that make R incredibly versatile for different types of analysis.\n\n\nThink of R packages like apps on your phone - they add new features and capabilities to the base system.\n\n\n\n\n\n\nTip\n\n\n\nR comes with several built-in packages (called ‚Äúbase R‚Äù), but thousands more are available for specific tasks, from data manipulation to complex statistical analyses.\n\n\n\n\nInstalling packages\nThere are two main ways to install R packages:\n\n\nYou only need to install a package once, but you need to load it every time you start a new R session.\n\nUsing the install.packages() function:\n\n\n# Install a single package\ninstall.packages(\"readr\")\n\n# Install multiple packages at once\ninstall.packages(c(\"readr\", \"readxl\"))\n\n\nUsing the RStudio interface:\n\nTools ‚Üí Install Packages‚Ä¶\nType the package name\nClick ‚ÄúInstall‚Äù\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou only need to install a package once on your computer. However, you‚Äôll need to load it each time you start a new R session.\n\n\n\n\nLoading packages\nTo use a package in your R session, you need to load it using the library() function:\n\n\nThe pacman package is a helpful tool that combines installing and loading packages in one step.\n\n# Load individual packages\nlibrary(readr)\nlibrary(readxl)\n\nA more efficient way to handle package management is using the pacman package. You can read more about it here.\n\n# Install pacman if not already installed\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\n# Load multiple packages at once\npacman::p_load(readr, readxl)\n\n\n\n\n\n\n\nNote\n\n\n\nThe :: operator in R allows you to use a function from a package without loading the entire package. For example, readr::read_csv() uses the read_csv function from the readr package.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-4-importing-data",
    "href": "labs/Lab01.html#exercise-4-importing-data",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "Exercise 4: importing data",
    "text": "Exercise 4: importing data\nNow that we understand R packages, let‚Äôs use some specific ones for importing data. We‚Äôll use: - readr for importing CSV files - readxl for importing Excel files\n\n\nCSV files are simple text files that can be opened by many programs. Excel files are more complex but can store multiple sheets and formatting.\nFirst, make sure these packages are installed and loaded:\n\n# Install and load required packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readr, readxl)\n\n\nExample data files\nBefore we begin, download these example files that we‚Äôll use throughout this exercise:\n\nStudent Scores (CSV)\nWeather Data (Excel)\n\nSave these files in your project‚Äôs data folder before proceeding.\n\n\nUnderstanding file paths\nBefore we import data, it‚Äôs important to understand how R finds your files using file paths:\n\n\nUsing relative paths makes your code more portable - it will work on any computer that has the same project structure.\n\nAbsolute paths start from the root of your system (e.g., /Users/yourname/Documents/data.csv on Mac/Linux or C:/Users/yourname/Documents/data.csv on Windows)\nRelative paths start from your current working directory (e.g., data/data.csv)\n\nWhen working in an R project:\n\nThe project folder becomes your working directory\nUse relative paths to make your code more portable\nOrganise files in subdirectories (like ‚Äòdata/‚Äô, ‚Äòscripts/‚Äô, ‚Äòoutput/‚Äô)\n\nYou can check your working directory with:\n\ngetwd() # Shows your current working directory\n\n\n\nImporting CSV files\nCSV files are simple text files where data is separated by commas. They‚Äôre widely used because they‚Äôre simple and can be read by most software.\n\n\nAlways check your data after importing it. A quick look at the structure and first few rows can catch common issues early.\nTo import a CSV file, we use the read_csv() function from the readr package:\n\n# Using a relative path from the project root\nstudent_data &lt;- read_csv(\"data/student_scores.csv\")\n\n# View the first few rows\nhead(student_data)\n\n# Get a summary of the data\nsummary(student_data)\n\n\n\n\n\n\n\nTip\n\n\n\nAlways try to use relative paths within your R project. This makes your code: - More portable (works on different computers) - Easier to share with others - Less likely to break when files move\n\n\n\n\nImporting Excel files\nFor Excel files, we use the read_excel() function from the readxl package:\n\n# Import an Excel file\nweather_data &lt;- read_excel(\"data/weather_data.xlsx\")\n\n# View the first few rows\nhead(weather_data)\n\n# Look at the structure of the data\nstr(weather_data)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen importing data:\n\nAlways check the first few rows using head()\nLook at the data structure using str()\nCheck for any missing values using summary()\n\n\n\n\n\nCommon importing issues\nHere are some common issues you might encounter when importing data:\n\n\nData import problems are common but can usually be fixed by specifying the correct options in your import function.\n\nFile path errors: Ensure you‚Äôre using the correct path relative to your project directory\nMissing values: R might interpret empty cells differently than expected\nColumn types: Sometimes R might guess the wrong data type for columns\nSpecial characters: Non-ASCII characters might not import correctly\n\nYou can handle these issues using additional arguments in the import functions:\n\n# Example with more options\nstudent_data &lt;- read_csv(\"data/student_scores.csv\",\n  na = c(\"\", \"NA\", \"missing\"), # Define missing value codes\n  col_types = cols( # Specify column types\n    student_id = col_character(),\n    quiz_score = col_double(),\n    homework_score = col_double(),\n    final_score = col_double()\n  )\n)",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#summing-up",
    "href": "labs/Lab01.html#summing-up",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "Summing up",
    "text": "Summing up\nIn this lab, you‚Äôve learned:\n\nBasic setup: Installing R/RStudio and creating projects\nCreating and formatting Quarto documents\nUsing R packages and basic operations\nImporting data from CSV and Excel files",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#take-home-exercises",
    "href": "labs/Lab01.html#take-home-exercises",
    "title": "Lab 01 ‚Äì Getting started",
    "section": "Take-home exercises",
    "text": "Take-home exercises\n\nExercise 5: Creating a lab report template\nCreate a Quarto document that will serve as your template for future lab reports. Your template should include:\n\nA YAML header with:\n\nYour name and student ID\nThe unit of study code (ENVX1002)\nThe current date\nOutput format set to HTML\n\nThe following sections (using appropriate header levels):\n\nIntroduction\nMethods\nResults\nDiscussion\nReferences\n\nInclude at least one example of each of these formatting elements:\n\nBold text\nItalic text\nA bullet point list\nA numbered list\nA link to a relevant website\nAn empty R code chunk\n\n\nSave this template as lab_report_template.qmd in your ENVX1002 project folder.\n\n\nExercise 6: Data exploration practice\nCreate a new Quarto document called data_exploration.qmd and complete the following tasks:\n\nLoad the required packages (readr and readxl)\nCreate a simple data frame with two columns. You may need to use the data.frame() function to create this data frame, or you could do this manually in Excel and then import it using one of the read_*() functions. The data frame should have the following structure:\n\nsite_id: A to E\ntemperature: 15.2, 14.8, 15.6, 14.9, 15.3\n\nUse these functions to explore your data frame:\n\nhead() to view the first few rows\nstr() to examine the structure and data types\ndim() to check the dimensions (rows and columns)\nnames() to see the column names\n\n\nRemember to add text explanations between your code chunks describing what each function does and what information it provides about your data.\n\n\n\n\n\n\nTip\n\n\n\nThese exercises will help reinforce the skills you‚Äôve learned today and create useful resources for future labs. Make sure to render your documents to check that everything works correctly.",
    "crumbs": [
      "**üñ•Ô∏è Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "module01/02-probability_distributions.html",
    "href": "module01/02-probability_distributions.html",
    "title": "Probability distributions",
    "section": "",
    "text": "Biological systems are variable, and we consequently need statistical methods to cope with this variability in drawing valid inferences from our data. As a consequence of this variability, we can never say anything with absolute certainty based on biological data: there will always be some ‚Äúdoubt‚Äù due to chance variation. Not surprisingly, we need the tools and language of probability to assist us with quantifying our level of ‚Äúdoubt‚Äù when we make our conclusions.\nWe won‚Äôt deal with probability theory to any great degree in this unit of study, but just outline some of the key concepts HSC Maths concepts that we will build on.\n\n\nSimple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\n\n\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\nComplementary Events\nProbability of an event not occurring = 1 ‚Äì probability of event occurring.\n\\(P(E) =1‚àí P(E)\\)\nExercises\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) ‚Äì P(A \\text{ and } B)\\)\nExercise\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\nProduct Rule\nWhen we do more than one thing (e.g.¬†toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.\n\n\n\n\nProbability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.\n\n\n\nJust as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g.¬†normal distribution, or discrete e.g.¬†binomial distribution. In this unit of study we will only consider continuous distributions.\n\n\n\nFor discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g.¬†the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don‚Äôt mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ‚â§ Y ‚â§ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student‚Äôs T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these‚Ä¶ If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = \"Standard Normal\")\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = \"Log-Normal\")\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) +\n  geom_line() +\n  facet_wrap(~Distribution, scales = \"free_x\") +\n  theme_minimal() +\n  labs(\n    title = \"Log-Normal PDF vs. Normal PDF\",\n    x = \"Value\",\n    y = \"Density\"\n  )\n\n\n\n\nWe began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the ‚Äúbell-shaped‚Äù distribution commonly observed in histograms of biological and environmental data e.g.¬†height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at Œº and œÉ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ‚Äôthe variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e.¬†\\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g.¬†\\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e.¬†probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e.¬†50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1 - pnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(‚Äì1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; ‚Äì1) \\approx  0.1587\\) (from R)\n\\(P(‚Äì1 &lt; Z &lt; 2) \\approx  0.9772 ‚Äì 0.1587 \\approx  0.8185\\)\n\npnorm(2)\npnorm(-1)\npnorm(2) - pnorm(-1)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e.¬†\\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n\n\n\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we‚Äôve obtained these probabilities we‚Äôll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‚Äòz‚Äô; ‚ÄòPDF via formula‚Äô; ‚ÄòPDF via NORMDIST‚Äô.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we‚Äôre going to obtain the corresponding probabilities in 2 different ways ‚Äì you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don‚Äôt need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution‚Ä¶\n\n\n\nNormality tests are the first introduction you‚Äôll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe‚Äôll expand these steps soon‚Ä¶\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e.¬†whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"data/TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "**üìï Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#probability",
    "href": "module01/02-probability_distributions.html#probability",
    "title": "Probability distributions",
    "section": "",
    "text": "Simple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\n\n\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\nComplementary Events\nProbability of an event not occurring = 1 ‚Äì probability of event occurring.\n\\(P(E) =1‚àí P(E)\\)\nExercises\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) ‚Äì P(A \\text{ and } B)\\)\nExercise\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\nProduct Rule\nWhen we do more than one thing (e.g.¬†toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.",
    "crumbs": [
      "**üìï Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#probability-density-functions",
    "href": "module01/02-probability_distributions.html#probability-density-functions",
    "title": "Probability distributions",
    "section": "",
    "text": "Probability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.",
    "crumbs": [
      "**üìï Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#types-of-distributions",
    "href": "module01/02-probability_distributions.html#types-of-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "Just as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g.¬†normal distribution, or discrete e.g.¬†binomial distribution. In this unit of study we will only consider continuous distributions.",
    "crumbs": [
      "**üìï Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#probability-for-continuous-distributions",
    "href": "module01/02-probability_distributions.html#probability-for-continuous-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "For discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g.¬†the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don‚Äôt mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ‚â§ Y ‚â§ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student‚Äôs T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these‚Ä¶ If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = \"Standard Normal\")\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = \"Log-Normal\")\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) +\n  geom_line() +\n  facet_wrap(~Distribution, scales = \"free_x\") +\n  theme_minimal() +\n  labs(\n    title = \"Log-Normal PDF vs. Normal PDF\",\n    x = \"Value\",\n    y = \"Density\"\n  )",
    "crumbs": [
      "**üìï Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#the-normal-distribution",
    "href": "module01/02-probability_distributions.html#the-normal-distribution",
    "title": "Probability distributions",
    "section": "",
    "text": "We began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the ‚Äúbell-shaped‚Äù distribution commonly observed in histograms of biological and environmental data e.g.¬†height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at Œº and œÉ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ‚Äôthe variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e.¬†\\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g.¬†\\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e.¬†probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e.¬†50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1 - pnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(‚Äì1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; ‚Äì1) \\approx  0.1587\\) (from R)\n\\(P(‚Äì1 &lt; Z &lt; 2) \\approx  0.9772 ‚Äì 0.1587 \\approx  0.8185\\)\n\npnorm(2)\npnorm(-1)\npnorm(2) - pnorm(-1)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e.¬†\\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n\n\n\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we‚Äôve obtained these probabilities we‚Äôll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‚Äòz‚Äô; ‚ÄòPDF via formula‚Äô; ‚ÄòPDF via NORMDIST‚Äô.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we‚Äôre going to obtain the corresponding probabilities in 2 different ways ‚Äì you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don‚Äôt need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution‚Ä¶\n\n\n\nNormality tests are the first introduction you‚Äôll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe‚Äôll expand these steps soon‚Ä¶\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e.¬†whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"data/TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "**üìï Module 1**",
      "Probability distributions"
    ]
  }
]