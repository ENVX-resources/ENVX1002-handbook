[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "module03/042-linear_functions_multi.html",
    "href": "module03/042-linear_functions_multi.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The formula or function for multiple linear regression is:\n\\[ y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\epsilon_i \\]\nTherefore, estimating the model involves estimating the values of \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), …, \\(\\beta_k\\).\n\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) to \\(\\beta_k\\) are the partial regression coefficients\n\\(\\epsilon\\) is the error term (aka residuals)\n\nFor simple linear regression (response and one predictor), we can visualise this with a scatterplot (response on the y-axis, predictor on the x-axis) and a line of best fit. Two variables, two dimensions, a 2D plot. When we have more than one predictor - this becomes more difficult to visualise.\nLikewise, interpretation becomes more complex. If \\(x_1\\) increases by one unit, \\(y_i\\) increases by \\(\\beta_1\\) units, if all other variables are held constant. This is why the coefficients in multiple linear regression are referred to as partial regression coefficients.\n\n\nThe principle of parsimony is the idea that the simplest explanation is the best explanation. In the context of multiple linear regression, this means we need to consider whether adding more variables is actually useful. This is because the more variables we include, the more complex the model becomes, and the more likely it is to overfit the data. If we have too few variables, we could underfit the data (low % variance explained).\n\nIf a model with 3 variables has the same predictive power as a model with 10 variables, it is better to use the model with 3 variables. A model with 3 variables is easier to fit and interpret, and it means the other 7 variables were not adding much to the model and could be culled.\nThe process of selecting the correct variables is called variable selection. There are methods to do this (e.g. stepwise regression with step()) but they are beyond the scope of this unit. A simple method is to remove predictors that do not appear significant in the model (ideally one by one) until all remaining variables are significant. This should be done in conjunction with checking the adjust R2 to ensure the model does not decline drastically in predictive capability.",
    "crumbs": [
      "**📘 Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#principle-of-parsimony",
    "href": "module03/042-linear_functions_multi.html#principle-of-parsimony",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "The principle of parsimony is the idea that the simplest explanation is the best explanation. In the context of multiple linear regression, this means we need to consider whether adding more variables is actually useful. This is because the more variables we include, the more complex the model becomes, and the more likely it is to overfit the data. If we have too few variables, we could underfit the data (low % variance explained).\n\nIf a model with 3 variables has the same predictive power as a model with 10 variables, it is better to use the model with 3 variables. A model with 3 variables is easier to fit and interpret, and it means the other 7 variables were not adding much to the model and could be culled.\nThe process of selecting the correct variables is called variable selection. There are methods to do this (e.g. stepwise regression with step()) but they are beyond the scope of this unit. A simple method is to remove predictors that do not appear significant in the model (ideally one by one) until all remaining variables are significant. This should be done in conjunction with checking the adjust R2 to ensure the model does not decline drastically in predictive capability.",
    "crumbs": [
      "**📘 Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#exploratory-data-analysis",
    "href": "module03/042-linear_functions_multi.html#exploratory-data-analysis",
    "title": "Multiple Linear Regression",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nFirst we will need to remove the missing values in the dataset.\n\nsummary(airquality)\ndata &lt;- na.omit(airquality) # remove NA and rename as new object (to not change original dataset)\n\nNext we will look at the relationships between Ozone and the other variables in the dataset. Below are some potential functions that can visualise the data and correlations – choose the one that works best visually for your data.\n\n# ### Base R\n# pairs(data)\n# cor(data) |&gt; round(2)\n# \n# ### corrplot\n# library(corrplot)\n# corrplot::corrplot(cor(data), method = \"circle\")\n\n### psych\npsych::pairs.panels(data)\n\nThe key thing to check is the plots and correlations between Ozone and the predictors (first column, first row). The best correlated variables are Temp (\\(r = 0.7\\)), Wind (\\(r = -0.61\\)), and Solar.R (\\(r = 0.35\\)). Month is negligibly correlated (\\(r = 0.14\\)) and Day has no relationship (\\(r = -0.01\\)).\nWe can see that Ozone does not have a linear relationship with Temp, Solar.R or Wind, so we do a pre-emptive natural log transformation.\n\ndata &lt;- data %&gt;%\n  mutate(Ozone_log = log(Ozone)) %&gt;%  # create the log-transformed column\n  select(-Ozone) %&gt;%                  # remove the original Ozone column\n  select(Ozone_log, everything())     # reorder so that Ozone_log is first\n\npsych::pairs.panels(data)\n\nThe linearity between Ozone_log and both Temp and Solar.R are higher, and for Wind it is slightly lower. The scatterplots however are looking more linear.",
    "crumbs": [
      "**📘 Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#assumptions",
    "href": "module03/042-linear_functions_multi.html#assumptions",
    "title": "Multiple Linear Regression",
    "section": "Assumptions",
    "text": "Assumptions\nThe assumptions for multiple linear regression are the same as simple linear regression - except one additional condition, called collinearity. This is when two or more predictors are highly correlated with each other. For the assumption to be broken, it requires a perfect relationship (\\(r\\) = -1 or 1), but strong and very correlations will still have an effect. This can cause problems with the model, as the model cannot distinguish between the two predictors and the resulting relationships are unreliable. Judging from the correlations above, this is not an issue for this dataset.\nWe fit the model with all variables and create our assumption plots.\n\nmod &lt;- lm(Ozone_log ~ Temp + Wind + Solar.R + Month + Day, data = data)\npar(mfrow = c(2,2))\nplot(mod)\n\nThe residuals vs fitted plot is fairly evenly distributed around the line, and the red line is mostly flat. The normal QQ sticks to the line well except one outlier - we cross-check with the residuals vs leverage plot, but the point does not exceed the dotted Cook’s distance line so it is not extreme enough to need action. Scale-location is slightly tilted but the line is fairly straight, and the points are evenly distributed so variances are equal.\nAll assumptions are met.",
    "crumbs": [
      "**📘 Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#model-evaluation",
    "href": "module03/042-linear_functions_multi.html#model-evaluation",
    "title": "Multiple Linear Regression",
    "section": "Model evaluation",
    "text": "Model evaluation\nLike simple linear regression, there is a hypothesis for multiple linear regression. The null hypothesis is that all partial regression coefficients are equal to zero, and the mean is a better predictor or line of best fit. The alternate hypothesis is that at least one of the partial regression coefficients is not equal to zero, and there is merit in the multiple linear regression model.\n\\[ H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0 \\] \\[ H_1: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0 \\]\n\nsummary(mod)\n\nWe can see that the p-value for the F-statistic is very low, so we fail to reject the alternate hypothesis, and the model is better than using the mean of the data. The Adjusted R-squared is 65.36%, which means that 65.36% of the variance in Ozone_log can be explained by the predictors. The Residual standard error is 0.5096.\nTemp, Wind and Solar.R are all significant predictors of Ozone_log, but Month and Day are not. So we try removing them from the model.\n\nmod &lt;- lm(Ozone_log ~ Temp + Wind + Solar.R, data = data)\nsummary(mod)\n\nTemp, Wind and Solar.R are all still significant predictors of Ozone_log. The Adjusted R-squared is 65.5% (which is higher) and the Residual standard error is 0.5086 (which is lower). This means that the model is slightly better without Month and Day.\nFollowing the principle of parsimony, the model with just Temp, Wind and Solar.R is the more parsimonious model, and the model we will use to predict Ozone_log.",
    "crumbs": [
      "**📘 Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/042-linear_functions_multi.html#interpretation",
    "href": "module03/042-linear_functions_multi.html#interpretation",
    "title": "Multiple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\nThe equation for our model is \\(log(\\text{Ozone}) = -0.026 + 0.049 \\cdot \\text{Temp} - 0.062 \\cdot \\text{Wind} + 0.003 \\cdot \\text{Solar.R}\\). For a unit increase in Temp, log(Ozone) increases by 0.049 units, and Ozone increases by \\(e^0.049 = 1.05\\) times or approximately 4.9%, given all other predictors are held constant. The most useful interpretation here is the percentage increase. Given all partial regression coefficient are \\(&lt; |0.25|\\), we can use a quick approximation (\\(\\times 100 \\%\\)).\n\nFor every one unit increase in Temp, Ozone increases by 4.9%, given all other predictors are held constant\nFor every one unit increase in Wind, Ozone decreases by 6.2%, given all other predictors are held constant\nFor every one unit increase in Solar.R, Ozone increases by 0.3%, given all other predictors are held constant\n\nThe variation in Ozone concentration in 1973 New York could be mostly (adjusted R2 = 65.5%) explained by the weather, i.e. temperature, wind speed, and solar radiation.",
    "crumbs": [
      "**📘 Module 3**",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html",
    "href": "module03/040-describing_relationships.html",
    "title": "Describing relationships",
    "section": "",
    "text": "In statistics, a relationship between two or more numerical variables means when one variable changes, the other variable/s will also change. We can describe these relationships in terms of direction, strength, and form.\nTo determine the relationship between two variables qualitatively with by visualising our data with scatter plots.\n\n\nDirection\nA relationship can be positive, negative, or non-existent.\n\nPositive relationship: when one variable increases, the other variable also increases, e.g. the amount of rainfall received and the resulting crop yield.\nNegative relationship: when one variable increases, the other variable decreases, e.g. the levels of insulin and glucose in blood.\nNo relationship: when one variable changes, the other variable does not change, e.g. shoe size and IQ.\n\nStrength\nThe strength of a relationship refers to how closely the two variables are related - weak, moderate, strong, very strong and perfect. A weak relationship will have more scatter than a strong relationship.\nForm\nThe form of a relationship refers to the shape of the relationship. The simplest form is a straight line or linear relationship. Some examples of non-linear forms include polynomials (e.g. quadratic, cubic), exponential, and logistic.\n\n\n\n\n\nTo measure the relationship quantitatively, we use correlation coefficients. These are numbers between -1 and 1, where -1 indicates a perfect negative relationship, 0 indicates no relationship, and 1 indicates a perfect positive relationship.\nThe most common correlation coefficient is the Pearson correlation coefficient (\\(r\\)). It measures linear relationships between two numerical variables.\nPearson’s Correlation (r) Formula:\n\\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \\]\nIn essence, it is the covariance divided by the product of the standard deviations.\nIn R, we use the function cor() to calculate the correlation coefficient. By default, it calculates the Pearson correlation coefficient.\n\ncor(x, y) \n\nDescribing the correlation coefficient in terms of strength can be a little subjective. Below are some approximate ranges for the common terms.\n\n|0 - 0.1| : no relationship\n|0.1 - 0.3| : weak\n|0.4 - 0.6| : moderate\n|0.7 - 1| : strong\n|0.9 - 1| : very strong\n\n\n\n\nIn the case of non-linear relationships, it is best not to use the Pearson’s correlation coefficient. Instead, we can use the Spearman’s rank correlation coefficient (\\(r_{s}\\)) or Kendall’s tau (\\(\\tau\\)). Note, these relationships must still be monotonic.\n\nMonotonic: a relationship that is consistently increasing or decreasing\nLinear: a relationship that is increasing or decreasing at a constant rate\n\nTo use either Spearman’s rank correlation coefficient or Kendall’s tau, we can use the cor() function with the method argument set to either \"spearman\" or \"kendall\".\n\ncor(x, y, method = \"spearman\")\ncor(x, y, method = \"kendall\")\n\nSpearman’s rank correlation coefficient essentially ranks the data (e.g. the smallest value will be ranked 1, the second smallest 2, etc.) for both the x and y axis, and then calculates the Pearson correlation coefficient for the ranks. Thus it works for any monotonic relationship.\nKendall’s tau looks at all possible x and y pairs, and determines if they are concordant. e.g. one pair of points \\((x_{i}, y_{i})\\) is concordant with another pair \\((x_{j}, y_{j})\\) if \\(x_{i} &gt; x_{j}\\) and \\(y_{i} &gt; y_{j}\\) or \\(x_{i} &lt; x_{j}\\) and \\(y_{i} &lt; y_{j}\\). Kendall’s tau is then calculated with:\n\\[ \\tau = \\frac{\\text{(number of concordant pairs)} - \\text{(number of discordant pairs)}}{\\text{(total number of pairs)}}\\]\n\n\n\n\nCorrelation does not imply causation. Just because two variables are correlated does not mean that one causes the other to change. Some example of spurious (i.e. ridiculous) correlations can be found at Spurious Correlations.\nBefore conducting an experiment to collect data (or analysing existing data), it is important to have a hypothesis about the relationship between the variables. Is there reason to believe the two variables should have a relationship? If not, any ‘relationship’ found via scatter plots or correlations are unlikely meaningful.",
    "crumbs": [
      "**📘 Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html#scatter-plots",
    "href": "module03/040-describing_relationships.html#scatter-plots",
    "title": "Describing relationships",
    "section": "",
    "text": "Direction\nA relationship can be positive, negative, or non-existent.\n\nPositive relationship: when one variable increases, the other variable also increases, e.g. the amount of rainfall received and the resulting crop yield.\nNegative relationship: when one variable increases, the other variable decreases, e.g. the levels of insulin and glucose in blood.\nNo relationship: when one variable changes, the other variable does not change, e.g. shoe size and IQ.\n\nStrength\nThe strength of a relationship refers to how closely the two variables are related - weak, moderate, strong, very strong and perfect. A weak relationship will have more scatter than a strong relationship.\nForm\nThe form of a relationship refers to the shape of the relationship. The simplest form is a straight line or linear relationship. Some examples of non-linear forms include polynomials (e.g. quadratic, cubic), exponential, and logistic.",
    "crumbs": [
      "**📘 Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html#correlation",
    "href": "module03/040-describing_relationships.html#correlation",
    "title": "Describing relationships",
    "section": "",
    "text": "To measure the relationship quantitatively, we use correlation coefficients. These are numbers between -1 and 1, where -1 indicates a perfect negative relationship, 0 indicates no relationship, and 1 indicates a perfect positive relationship.\nThe most common correlation coefficient is the Pearson correlation coefficient (\\(r\\)). It measures linear relationships between two numerical variables.\nPearson’s Correlation (r) Formula:\n\\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} \\]\nIn essence, it is the covariance divided by the product of the standard deviations.\nIn R, we use the function cor() to calculate the correlation coefficient. By default, it calculates the Pearson correlation coefficient.\n\ncor(x, y) \n\nDescribing the correlation coefficient in terms of strength can be a little subjective. Below are some approximate ranges for the common terms.\n\n|0 - 0.1| : no relationship\n|0.1 - 0.3| : weak\n|0.4 - 0.6| : moderate\n|0.7 - 1| : strong\n|0.9 - 1| : very strong\n\n\n\n\nIn the case of non-linear relationships, it is best not to use the Pearson’s correlation coefficient. Instead, we can use the Spearman’s rank correlation coefficient (\\(r_{s}\\)) or Kendall’s tau (\\(\\tau\\)). Note, these relationships must still be monotonic.\n\nMonotonic: a relationship that is consistently increasing or decreasing\nLinear: a relationship that is increasing or decreasing at a constant rate\n\nTo use either Spearman’s rank correlation coefficient or Kendall’s tau, we can use the cor() function with the method argument set to either \"spearman\" or \"kendall\".\n\ncor(x, y, method = \"spearman\")\ncor(x, y, method = \"kendall\")\n\nSpearman’s rank correlation coefficient essentially ranks the data (e.g. the smallest value will be ranked 1, the second smallest 2, etc.) for both the x and y axis, and then calculates the Pearson correlation coefficient for the ranks. Thus it works for any monotonic relationship.\nKendall’s tau looks at all possible x and y pairs, and determines if they are concordant. e.g. one pair of points \\((x_{i}, y_{i})\\) is concordant with another pair \\((x_{j}, y_{j})\\) if \\(x_{i} &gt; x_{j}\\) and \\(y_{i} &gt; y_{j}\\) or \\(x_{i} &lt; x_{j}\\) and \\(y_{i} &lt; y_{j}\\). Kendall’s tau is then calculated with:\n\\[ \\tau = \\frac{\\text{(number of concordant pairs)} - \\text{(number of discordant pairs)}}{\\text{(total number of pairs)}}\\]",
    "crumbs": [
      "**📘 Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html#causation",
    "href": "module03/040-describing_relationships.html#causation",
    "title": "Describing relationships",
    "section": "",
    "text": "Correlation does not imply causation. Just because two variables are correlated does not mean that one causes the other to change. Some example of spurious (i.e. ridiculous) correlations can be found at Spurious Correlations.\nBefore conducting an experiment to collect data (or analysing existing data), it is important to have a hypothesis about the relationship between the variables. Is there reason to believe the two variables should have a relationship? If not, any ‘relationship’ found via scatter plots or correlations are unlikely meaningful.",
    "crumbs": [
      "**📘 Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html",
    "href": "module02/02-ttest2.html",
    "title": "Two-sample \\(t\\)-test",
    "section": "",
    "text": "In the one-sample tests just considered, we compared a population mean, \\(\\mu\\) (where our experimental sample represents the population) with a fixed numeric value of interest. In practice, this is fairly rare. More frequently we have samples drawn from two (or more) populations and we compare the two means to see if the treatments produce similar results.\nThe null hypothesis is that the two samples come from a population with the same true mean, \\(\\mu\\). This is commonly expressed as\n\\[H_0: \\mu_1 = \\mu_2\\ \\text{or}\\ H_0:\\mu_1-\\mu_2 =0\\]\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are the means of the two populations. The alternative hypothesis is that the two means are different, i.e.\n\\[H_1: \\mu_1 \\neq \\mu_2\\]",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#assumptions",
    "href": "module02/02-ttest2.html#assumptions",
    "title": "Two-sample \\(t\\)-test",
    "section": "Assumptions",
    "text": "Assumptions\nThe two-sample \\(t\\)-test assumes that the data are :\n\nContinuous,\nat least approximately normally distributed, and\nthe variances of the two sets are homogeneous (i.e. the same).\n\nIdeally these assumptions should be tested before you carry about the two-sample \\(t\\)-test.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#what-if-assumptions-are-not-met",
    "href": "module02/02-ttest2.html#what-if-assumptions-are-not-met",
    "title": "Two-sample \\(t\\)-test",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\nIf the data do not meet either of these assumptions, then you may choose an appropriate transformation or look for another technique to use e.g. a non-parametric method (see non-parametric sections of this book).\nIf the variances cannot be assumed to be equal (but the data are normally distributed), there is a way of adjusting the two-sample \\(t\\)-test to compensate for this – it’s called the Satterthwaite’s approximation – more on this later!",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#three-variants-of-the-two-sample-t-test",
    "href": "module02/02-ttest2.html#three-variants-of-the-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Three Variants of the Two-Sample \\(t\\)-test",
    "text": "Three Variants of the Two-Sample \\(t\\)-test\nThere are 3 different ways in which a two-sample \\(t\\)-test can be used. They ALL assume that the data is approximately normally distributed.\n\nIndependent samples, equal variance - run a two-sample \\(t\\)-test assuming equal variances.\nIndependent samples, unequal variance - run a two-sample \\(t\\)-test assuming unequal variances.\nPaired samples - run a paired \\(t\\)-test.\n\nIn choosing which variant of the \\(t\\)-test is applicable in your situation, you first need to decide whether your two samples are paired or unpaired (independent).",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#paired-data",
    "href": "module02/02-ttest2.html#paired-data",
    "title": "Two-sample \\(t\\)-test",
    "section": "Paired Data",
    "text": "Paired Data\nPaired samples arise when we measure pairs of similar experimental units. In this pair of experimental units, one unit receives Treatment 1 and the other receives Treatment 2.\nIn some cases, treatments are applied to the same experimental unit e.g. one half of a piece of fruit receives Trt 1 and the other half Trt 2; two plants of different varieties are grown in the same pot (here variety is the treatment).\nIn recognising this pairing in our analysis we are taking into account the fact that biological variation between pairs is likely to be larger than within pairs. This way, we get a clearer picture of the difference that is due to the treatment factor.\n\nExamples of paired data\n\nObservations at two times on the same experimental unit\n\nBefore and after readings of particle matter in the air on 3 sites near a new power station (before the station was built, and after it became operational). (Dytham 2003, 80)\nMeasurements of water flow on two consecutive days at 6 sites along a river. (Dytham 2003, 83)\n\nObservations on 2 halves/parts of the same experimental unit\n\nOne half of each (uncut) grapefruit was exposed to sunlight, and the other half was shaded (McConway et al 1999, p. 198).\nA standard (recommended) variety of wheat is compared with a new variety via 2 similar plots on each of 8 farms (Clewer and Scarisbrick 2001, p. 46).",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#two-sample-t-test-for-independent-samples",
    "href": "module02/02-ttest2.html#two-sample-t-test-for-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Two-Sample t-Test for Independent Samples",
    "text": "Two-Sample t-Test for Independent Samples\nProcedure for the test:\n\nSet up the null and alternate hypotheses\nDecide on the level of significance, 5%, 1%, 0.01% etc.\nCheck the assumptions of normality and equal variance. We use an F-test to formally test for equality of variance.\nCalculate the test statistic \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{SED}\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means, and \\(SED\\) is the standard error of the difference between the means.\nCalculate the degrees of freedom (df).\nFind the P-value in printed statistical tables or via GenStat or Excel.\nMake a statistical conclusion by comparing this P-value to your chosen level of significance (if P &lt;α, then reject null hypothesis).\nCalculate the confidence interval.\nInterpret your results biologically.\n\n\n\n\n\n\n\nCalculating the test statistic\n\n\n\nWhen calculating the test statistic, use \\(y_1\\) as the larger mean. This will give a positive value for \\(t\\).",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "href": "module02/02-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)",
    "text": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)\nA two-sample t-test shows whether there is evidence of a difference in population means. The magnitude of this difference can be estimated with a confidence interval.\nA 95% confidence interval for the true difference \\(\\mu_1 - \\mu_2\\) is given by \\(\\bar{y}_1 - \\bar{y}_2 \\pm t^{\\alpha/2}_{df} \\times SED\\)\nwhere \\(\\bar{y}_1 - \\bar{y}_2\\) is the difference between the sample means, \\(t^{\\alpha/2}_{df}\\) is the critical value from the t-distribution for the chosen level of significance and degrees of freedom, and \\(SED\\) is the standard error of the difference between the means.\nThe df and SED need to take into account whether or not you are assuming equal variances.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "href": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with EQUAL Variances",
    "text": "SED and df for Independent Samples with EQUAL Variances\n\\[\nSED = \\sqrt{s^2_p \\frac{1}{n1}+ \\frac {1}{n2}}\n\\] and \\[ s^2_p = \\frac{(n_1- 1)s^2_1 + (n_2 -1)s^2_2}{n_1 + n_2 -2}\\] \\(s^2_p\\) is the pooled estimate of variance and \\(df = n_1 +n_2 -2\\). We used a a pooled estimate in this estimate in this instance since we are assuming \\(\\sigma^2_1 = \\sigma^2_2\\). \\(df = n_1 + n_2 -2\\)",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "href": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with UNequal Variances",
    "text": "SED and df for Independent Samples with UNequal Variances\nWhat do you do if when you’re checking your assumptions for a two- sample t-test you find that the variances are not equal but the data is normally distributed? You can go ahead with a modified t-test or you can choose a different test.\nThis modified t-test used in the case of unequal variances is often called Satterthwaite’s approximate t-test.\n\nSatterthwaite’s Approximate t-Test\nThe null and alternate hypotheses remain the same i.e. \\[H_0: \\mu1 = \\mu2\\] or \\[H_0: \\mu1 - \\mu2 = 0\\]\nThe formula for the test statistic, \\(t\\), in Satterthwaite’s approximate test is a little different to that for the t-test with equal variances. The s.e.d. changes because we can no longer use a pooled estimate of the variance (since the variances cannot be assumed equal). \\[\nt = \\frac {\\bar{y_1 }- \\bar{y}_2}{SED} \\text {, where} \\\nSED = \\sqrt{\\frac{s^2_1}{n^1}+ \\frac{s^2_2}{n_2}}\n\\]\nA correction for unequal variance is made to the degrees of freedom. \\[\ndf = \\frac{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})^2}\n{\\frac{(\\frac{s^2_1}{n_1})^2}{n_1 - 1} + \\frac{(\\frac{s^2_2}{n_2})^2}{n_2 - 1}}\n\\]\n\nThe result of this equation for df is rounded to the nearest integer\n\nThen proceed as usual through the rest of the test - i.e. find the P-value; draw a statistical conclusion about your hypothesis; interpret your results biologically.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#f-test-for-equality-of-variances",
    "href": "module02/02-ttest2.html#f-test-for-equality-of-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "F-Test for Equality of Variances",
    "text": "F-Test for Equality of Variances\nOne of the conditions for the independent sample t-test to be valid is that the population variances \\(\\sigma_{12}\\) and \\(\\sigma_{22}\\) are equal.\nTo test the null hypothesis that \\(\\sigma_{12}=\\sigma_{22}\\) divide the larger s^2 value by the smaller s^2 to obtain the variance ratio, v.r.: \\[\nv.r. = \\frac{larger\\ s^2}{smaller\\ s^2}\n\\] To undertake this two-tailed test at the 5% level you need to carry out the one-tailed test at the 2.5% level. You can use the 2.5% F table of critical values, or you can use GenStat via the menus Data&gt;Probability Calculations… to find the P-value.\nIf you use the printed statistical table, you will need to compare the critical value you find there with the variance ratio you calculated. If the calculated variance ratio &gt; critical value, reject H0 and conclude that the variances are significantly different.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#example-two-sample-t-test",
    "href": "module02/02-ttest2.html#example-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Example: Two-Sample t-Test",
    "text": "Example: Two-Sample t-Test\nWeights of two breeds of cattle are to be compared: 15 cattle from Breed 1 and 12 cattle from Breed 2 were randomly sampled. Their recorded weights (kg) are shown.\n\n\n\nBreed 1\nBreed 2\n\n\n\n\n148.1\n187.6\n\n\n146.2\n180.3\n\n\n152.8\n198.6\n\n\n135.3\n190.7\n\n\n151.2\n196.3\n\n\n146.3\n203.8\n\n\n163.5\n190.2\n\n\n146.6\n201.0\n\n\n162.4\n194.7\n\n\n140.2\n221.1\n\n\n159.4\n186.7\n\n\n181.8\n203.1\n\n\n165.1\n\n\n\n165.0\n\n\n\n141.6\n\n\n\n\nThe following descriptive statistics are obtained from these data.\n\n\n\n\nBreed 1\nBreed 2\n\n\n\n\nSample mean (kg)\n153.700\n196.175\n\n\nSample s.d. (kg)\n12.301\n10.616\n\n\n\nIs there any systematic difference in their weights?\nThe question, “Is there any systematic difference in their weights?”, is asking whether or not there is any significant difference between the 2 samples – and in effect if there is any significant difference between the 2 breeds because the factor that distinguishes the 2 samples is breed.\nWe could answer this question by testing whether or not the population mean weights for the 2 breeds can be assumed to be equal:\n\\[H_0: \\mu_{breed\\ 1} = \\mu_{breed\\ 2}\\]versus\n\\[H_1: \\mu_{breed\\ 1} \\neq \\mu_{breed\\ 2}\\] To test this particular hypothesis about the equality of the means (as an avenue for answering our broader question), we would use a two-sample t-test.\nHowever, it is REALLY important to remember that there are other tests and hypotheses that we could use to answer our broader question (about whether or not there is any statistical difference between the weights of the 2 breeds). We’ll consider some non-parametric alternatives in Biometry 2.\nTo proceed with a two-sample t-test, there are 2 assumptions that we need to check:\n\nNormality of Breed 1 data and normality of Breed 2 data\nEquality of variances of the 2 samples\n\nBoth of these checks are hypothesis tests in their own right and contain the usual elements of a hypothesis test i.e. null & alternate hypothesis; test statistic; df; P-value or critical value; conclusion.\nTesting the Assumption of Normality\nIf you are doing the test by hand, you would need to assume that the data are normally distributed (and hope this is true). At least the data in this example is continuous… which is one small step towards normality.\nTesting the Assumption of Equality of Variance \\[H_0: \\sigma^2~Breed1~ = \\sigma^2~Breed2~\\  \\text{versus}\\  H_0: \\sigma^2~Breed1~ \\neq \\sigma^2~Breed2~ \\]\nTest statistic:\n\\[F_{Observed} = variance\\ ratio \\ (v.r.) = \\frac {larger\\ s^2}{smaller\\ s^2} = \\frac {12.301^2}{10.616^2} = \\frac {151.315}{112.699} =1.34\\] There are 2 degrees of freedom to calculate for an F test. They are called the numerator df, \\(\\nu1\\) and the denominator df, \\(\\nu2\\). (Remember from school days, numerator is the top half of a fraction, denominator is the bottom half of a fraction.) The \\(\\nu\\) that looks like a curly ‘v’ is the Greek letter ‘nu’.\nFor an F test used to test equality of variance, df are: \\(\\nu1 = n1 – 1\\), \\(\\nu 2 = n2 – 1\\).\nHere, \\(\\nu1 = n_{Breed 1} -1 = 15 - 1 = 14\\) and \\(\\nu2 = n_{Breed 2} - 1 = 12 – 1 = 11\\)\nUsing the 2.5% F table, we can compare (at the upper tail) the Fcritical and Fobserved values. If \\(Fobs &gt; Fcrit\\), we reject H0 and conclude that the variances of the 2 samples are NOT equal.\nFrom these tables, it is not possible to find exactly\\(F^{0.025}_{14,11}\\), so we will make do with the closest possible value \\(F^{0.025}_{15,11} = 3.33\\).\nWe find \\(F_{crit} \\approx 3.33\\) and \\(F_{obs} = 1.34\\). Since \\(F_{crit} &gt; F_{obs}\\) ,we CAN assume that the variances are equal.\nSo… assuming normality and equal variances, we can complete a “pooled” two-sample t-test where \\[\nSED = \\sqrt{s^2_p \\frac{1}{n1}+ \\frac {1}{n2}}\n\\] and \\[ s^2_p = \\frac{(n_1- 1)s^2_1 + (n_2 -1)s^2_2}{n_1 + n_2 -2}\\] \\(s^2_p\\) is the pooled estimate of variance and \\(df = n_1 +n_2 -2\\).\nTesting the null hypothesis that H0: \\(\\mu_{Breed 1} = \\mu_{Breed 2}\\), we find that there is a significant difference in the mean weights of the two breeds of cattle (T = 9.46, df = 25, \\(P &lt; 0.001\\)). The mean weight of Breed 2 is significantly higher and we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1.\nThis last piece of information, “we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1”, is obtained from the 95% confidence interval for the true difference (\\(\\mu1 = \\mu2\\)).\nNB. Recall that a 95% confidence interval for the true difference (\\(\\mu1 = \\mu2\\)) is\n\\[\n(\\overline{y_1}- \\overline{y_2})\\pm t^{0.025}_{df} \\times SED\n\\]",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module01/01-intro_statistical_programming.html",
    "href": "module01/01-intro_statistical_programming.html",
    "title": "Introduction to Statistical Programming",
    "section": "",
    "text": "Note\n\n\n\nThis section will not teach you the basics of R and RStudio. If you need that, please refer to A brief R guide for surviving ENVX1002. You will need to be logged into Canvas to access this link.",
    "crumbs": [
      "**📕 Module 1**",
      "Introduction to Statistical Programming"
    ]
  },
  {
    "objectID": "module01/01-intro_statistical_programming.html#data-introduction",
    "href": "module01/01-intro_statistical_programming.html#data-introduction",
    "title": "Introduction to Statistical Programming",
    "section": "Data Introduction",
    "text": "Data Introduction\nWhile we are all not ecologists, we will be working with two datasets from the Long Term Ecological Research (LTER) Network:\n\nSalamander Counts (and_vertebrates): Data from the Andrews Forest LTER site tracking salamander populations across different streams.\nLake Ice Duration (ntl_icecover): Records from the North Temperate Lakes LTER site measuring annual ice cover duration.\n\nLoad the data to get started:\n\npacman::p_load(lterdatasampler)\n\n# load the data\ndata(\"and_vertebrates\", \"ntl_icecover\")\n\nThese are very large datasets – but don’t be intimidated! The point is to show you that statistical programming can handle big data, and it will all make sense as we go along.\n\nMetadata\nStatistical analysis starts by exploring the data’s structure. Sometimes, datasets include metadata—context that helps explain the data.\nFor the data that we are using today, metadata is available in the documentation. Use ?and_vertebrates and ?ntl_icecover to access it.\n?and_vertebrates\n?ntl_icecover\n\n\nUnderstanding Data Structure\nThe str() function shows us the types of variables and how the data is “arranged”:\n\nstr(and_vertebrates)\n\ntibble [32,209 × 16] (S3: tbl_df/tbl/data.frame)\n $ year       : num [1:32209] 1987 1987 1987 1987 1987 ...\n $ sitecode   : chr [1:32209] \"MACKCC-L\" \"MACKCC-L\" \"MACKCC-L\" \"MACKCC-L\" ...\n $ section    : chr [1:32209] \"CC\" \"CC\" \"CC\" \"CC\" ...\n $ reach      : chr [1:32209] \"L\" \"L\" \"L\" \"L\" ...\n $ pass       : num [1:32209] 1 1 1 1 1 1 1 1 1 1 ...\n $ unitnum    : num [1:32209] 1 1 1 1 1 1 1 1 1 1 ...\n $ unittype   : chr [1:32209] \"R\" \"R\" \"R\" \"R\" ...\n $ vert_index : num [1:32209] 1 2 3 4 5 6 7 8 9 10 ...\n $ pitnumber  : num [1:32209] NA NA NA NA NA NA NA NA NA NA ...\n $ species    : chr [1:32209] \"Cutthroat trout\" \"Cutthroat trout\" \"Cutthroat trout\" \"Cutthroat trout\" ...\n $ length_1_mm: num [1:32209] 58 61 89 58 93 86 107 131 103 117 ...\n $ length_2_mm: num [1:32209] NA NA NA NA NA NA NA NA NA NA ...\n $ weight_g   : num [1:32209] 1.75 1.95 5.6 2.15 6.9 5.9 10.5 20.6 9.55 13 ...\n $ clip       : chr [1:32209] \"NONE\" \"NONE\" \"NONE\" \"NONE\" ...\n $ sampledate : Date[1:32209], format: \"1987-10-07\" \"1987-10-07\" ...\n $ notes      : chr [1:32209] NA NA NA NA ...\n\n\nThere is also glimpse(), which requires you to load the tidyverse package:\n\npacman::p_load(tidyverse)\nglimpse(and_vertebrates)\n\nRows: 32,209\nColumns: 16\n$ year        &lt;dbl&gt; 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987…\n$ sitecode    &lt;chr&gt; \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\", \"MACKCC-L\"…\n$ section     &lt;chr&gt; \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\", \"CC\"…\n$ reach       &lt;chr&gt; \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\"…\n$ pass        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ unitnum     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ unittype    &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"…\n$ vert_index  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1, …\n$ pitnumber   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ species     &lt;chr&gt; \"Cutthroat trout\", \"Cutthroat trout\", \"Cutthroat trout\", \"…\n$ length_1_mm &lt;dbl&gt; 58, 61, 89, 58, 93, 86, 107, 131, 103, 117, 100, 127, 99, …\n$ length_2_mm &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ weight_g    &lt;dbl&gt; 1.75, 1.95, 5.60, 2.15, 6.90, 5.90, 10.50, 20.60, 9.55, 13…\n$ clip        &lt;chr&gt; \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"NONE\", \"N…\n$ sampledate  &lt;date&gt; 1987-10-07, 1987-10-07, 1987-10-07, 1987-10-07, 1987-10-0…\n$ notes       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nYou will soon realise that there are many ways to “skin a cat” in R – that is, there are multiple ways to achieve the same outcome. You can pick any method as long as you are comfortable with it.\nIn general, the outputs from str() and glimpse() can seem overwhelming at first. This is completely normal, especially when you’re new to data exploration. Take your time, and don’t worry if it doesn’t all make sense immediately – it will come with practice.\n\n\nPreviewing the Data\nThe head() function shows us the first few rows of a dataset:\n\nhead(and_vertebrates)\n\n# A tibble: 6 × 16\n   year sitecode section reach  pass unitnum unittype vert_index pitnumber\n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1  1987 MACKCC-L CC      L         1       1 R                 1        NA\n2  1987 MACKCC-L CC      L         1       1 R                 2        NA\n3  1987 MACKCC-L CC      L         1       1 R                 3        NA\n4  1987 MACKCC-L CC      L         1       1 R                 4        NA\n5  1987 MACKCC-L CC      L         1       1 R                 5        NA\n6  1987 MACKCC-L CC      L         1       1 R                 6        NA\n# ℹ 7 more variables: species &lt;chr&gt;, length_1_mm &lt;dbl&gt;, length_2_mm &lt;dbl&gt;,\n#   weight_g &lt;dbl&gt;, clip &lt;chr&gt;, sampledate &lt;date&gt;, notes &lt;chr&gt;\n\n\nWhile tail() shows the last few rows:\n\ntail(ntl_icecover)\n\n# A tibble: 6 × 5\n  lakeid      ice_on     ice_off    ice_duration  year\n  &lt;fct&gt;       &lt;date&gt;     &lt;date&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 Lake Monona 2014-12-02 2015-04-02          105  2014\n2 Lake Monona 2016-01-11 2016-03-13           62  2015\n3 Lake Monona 2016-12-16 2017-03-07           81  2016\n4 Lake Monona 2017-12-26 2018-03-29           93  2017\n5 Lake Monona 2018-12-11 2019-03-31           97  2018\n6 Lake Monona 2019-12-16 2020-03-20           80  2019\n\n\n\n\nQuick Statistical Overview\nThe summary() function provides a statistical overview of your data, adapting its output based on variable types (numerical, categorical, etc.). This means that it can be useful for some datasets but not for others.\n\nsummary(and_vertebrates)\n\n      year        sitecode           section             reach          \n Min.   :1987   Length:32209       Length:32209       Length:32209      \n 1st Qu.:1998   Class :character   Class :character   Class :character  \n Median :2006   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2005                                                           \n 3rd Qu.:2012                                                           \n Max.   :2019                                                           \n                                                                        \n      pass          unitnum         unittype           vert_index    \n Min.   :1.000   Min.   : 1.000   Length:32209       Min.   :  1.00  \n 1st Qu.:1.000   1st Qu.: 3.000   Class :character   1st Qu.:  5.00  \n Median :1.000   Median : 7.000   Mode  :character   Median : 13.00  \n Mean   :1.224   Mean   : 7.696                      Mean   : 20.17  \n 3rd Qu.:1.000   3rd Qu.:11.000                      3rd Qu.: 27.00  \n Max.   :2.000   Max.   :20.000                      Max.   :147.00  \n                                                                     \n   pitnumber          species           length_1_mm      length_2_mm   \n Min.   :   62048   Length:32209       Min.   : 19.00   Min.   : 28.0  \n 1st Qu.:13713632   Class :character   1st Qu.: 47.00   1st Qu.: 77.0  \n Median :18570447   Mode  :character   Median : 63.00   Median : 98.0  \n Mean   :16286432                      Mean   : 73.83   Mean   :100.5  \n 3rd Qu.:19132429                      3rd Qu.: 97.00   3rd Qu.:119.0  \n Max.   :28180046                      Max.   :253.00   Max.   :284.0  \n NA's   :26574                         NA's   :17       NA's   :19649  \n    weight_g           clip             sampledate            notes          \n Min.   :  0.090   Length:32209       Min.   :1987-10-06   Length:32209      \n 1st Qu.:  1.510   Class :character   1st Qu.:1998-09-04   Class :character  \n Median :  6.050   Mode  :character   Median :2006-09-06   Mode  :character  \n Mean   :  8.903                      Mean   :2005-08-05                     \n 3rd Qu.: 11.660                      3rd Qu.:2012-09-05                     \n Max.   :134.590                      Max.   :2019-09-05                     \n NA's   :13268                                                               \n\nsummary(ntl_icecover)\n\n          lakeid        ice_on              ice_off            ice_duration  \n Lake Mendota:167   Min.   :1851-12-13   Min.   :1852-03-25   Min.   : 21.0  \n Lake Monona :167   1st Qu.:1895-12-04   1st Qu.:1896-04-04   1st Qu.: 92.0  \n Lake Wingra :  0   Median :1936-12-07   Median :1937-10-01   Median :103.0  \n                    Mean   :1937-03-15   Mean   :1937-09-25   Mean   :102.8  \n                    3rd Qu.:1978-12-09   3rd Qu.:1979-04-08   3rd Qu.:115.0  \n                    Max.   :2020-01-12   Max.   :2020-03-22   Max.   :161.0  \n                    NA's   :1            NA's   :2            NA's   :3      \n      year     \n Min.   :1851  \n 1st Qu.:1894  \n Median :1936  \n Mean   :1936  \n 3rd Qu.:1978  \n Max.   :2019",
    "crumbs": [
      "**📕 Module 1**",
      "Introduction to Statistical Programming"
    ]
  },
  {
    "objectID": "module01/01-intro_statistical_programming.html#summary-statistics",
    "href": "module01/01-intro_statistical_programming.html#summary-statistics",
    "title": "Introduction to Statistical Programming",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\nMean: central tendency\nThe arithmetic mean is often our first step in understanding typical values in data.\nMathematically, the mean (\\(\\bar{x}\\)) is calculated as the sum of all values (\\(x_i\\)) divided by the number of values (\\(n\\)):\n\\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\nFor example, consider the following set of salamander weights (in grams): 2, 3, 5, 7, and 8. The mean weight is:\n\\[\n\\bar{x} = \\frac{2 + 3 + 5 + 7 + 8}{5} = \\frac{25}{5} = 5\n\\]\nIn simpler terms, the mean is what you get if you add up all the numbers and then divide by how many numbers you added. It’s a way to find the “average” value.\nIn R, we can calculate the mean using the mean() function. Note that we used the $ operator to access the specific variables in the dataset. It will make sense once you start typing the code as RStudio will provide you with suggestions.\n\n# Calculate mean salamander weight\nmean(and_vertebrates$weight_g, na.rm = TRUE)\n\n[1] 8.902859\n\n# Calculate mean ice duration\nmean(ntl_icecover$ice_duration, na.rm = TRUE)\n\n[1] 102.8187\n\n\nMissing values are called NA in R. The na.rm = TRUE argument tells R to ignore these missing values when calculating the mean. If you don’t include this argument, R will return NA as the mean as mathematically, you can’t do calculations with unknown values.\n\n# This will return NA\nmean(and_vertebrates$weight_g)\n\n[1] NA\n\n\n\n\nMedian: a “robust” central measure\nThe median is the middle value when all numbers are arranged in order, like finding the middle person if everyone lined up by height. This makes it especially good at representing “typical” values when there are some extreme measurements in your data.\nFor example, let’s take these salamander weights (in grams): 2, 3, 5, 7, 20 To find the median:\n\nArrange in order: 2, 3, 5, 7, 20\nFind the middle number: 5\n\nEven though there’s one unusually large salamander (20g), the median (5g) still represents a typical salamander weight. Compare this to the mean:\n(2 + 3 + 5 + 7 + 20) ÷ 5 = 7.4g\nThe mean is pulled higher by the unusually large salamander, making it less representative of a typical salamander in this case. This is why we say the median is “robust” - it’s not easily influenced by extreme values or outliers.\nFor datasets with an even number of values, we take the average of the two middle numbers. For example, with weights 2, 3, 5, 7:\n\nArrange in order: 2, 3, 5, 7\nFind the middle pair: 3 and 5\nCalculate their average: (3 + 5) ÷ 2 = 4\n\nThis robustness proves particularly valuable in data where extreme values are common:\n\nHouse prices in a neighborhood (a few mansion prices won’t skew the “typical” house value)\nAnimal body sizes (a few giants won’t affect the “typical” size)\nStream depths (a few very deep pools won’t change the “typical” depth)\n\nLet’s look at our full salamander data:\n\n# Median ice_duration\nmedian(ntl_icecover$ice_duration, na.rm = TRUE)\n\n[1] 103\n\n# Compare with mean\nmean(ntl_icecover$ice_duration, na.rm = TRUE)\n\n[1] 102.8187\n\n\nFrom the above you can see that both values are close indicating that the data is not skewed by extreme values. We will learn more about how this relates to statistics and data distributions in the next few chapters.\n\n\nMode: Most frequent value\nThe mode represents the most frequently occurring value within a dataset, making it especially valuable for understanding the most common category or value in a dataset, whereas the mean and median are more suitable for continuous numerical data.\nFor example, consider these salamander counts in different sections of a stream: 2, 3, 2, 4, 2, 1, 3\nTo find the mode: 1. Count how often each value appears: - 1 appears once - 2 appears three times - 3 appears twice - 4 appears once 2. The mode is 2 because it appears most frequently (three times)\nSome datasets can have multiple modes:\n\nIf we had: 2, 3, 2, 4, 3, 1, 3\nBoth 2 and 3 appear three times\nThis dataset would be “bimodal” (having two modes)\n\nWhile the mode is useful for discrete data like species counts, R lacks a built-in function for it. We could write one, but we’ll use existing packages instead. It is good practice to find out if a package exists before writing your own function since you are (probably) not in this unit to become a software developer.\nA Google search should lead you to multiple packages that can calculate the mode, we’ll just use the modeest package for now. Note that the function used is called mlv() (Maximum Likelihood Value):\n\npacman::p_load(modeest)\nml &lt;- mlv(and_vertebrates$weight_g, method = \"shorth\", na.rm = TRUE)\n\nNow that we know that the modal weight in grams is 2, we can use this information to understand the distribution of salamander weights in the dataset.",
    "crumbs": [
      "**📕 Module 1**",
      "Introduction to Statistical Programming"
    ]
  },
  {
    "objectID": "module01/01-intro_statistical_programming.html#measures-of-spread",
    "href": "module01/01-intro_statistical_programming.html#measures-of-spread",
    "title": "Introduction to Statistical Programming",
    "section": "Measures of Spread",
    "text": "Measures of Spread\n\nRange\nThe range helps us understand the full scope of variation in their measurements. For salamanders, this might reveal the difference between the smallest juvenile and largest adult:\n\n# Range of salamander weights\nrange(and_vertebrates$weight_g, na.rm = TRUE)\n\n[1]   0.09 134.59\n\n# Alternative using min and max\nmin(and_vertebrates$weight_g, na.rm = TRUE)\n\n[1] 0.09\n\nmax(and_vertebrates$weight_g, na.rm = TRUE)\n\n[1] 134.59\n\n\nThe range provides a basic understanding of data spread, showing the total span of values. It’s useful for a quick look at the extent of variation in data. However, the range doesn’t tell us how the values are distributed within that span. To understand this, we need more advanced measures like variance and standard deviation.\n\n\nVariance and standard deviation\nVariance and standard deviation help us understand how spread out our data is from the mean. Think of them as measuring how much values typically “deviate” or differ from the average.\n\nVariance\nVariance measures the average squared distance of each value from the mean. Let’s break this down with a simple example using salamander weights (in grams): 2, 3, 5, 7, 8\n\nCalculate the mean: (2 + 3 + 5 + 7 + 8) ÷ 5 = 5\nFind how far each value is from the mean:\n\n2 is 3 below the mean: (2 - 5) = -3\n3 is 2 below the mean: (3 - 5) = -2\n5 equals the mean: (5 - 5) = 0\n7 is 2 above the mean: (7 - 5) = 2\n8 is 3 above the mean: (8 - 5) = 3\n\nSquare these differences (this makes all values positive):\n\n(-3)² = 9\n(-2)² = 4\n0² = 0\n2² = 4\n3² = 9\n\nCalculate the average of these squared differences: (9 + 4 + 0 + 4 + 9) ÷ 5 = 5.2\n\nThe variance in this case is 5.2 (square grams). This squared unit is a bit awkward - imagine trying to explain that salamanders vary by “5.2 square grams” from the mean! This is one of the main reasons we use standard deviation instead of variance.\n\n\nStandard Deviation\nThe standard deviation is simply the square root of the variance. We often prefer it because: - It’s in the same units as our original data (grams instead of square grams) - It’s easier to interpret in relation to the mean\nFor our example: - Variance = 5.2 - Standard deviation = √5.2 ≈ 2.28 grams\nThis means that salamander weights typically vary by about 2.28 grams above or below the mean.\nLet’s calculate these for our full salamander dataset:\n\n# Variance in salamander weights\nvar(and_vertebrates$weight_g, na.rm = TRUE)\n\n[1] 113.9829\n\n# Standard deviation (square root of variance)\nsd(and_vertebrates$weight_g, na.rm = TRUE)\n\n[1] 10.67628\n\n\nThe mean, standard deviation, and variance are key for understanding data variability and underpin many statistical tests/models. Expect to see them often in research.\n\n\n\nInterquartile range (IQR)\nThe Interquartile Range (IQR) is a robust measure of spread that tells us the range where the middle 50% of our data falls. It’s calculated as the difference between the 75th percentile (Q3) and the 25th percentile (Q1).\nTo understand IQR, let’s first understand quartiles using a simple example with salamander weights (in grams): 2, 2, 3, 4, 5, 5, 6, 8, 10\n\nFirst, arrange the data in order (already done above)\nFind the median (Q2) - it’s 5\nSplit the data into two halves:\n\nLower half: 2, 2, 3, 4\nUpper half: 5, 6, 8, 10\n\nFind Q1 (median of lower half) = 2.5\nFind Q3 (median of upper half) = 7\nCalculate IQR = Q3 - Q1 = 7 - 2.5 = 4.5\n\nThe IQR tells us that the middle 50% of salamander weights vary by 4.5 grams. This is particularly useful because:\n\nIt’s not affected by extreme values (very large or small salamanders)\nIt gives us a sense of “typical” variation in the population\nIt helps identify outliers: values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR are often considered outliers\n\nThe IQR is particularly useful in studies where we want to focus on typical variations while excluding extreme values. For lake ice duration, this helps us understand normal annual patterns whilst excluding unusually warm or cold years:\n\n# IQR for ice duration\nIQR(ntl_icecover$ice_duration, na.rm = TRUE)\n\n[1] 23\n\n# Get quartiles (handling missing values)\nquantile(ntl_icecover$ice_duration, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n  21   92  103  115  161 \n\n\nLooking at the quartiles:\n\nQ1 (25th percentile): 25% of ice durations are below this value\nQ2 (50th percentile/median): The middle ice duration\nQ3 (75th percentile): 75% of ice durations are below this value\nIQR = Q3 - Q1: The range where the middle 50% of ice durations fall\n\nThis information helps us understand the typical variation in ice duration while being less sensitive to extreme weather events that might produce unusually long or short ice cover periods.",
    "crumbs": [
      "**📕 Module 1**",
      "Introduction to Statistical Programming"
    ]
  },
  {
    "objectID": "labs/Lab11.html",
    "href": "labs/Lab11.html",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to perform MLR and interpret the results using R;\nUndertake hypothesis testing to determine if model is significant\nUndertake hypothesis testing to determine if the true partial regression slope \\(\\neq\\) 0\nCheck assumptions are filled prior to assessing model output\nAssess model summary in terms of fit and P-values\nConsider more parsimonious models",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#before-you-begin",
    "href": "labs/Lab11.html#before-you-begin",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-11.Rmd or similar. The following data files are required:\n\nENVX1002_wk11_practical_data_Regression.xlsx\n\nLast week you explored simple linear regression and assessed the output of your models.\nThis week we will build upon this and venture into multiple linear regression.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon’t forget to save as you go!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-1-corn-yields",
    "href": "labs/Lab11.html#exercise-1-corn-yields",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 1: Corn yields",
    "text": "Exercise 1: Corn yields\nData: Corn spreadsheet\nIn this data\n\ny = P content of corn\n\nx1 = inorganic P content of soil\n\nx2 = organic P content of soil\n\nn = 17 sites (The original data had 18 sites, one is removed here.)\n\nAim of our investigation: Understand the relationship between Organic and Inorganic phosphorous contents in the soil, and the phosphorous content of corn. This will allow us to see which type of phosphorous is being taken up by the corn.\n\nlibrary(readxl)\nCorn &lt;- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Corn\")\nhead(Corn)\n\n# A tibble: 6 × 3\n  CornP InorgP  OrgP\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    64    0.4    53\n2    60    0.4    23\n3    71    3.1    19\n4    61    0.6    34\n5    54    4.7    24\n6    77    1.7    65\n\n\n\n\n1.1 Examine the correlations\nSome people find it difficult to visually interpret graphical summaries of data in more than 2 dimensions; however, 3-dimensional surface plots are reasonably common in statistics although not usually in descriptive statistics.\nInitially we will examine the pairwise correlations to “get a feel” for the data. We will then make a 3-dimensional surface plot using the lattice package .\nUsing R, we can calculate the correlation matrix quite easily.\nNote the use of round() to limit the number of significant digits.\n\nround(cor(Corn),3)\n\n       CornP InorgP  OrgP\nCornP  1.000  0.720 0.212\nInorgP 0.720  1.000 0.399\nOrgP   0.212  0.399 1.000\n\n\n\nWhat do the results of the correlation matrix tell you?\n\n\nBased on the correlation matrix, if we were to fit a single predictor model involving EITHER InorgP OR OrgP, then which model would be more successful?\n\n          (Hint, the r2 is exactly that for a single predictor regression, the square of the correlation, r).\nThe pairs plot creates scatterplots between each possible pair of variables. Like a single scatterplot the pairs plot allows us to visually observe any trends.\n\nObserving the pairs plot below, do you see any strong trends? How well does this link to your correlation matrix?\n\n\npairs(Corn)\n\n\n\n\n\n\n\n\n\n\nSimple 3-D plot\nUnlike a simple plot we can creat for a simple linear regression, it is a bit more complex to visualise a model with more predictors. One way we can visualise the relationship is with a 3-D plot, which can be made using the function levelplot() in lattice.\nHere we plot the OrgP and InorgP in the axes and the levels in the plot are CornP.\nNote the package Viridis has been called, this is through personal choice.\nThe Viridis package has a range of assembled colour ramps which are easier for the reader to differentiate the colours, especially when printed in grayscale, or if the reader is colourblind.\n\nlibrary(lattice, quiet = T)\nlibrary(viridis, quiet = T) # will need to install.packages(\"viridis\")   \n\nlevelplot(CornP ~ InorgP + OrgP, data = Corn\n          , col.regions = viridis(100))\n\n\n\n\n\n\n\n\nThe level plot shows us the x (InorgP) and y variable (OrgP), with the colour scale representing the z variable, which in this case is Phosphorous being taken up by the corn (cornP). From the plot we can see that with higher levels of OrgP and InorgP in the soil, the Phosphorous content in the corn is generally higher.\nIt is clear that the 3-D surface plot does not have colours everywhere, but this relates of course to the underlying data. In this case we don’t have continuous data in both directions, so the response (the colour) is only plotted where we have input variables.\nIf we did have continuous data in both directions the plot would look more like a heatmap, here are some examples.\n\n\n\n\n1.2 Fit the model\nWe will now use regression to estimate the joint effects of both inorganic phosphorus and organic phosphorus on the phosphorus content of corn.\n\\(CornP = \\beta_0 + \\beta_1 InorgP + \\beta_2 OrgP + error\\)\nThis is fairly simple and follows the same structure as simple linear regression and uses lm().\n\nMLR.Corn &lt;- lm(CornP ~ InorgP + OrgP, data=Corn)\n\n\n\n\n1.3 Check assumptions\nLet’s check the assumptions of regression are met via residual diagnostics.\n\nAre there any apparent problems with normality of CornP residuals or equality of variance for this small data set?\n\n\npar(mfrow=c(2,2))\nplot(MLR.Corn)\n\n\n\n\n\n\n\n\n\n\n\n1.4 Model output\nAfter checking our assumptions and we are happy with them, we can interpret our model output.\n\nIncorporating the partial regression coefficient estimates, what is the model equation?\n\n\nsummary(MLR.Corn)\n\n\nCall:\nlm(formula = CornP ~ InorgP + OrgP, data = Corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.282  -4.428   2.645   4.949  16.946 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  66.4654     9.8496   6.748 9.35e-06 ***\nInorgP        1.2902     0.3428   3.764  0.00209 ** \nOrgP         -0.1110     0.2486  -0.447  0.66195    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.25 on 14 degrees of freedom\nMultiple R-squared:  0.5253,    Adjusted R-squared:  0.4575 \nF-statistic: 7.746 on 2 and 14 DF,  p-value: 0.005433\n\n\n\nSimple linear regression allowed us to describe the relationship incorporating our regression coefficient estimate. We would interpret it as follows:\n*“As* \\(x\\) increases by 1, \\(y\\) decreases by \\(b_1\\) units” (depending on the direction of the relationship).\nThis week it is a bit different because we are dealing with partial regression coefficients instead.\nInstead, we would say:\n*“as* \\(x_1\\) increases by 1, \\(y\\) decreases by \\(b_1\\) units given all other partial regression coefficients are held constant”.\nApplied to our model, if we wanted to describe the relationship between InorgP and CornP, we would say:\n“As InorgP increases by 1, CornP increases by 1.2902, given OrgP is held constant.”\n\nGiven the above, how would you interpret the relationship between OrgP and CornP?\n\n\n\n\n1.5 Is the model useful?\nRemember now that the F-test and T-test are testing slightly different things.\n\n\nF-test\n\\(H_0:\\) all \\(\\beta_k = 0\\), i.e. \\(\\beta_1 = \\beta_2 = 0\\)\n\\(H_1:\\) at least 1 \\(\\beta_k \\neq 0\\), i.e. our model is significant\nWe will find the P-value for this test at the end of the summary output.\n\n\n\nt-test\n\\(H_0: \\beta_k = 0\\)\n\\(H_1: \\beta_k \\neq 0\\)\nWhere \\(\\beta_k\\) refers to one of the model partial regression coefficients. In the case of our model we only have 2: \\(b_1\\) (InorgP) and \\(b_2\\) (OrgP).\n\nNow it is your turn:\n\nLooking at the summary() output, is our overall model significant?\n\n\nWhich independent variable is a significant predictor of corn yield?\n\n\n\n\n\n1.6 How good is the model?\n\nHow much of the variation in CornP content is explained by the two independent variables?\n\n\nRun the model again but this time with only the significant independent variable. How do the model performance criteria (r2-adj, P-values, Residual Standard Error) change?\n\n\n\n\n1.7 Our conclusions\nWriting up conclusions for multiple linear regression are similar to simple linear regression, just with a couple of extra P-values to state.\nWe would first mention that our overall model is significant as we rejected the null hypothesis (P = 0.05). We could then describe the hypothesis test results for our predictor variables. Finally, we would describe the model fit and our adjusted-r2.\nRemember the scientific conclusion then relates our findings back to the context, answering aims.\n\nWhat would our statistical conclusion be?\n\n\nWhat would our Scientific conclusion be?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-2-water-quality",
    "href": "labs/Lab11.html#exercise-2-water-quality",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 2: Water quality",
    "text": "Exercise 2: Water quality\nData: Microbes spreadsheet\nThis exercise will use data from the NOAA Fisheries data portal. The dataset contains the results of microbial study of Pudget Sound, an estuary in Seattle, U.S.A.\nThe dataset contains the following variables:\n\ntotal_bacteria = Total bacteria (cells per ml) –&gt; this will be our response variable\nwater_temp = Water temperature (°C)\ncarbon_L_h = microbial production (µg carbon per L per hour)\nNO3 = Nitrate (µm)\n\nFirst thing to do, is read in the data. This time we will be using the Microbes spreadsheet:\n\nmic &lt;- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Microbes\")\n\n# Check the structure\nstr(mic)\n\ntibble [55 × 4] (S3: tbl_df/tbl/data.frame)\n $ total_bacteria: num [1:55] 498353 529135 529911 603798 632847 ...\n $ water_temp    : num [1:55] 11.2 8.5 8.6 11.2 8.4 8.9 11.3 8.5 9 8.3 ...\n $ carbon_L_h    : num [1:55] 0.286 0.37 0.324 0.287 0.637 ...\n $ NO3           : num [1:55] 21.1 23.5 23.5 21.8 23 ...\n\n\n\n\n2.1 Examine the correlations\nFor this dataset we may expect to see some correlations;\n\nWarmer water temperature we would expect to see a higher amount of bacterial growth\nCarbon is a proxy for microbial production, so if we see a higher rate of carbon production, we would expect to see higher levels of bacteria\nNO3 (Nitrate) is an essential nutrient for plants and some bacteria species metabolise this\n\n\nLet’s test this. Observe the correlation matrix and pairs plots. Do you notice any strong correlations?\n\n\ncor(mic)\n\n               total_bacteria water_temp carbon_L_h        NO3\ntotal_bacteria      1.0000000  0.6445878  0.6629503 -0.7587849\nwater_temp          0.6445878  1.0000000  0.5883947 -0.6958635\ncarbon_L_h          0.6629503  0.5883947  1.0000000 -0.7653497\nNO3                -0.7587849 -0.6958635 -0.7653497  1.0000000\n\npairs(mic)\n\n\n\n\n\n\n\n\n\n\n\n2.2 Fit the model\nWe can now fit the model to see how much these predictors account for the variation in total bacteria.\n\\(total bacteria = \\beta_0 + \\beta_1 watertemp + \\beta_2 carbon + \\beta_3 NO3 + error\\)\nThere are two forms the lm code can take; you can either specify which variables you want to include by naming each one, or if only your desired variables are within your dataset, you can use the ~. to specify all columns.\n\nnames(mic) # tells us column names within the dataset\n\n[1] \"total_bacteria\" \"water_temp\"     \"carbon_L_h\"     \"NO3\"           \n\n# Form 1:\nmic.lm &lt;- lm(total_bacteria ~ water_temp + carbon_L_h + NO3, data = mic)\n\n# Form 2\nmic.lm &lt;- lm(total_bacteria ~ ., data = mic)\n\n\n\n\n2.3 Check assumptions\nLet’s check the assumptions of regression are met via residual diagnostics.\n\nAre there any apparent problems with normality of total_bacteria residuals or equality of variance for this data set?\n\n\npar(mfrow=c(2,2))\nplot(mic.lm)\n\n\n\n\n\n\n\n\n\n\n\n2.4 Model output\nAfter investigating the assumptions, they seem to be ok, so we can move onto the model summary.\n\nsummary(mic.lm)\n\n\nCall:\nlm(formula = total_bacteria ~ ., data = mic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-536948 -191145   -2584  154144  539394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   766294     314957   2.433  0.01852 * \nwater_temp     40051      23547   1.701  0.09505 . \ncarbon_L_h     84425      67427   1.252  0.21624   \nNO3           -16586       5256  -3.156  0.00268 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 250500 on 51 degrees of freedom\nMultiple R-squared:  0.614, Adjusted R-squared:  0.5913 \nF-statistic: 27.04 on 3 and 51 DF,  p-value: 1.318e-10\n\n\n\nIncorporating the partial regression coefficient estimates, what is the equation for this model?\n\n\nLike you did in Exercise 1.4, how would you interpret the relationship between total_bacteria and water_temp?\n\n\n\n\n2.5 Is the model useful?\n\nObserving the P-value of the F-statistic in the summary, can we say our model is significant?\n\n\nAre any predictors significant?\n\n\n\n\n2.6 How good is the model?\n\nHow much of the variation in total bacteria is explained by the three independent variables?\n\n\nRun the model again but this time excluding the variable with the largest P-value. How do the model performance criteria (r2-adj, P-values, Residual Standard Error) change?\n\n\nsummary(lm(total_bacteria ~ water_temp + NO3, data = mic))\n\n\nCall:\nlm(formula = total_bacteria ~ water_temp + NO3, data = mic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-506706 -203093  -11950  157707  533145 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   858466     307901   2.788  0.00739 ** \nwater_temp     43611      23502   1.856  0.06917 .  \nNO3           -20620       4175  -4.938 8.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 251900 on 52 degrees of freedom\nMultiple R-squared:  0.6021,    Adjusted R-squared:  0.5868 \nF-statistic: 39.34 on 2 and 52 DF,  p-value: 3.927e-11\n\n\n\n\n\n2.7 Conclusions\n\nWhat would the statistical conclusion be for this model?\n\n\nWhat would our scientific conclusion be?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-3-dippers",
    "href": "labs/Lab11.html#exercise-3-dippers",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 3: Dippers",
    "text": "Exercise 3: Dippers\nData: Dippers spreadsheet\nWe will revisit the Dippers dataset from last week, but now incorporating other factors which may be influencing the distribution.\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\nTwenty-two sites were included in the survey. Variables are as follows\n\nwater hardness\nriver-bed slope\nthe numbers of caddis fly larvae\nthe numbers of stonefly larvae\nthe number of breeding pairs of dippers per 10 km of river\n\nIn the analyses, the four invertebrate variables were transformed using a Log(Number+1) transformation.\n\nNow it is your turn to work through the steps as above. What other factors are influencing the number of breeding pairs of Dippers?\n\nRead in the data from today’s Excel sheet, the corresponding sheet name is “Dippers”\n\n\nInvestigate a correlation matrix and pairs plot of the dataset, are there signs of a relationship between breeding pair density and other independent variables?\n\n\npairs(Dippers)\n\n\n\n\n\n\n\ncor(Dippers)\n\n             Hardness RiverSlope   Br_Dens   LogCadd   LogStone\nHardness   1.00000000  0.2420874 0.3503482 0.3337586 0.02951218\nRiverSlope 0.24208735  1.0000000 0.7102216 0.4313081 0.57479242\nBr_Dens    0.35034817  0.7102216 1.0000000 0.6126891 0.76294367\nLogCadd    0.33375857  0.4313081 0.6126891 1.0000000 0.44347440\nLogStone   0.02951218  0.5747924 0.7629437 0.4434744 1.00000000\n\n\n\nLet’s investigate further. Run the model incorporating all of our predictors, but before looking at our model output, are the assumptions ok?\n\n\n# Run model\ndipper.lm &lt;- lm(Br_Dens ~ ., data=Dippers)\n\n# Check assumptions\npar(mfrow = c(2,2))\nplot(dipper.lm)\n\n\n\n\n\n\n\n\n\nOnce you are happy assumptions are good, you can interpret the model output using summary().\n\nWhat is the equation for our model, incorporating our partial regression coefficients?\n\n\nBased on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd this time?\n\n\nIs LogCadd still a significant predictor of Dipper breeding pair density?\n\n\nWhat are the significant predictors of this model?\n\n\nHow good is the fit of our model?\n\n\nWhat might we do to improve the model fit?\n\n\nWhat statistical and scientific conclusions can we make from this model output?\n\n\nAnother week of linear models done! Great work fitting Multiple Linear Regression! Next week we break away and explore non-linear functions.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-1-house-prices",
    "href": "labs/Lab11.html#exercise-1-house-prices",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 1: House Prices",
    "text": "Exercise 1: House Prices\nData: - Housing\nThis exercise will use a dataset from kaggleto explore the variables affecting house prices.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-2-energy-use",
    "href": "labs/Lab11.html#exercise-2-energy-use",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 2: Energy use",
    "text": "Exercise 2: Energy use\nData:\n\nenergy\n\nUse the dataset from kaggle to explore which variables affect energy consumption.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-3-sales-vs-advertising-budget",
    "href": "labs/Lab11.html#exercise-3-sales-vs-advertising-budget",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 3: Sales vs advertising budget",
    "text": "Exercise 3: Sales vs advertising budget\nData:\n\nadvertising budget\n\nUse the dataset from kaggle to explore how different advertising budgets affect sales.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab09.html#before-you-begin",
    "href": "labs/Lab09.html#before-you-begin",
    "title": "Lab 9 – Describing relationships",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-09.Rmd or similar. The following data files are required:\n\nENVX1002_practical_data_Regression.xlsx",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 -- Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-1-linear-modelling-in-excel",
    "href": "labs/Lab09.html#exercise-1-linear-modelling-in-excel",
    "title": "Lab 9 – Describing relationships",
    "section": "Exercise 1: Linear Modelling in Excel",
    "text": "Exercise 1: Linear Modelling in Excel\nThis exercise focusses on fitting the model parameters and demonstrating two ways a model can be fitted – numerical or analytical;\n\nAnalytical: equation(s) are used directly to find solution, e.g. estimate parameters that minimise residual sum of squares\nNumerical: computer uses “random guesses” to find set of parameters to that minimises objective function, in this case residual sum of squares\n\nWe mostly use R for modelling, but R does everything automatically. It is important to know what is going on ‘behind the scenes’, which is why we are starting in Excel. Similar to the tutorial, you will be calculating each component of the model parameter step by step in the exercises that follow.\n\n\n1.1 Horses\nThis is our example of Analytical fitting method.\nThe number of horses on Canadian farms appeared to decrease after the war:\n\n\n\n\n\nyear\n1944\n1945\n1946\n1947\n1948\n\n\nhorse\n28\n26\n22\n20\n19\n\n\n\n\n\n\nTo see whether this is likely to be true, fit a model to the above data ‘by hand’ in Excel. To aid the calculation it is recommended to fill out the Excel table provided ENVX1002_practical_data_Regression.xlsx, you can find it in the spreadsheet labelled Horses.\n\nThe table we have provided in Excel has broken the regression parameter equations (b0, b1) into smaller components so you can understand the underlying mechanisms and where these values come from.\n\nPlot the two variables in Excel and fit a line. You can fit a number of models in Excel simply by right clicking on the scatter of points clicking Add Trendline …. Within the add Tendline window (see screenshot below), a number of options are given, here we want Linear and we want to tick display the Equation and Display r-squared on the chart.\n\n\n\n\n\nScreenshot: Format Trendline\n\n\n\nThe R-squared value is a measure of how well the model fits the data where 1.0 is a perfect fit; we will discuss this more in Week 10. The values which appear in the model equation should be the same as those obtained in your earlier calculations.\n\nAlthough it is important for the model equation, do you think the intercept provides a realistic value in this particular case? What does it mean?\n\n\nCalculate the correlation coefficient using the =CORREL function in Excel. Type =CORREL( and highlight the Year column, and then after a comma highlight the Horses column and close the brackets ).\n\n\nIf the relationship was non-linear would this would be a good statistic to use to describe the relationship between horse and years? Explain your answer.\n\n\n\n\n1.2 Fertiliser data\nThis is our example of numerical fitting of a model.\nFigure 1 shows a plot of yield against fertiliser where a linear model is fitted through the scatterplot of raw observations. Intuitively you would draw this as a line that comes as close to possible to all observations which you may have come across as a ‘line of best fit’. In this exercise we will explore how models can be fitted automatically based on least-squares estimation.\n\n\n\n\nFigure 1: Plot of Yield-response to fertiliser\n\n\n\nIn Figure 1 you will notice that the line does not fit the data perfectly which is typical of biological and environmental data. A measure of how far the model is from the data is the residual.\n\\[\\begin{equation}\nresidual = y_i - \\hat{y}_1\n\\label{1}\n\\end{equation}\\]\nWhere \\(y_i\\) is the observed value for the ith observation and \\(\\hat{y}_1\\) is the predicted value for the ith observation. In this case the predicted value is based on the linear model.\nIf we add up the square of the residuals for the n observations we get something called the Residual Sum of Squares (\\(SS_{res}\\)):\n\\[\\begin{equation}\nSS_{res} =\\sum_{i = 1}^{n} (y - \\hat{y})^2 \\label{2}\n\\end{equation}\\]\nThe best fitting model will have the smallest RSS. The general method is called least-squared estimation. We will now use Excel to find the optimal model.\nEnter values of 2 for the y-intercept (\\(b_0\\)) and 3 for the slope (\\(b_1\\)) in cells H2:H3. These are the initial guess values.\n\nNow use these parameter values to create predictions for each value of fertiliser in the Predicted column.\n\nInstead of typing ‘2’ and ‘3’ directly in your formula, you must use $H$2 and $H$3. The dollar signs lock these cells as reference points, so they won’t shift when you copy the formula. To apply the formula to other rows, either drag the small box at the bottom right of the cell down the column or double-click it.\n\n\n\n\nScreenshot of Predicted column input\n\n\n\n\nUse this information to calculate (i) residuals (ii) residuals2 (iii) RSS.\nCreate a plot similar to Figure 1 where the observations are plotted as symbols and the model predictions are a line. You should have your spreadsheet set up so that if you change the values of the parameters the plotted line changes as well. Try to fit the line manually. This can be difficult, especially for non-linear models.\nFollow instructions provided in the Tutorial, or in the file How to install Solver to ensure you have Solver ready to use in Excel.\n\nOnce you have added Solver, click on the tab Data &gt;&gt; Solver, and you will see the following (see screenshot below). For Set Objective, you need to select the cell where your RSS value has been calculated. We wish to minimize this so we click on Min, and we do this by Changing Cells where the parameters of the model are found, in this case the y-intercept and slope. Before clicking Solve, make sure you can see your calculated values so you can see how your how it all changes.\n\n\n\n\nScreenshot of solver with input values\n\n\n\nWhen ready, click on Solve and it should find a solution for the minimum RSS. Solver uses an iterative procedure to find the minimum RSS which means it successively guesses values until it finds the optimal value. This is a numerical solution to the problem of model fitting.\nYour ‘SOLVED’ parameters should be the same as what appears in your trendline equation.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 -- Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-2-fitting-a-model-in-r",
    "href": "labs/Lab09.html#exercise-2-fitting-a-model-in-r",
    "title": "Lab 9 – Describing relationships",
    "section": "Exercise 2: Fitting a model in R",
    "text": "Exercise 2: Fitting a model in R\nNow we have a deeper understanding of what is going on behind the scenes, we can fit linear models in R.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon’t forget to save as you go!\n\n\n2.1 Have a go - Fertiliser data\nYou will use the fertiliser data to fit a linear model in R. As we covered fitting linear models in the Tutorial, it is now your turn to have a go at fitting the models (with some hints along the way).\n\nRead the following code into R:\n\n\n# add the data to R Studio\nfert &lt;- c(0, 1, 2, 3, 4, 5)\nyield &lt;- c(2, 13, 19, 18, 25, 33)\n\n\nTo visually identify any trends or relationships, create a scatterplot of fertiliser vs yield. From the scatterplot you see, are there any relationships or trends evident?\n\n\n# Create a scatterplot\nplot(fert, yield)\n\n\n\n\n\n\n\n\n\nTo numerically determine whether there is a relationship, calculate the correlation coefficient. (Assume data is normally distributed). Does the correlation coefficient indicate a relationship between fertiliser and yield?\n\n\n# To calculate the correlation coefficient:\ncor(fert, yield)\n\n[1] 0.9636686\n\n\n\nYou can now fit the model in R using the lm() function. Remember to tell R the name of the object you want to store it as (in this case, model.lm &lt;-), then state the name of the function. The arguments within the function (i.e. between the brackets) will be yield ~ fert, with yield being the response variable and fert being the predictor.\n\n\n# Run your model\n## yield = response variable (x)\n## fert = predictor variable (y)\nmodel.lm &lt;- lm(yield ~ fert)\n\n# Obtain model summary - In here you can obtain the model parameters\n# Look for Intercept Estimate and fert Estimate\nsummary(model.lm)\n\n\nCall:\nlm(formula = yield ~ fert)\n\nResiduals:\n     1      2      3      4      5      6 \n-2.762  2.810  3.381 -3.048 -1.476  1.095 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   4.7619     2.2778   2.091  0.10476   \nfert          5.4286     0.7523   7.216  0.00196 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.147 on 4 degrees of freedom\nMultiple R-squared:  0.9287,    Adjusted R-squared:  0.9108 \nF-statistic: 52.07 on 1 and 4 DF,  p-value: 0.001956\n\n\n\nIn the model output obtained from summary(model.lm) the model parameters will be listed under ‘Estimate’ for the intercept and ‘fert’. Compare these values to what you have calculated in Excel.\n\n\nBased on this output, what would the model equation be? Does it match your findings in Excel?\n\n\nYou can now fit your model to the scatterplot you created previously using the abline() function. Make sure you run the plot function and the abline function in one go. If the lines are run separately, an error may appear saying “plot.new hasn’t been called yet”; this is because the abline function requires a current plot on which it can overlay the line.\n\nAlso remember, when presenting plots (e.g. in a report), they should be able to stand alone and be self-explanatory. We therefore need to make sure there are clear axis labels. This can be done using ‘xlab’ and ‘ylab’ arguments.\n\n# Add the linear model to your scatterplot\nplot(fert, yield, xlab = \"Fertiliser Applied\", ylab = \"Yield\")\nabline(model.lm, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n2.2 ABARES data\nIn this final example we will be using a dataset obtained from the Australian Bureau of Agricultural and Resource Economics and Sciences (ABARES). The dataset provides a measure of productivity growth (TFP; Total Factor Productivity) in the Australian dairy industry from the years 1978 to 2018.\nMore information about the ABARES dataset and productivity can be found here.\n\nRead in the data from the Excel file for today’s practical.\n\nBecause we have such a large dataset this time, it is better to read the data straight from Excel than read in each individual value. Reading straight from the source file in Excel saves time and reduces chance of input error.\n\nlibrary(readxl)\n\nABARES &lt;- read_excel(\"data/ENVX1002_practical_data_Regression.xlsx\", sheet = \"ABARES\")\n\n\nCreate a scatterplot of Year against TFP. Dont forget the format will be different now - instead of only mentioning the object name, e.g. plot(yield, fert), you will need to refer to the specific columns within the ABARES dataset. (i.e. ABARES$Year).\n\n\nCan you see a trend between TFP and Year? Or are the points evenly scattered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary. Year will be our predictor and TFP will be our response variable. What are the model parameters (i.e. \\(b_0\\) and \\(b_1\\))?\n\n\nWhat would the equation for this model be?\n\n\nOverlay your model onto the scatterplot you produced earlier. When plotting make sure you refer to the column names as you did for the model (e.g. ABARES$Year).\n\n\nThat’s it! Great work today. Next week: interpreting linear models!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 -- Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-1-cars-stopping-distance",
    "href": "labs/Lab09.html#exercise-1-cars-stopping-distance",
    "title": "Lab 9 – Describing relationships",
    "section": "Exercise 1: Cars stopping distance",
    "text": "Exercise 1: Cars stopping distance\nFor this exercise we will use the inbuilt dataset cars to see if there is a relationship between a car’s speed (mph) and dist )stopping distance, fft).\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n\n\n\nCreate a scatterplot of speed vs dist.\n\n\nIs there are trend? Or are the point evenly scattered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary.\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 -- Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-2-penguins",
    "href": "labs/Lab09.html#exercise-2-penguins",
    "title": "Lab 9 – Describing relationships",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nFor this exercise, we will be using the palmer penguin data set to see if there is a relationship between bill and flipper length (bill_length_mm, flipper_length_mm).\n\n# Load libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# Clean data\npenguins &lt;- penguins %&gt;%\n  na.omit() # remove missing data\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           36.7          19.3               193        3450\n5 Adelie  Torgersen           39.3          20.6               190        3650\n6 Adelie  Torgersen           38.9          17.8               181        3625\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nCreate a scatterplot of bill_length_mm vs flipper_length_mm.\n\n\nIs there are trend? Or are the point evenly scattered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary. What are the parameters?\n\n\nWhat would the equation of the linear model be?\n\n\nOverlay your model onto the scatterplot you produced earlier.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 -- Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-3-old-faithful-geyser-data",
    "href": "labs/Lab09.html#exercise-3-old-faithful-geyser-data",
    "title": "Lab 9 – Describing relationships",
    "section": "Exercise 3: Old Faithful Geyser Data",
    "text": "Exercise 3: Old Faithful Geyser Data\nFor this exercise, we will be looking at the relationship between geyser eruption time (eruptions) and the time between eruptions (waiting), using the inbuilt data set faithful.\n\nhead(faithful)\n\n  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55\n\n\n\nCreate a scatterplot of eruptions vs waiting (time).\n\n\nIs there are trend? Or are the point evenly scattered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary.\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 -- Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab07.html",
    "href": "labs/Lab07.html",
    "title": "Lab 07 – Chi-squared test",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nLearn to use R to calculate a chi-squared test for:\n\nTest of proportions\nTest of independence\n\nLearn how to interpret statistical output.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#before-we-begin",
    "href": "labs/Lab07.html#before-we-begin",
    "title": "Lab 07 – Chi-squared test",
    "section": "Before we begin",
    "text": "Before we begin\nCreate your Quarto document and save it as Lab-07.qmd or similar. There are no data files to download for this lab.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#quick-introduction",
    "href": "labs/Lab07.html#quick-introduction",
    "title": "Lab 07 – Chi-squared test",
    "section": "Quick introduction",
    "text": "Quick introduction\nThe chi-square test is used to compare the observed distribution to an expected distribution, in a situation where we have two or more categories in a discrete data. In other words, it compares multiple observed proportions to expected probabilities.\nThe formula is:\n\\[\\chi^2 = \\sum_{i=1}^{k}\\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) is the observed frequency, \\(E_i\\) is the expected frequency for each categorym and \\(k\\) is the number of categories.\nFor more information about the technique, consult your lecture slides and tutorial 7.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-1-wild-tulips-walk-through",
    "href": "labs/Lab07.html#exercise-1-wild-tulips-walk-through",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 1: wild tulips (walk-through)",
    "text": "Exercise 1: wild tulips (walk-through)\n\nBackground\nSuppose we collected wild tulips and found that 81 were red, 50 were yellow and 27 were white. Are these colours equally common?\nIf these colours were equally distributed, the expected proportion would be 1/3 for each of the colour. Therefore, we want to test if the observed proportions are significantly different from the expected proportions.\nThe data is below.\n\ntulip &lt;- c(81, 50, 27)\n\n\n\nInstructions\nUtilise the HATPC process and test the hypothesis that the proportion of flower colours of tulips are equally common, assuming that the samples are independent. We can explore the data as we check the assumptions of the test.\n\n\nHATPC:\n\nHypothesis\nAssumptions\nTest (statistic)\nP-value\nConclusion\n\n\n\n\n\n\n\nLevel of significance\n\n\n\nThe level of significance is usually set at 0.05. This value is generally accepted in the scientific community and is also linked to Type 2 errors, where choosing a lower significance increases the likelihood of failing to reject the null hypothesis when it is false.\n\n\n\n\nHypotheses\nWhat are the null hypothesis and alternative hypotheses?\n\n\nClick here to view answer\n\n\nH0: There is no significant difference between the observed and the expected proportions of flower colours.\n\nH1: There is a significant difference between the observed and the expected proportions of flower colours.\n\n\n\n\n\nAssumptions\nRecall that the assumptions of the \\(\\chi^2\\) test are:\n\nNo cell has expected frequencies less than 1\nNo more than 20% of cells have expected frequencies less than 5\n\n\n\n\n\n\n\nNote\n\n\n\nIn the case that the above assumptions are violated then the probability of a type 1 error occurring (rejecting the null hypothesis when it is true, i.e. false positive) increases.\n\n\nTo calculate expected frequencies, we first calculate the total number of tulips and then divide by the number of categories.\n\nexpected &lt;- rep(sum(tulip) * 1 / 3, 3) #rep function replicated the value we're calculating inside the brackets\nexpected\n\nDoes the data satisfy the assumptions of a \\(\\chi^2\\) test?\n\n\nClick here to view answer\n\nYes, as expected frequencies &gt; 5.\n\n\n\nTest statistic\nThe chisq.test() function in R is used to calculate the chi-squared test.\n\nres &lt;- chisq.test(tulip, p = c(1 / 3, 1 / 3, 1 / 3))\nres\n\nNote that we coud check our assumptions post-analysis by checking the expected frequencies stored in the expected object of the output:\n\nres$expected\n\n\n\nP-value\nWrite down how you should report the critical value, p-value and df in a scientific paper?\n\n\nClick here to view answer\n\n\\(\\chi^2 = 27.9, d.f. =2, p &lt; 0.01\\)\n\n\n\nConclusions\nBased on the p-value, do we accept or reject the null hypothesis?\n\n\nClick here to view answer\n\nWe reject the null hypothesis as the p-value is less than 0.05.\n\nNow write a scientific (biological) conclusion based on the outcome.\n\n\nClick here to view answer\n\nThere is a significant difference in the proportion of flower colours of tulips (\\(\\chi^2 = 27.9, d.f. =2, p &lt; 0.01\\)).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-2-hermit-crabs",
    "href": "labs/Lab07.html#exercise-2-hermit-crabs",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 2: hermit crabs",
    "text": "Exercise 2: hermit crabs\n\nBackground\nIn a study of hermit crab behaviour at Point Lookout, North Stradbroke Island, a random sample of 3 types of gastropod shells was collected. Each shell was then scored as being either occupied by a hermit crab or empty. Do hermit crabs prefer a certain shell?\n\n\n\nShell species\nOccupied\nEmpty\n\n\n\n\nAustrochochlea\n47\n42\n\n\nBembicium\n10\n41\n\n\nCirithid\n125\n49\n\n\n\nThe data is stored in a table object in R below. Note that it is different from a data.frame object. You can verify this by using the str() or class() functions.\n\ncrabs &lt;- as.table( #Make a table with values for each row\n  rbind(\n    Aus = c(47, 42),\n    Bem = c(10, 41),\n    Cir = c(125, 49)\n  )\n)\n\ncolnames(crabs) &lt;- c(\"Occupied\", \"Empty\") #Add column names to table\nstr(crabs)\ncrabs\n\n\nData exploration\nSince we have a multi-dimensional dataset, we can try to plot the data to visualise it.\nA mosaic plot is a graphical representation of the data in a two-way contingency table. It is a way of visualising the relationship between two categorical variables.\nTry the code below. Can you interpret the plot?\n\n# mosaic plot of crabs\nmosaicplot(crabs, main = \"Hermit crabs and shell species\")\n\n\n\nClick here to view interpretation\n\nThe plot shows that the distribution of hermit crabs in the different shell species is not equal. The majority of hermit crabs are found in the Cirithid shell species, followed by Austrochochlea and Bembicium. This can be observed by the width of the boxes in the plot.\nThere are also differences in the number of empty shells in the different shell species. The Bembicium shell species has the highest number of empty shells, followed by Austrochochlea and Cirithid. This can be observed by the height of the boxes in the plot.\n\n\n\n\nHATPC\nNow it is your turn to test the hypothesis that the three shell species are equally preferred by hermit crabs. Follow the HATPC process with the following questions in mind (but you don’t have to answer them individually):\n\nWhat are the null hypothesis and alternative hypotheses?\nDoes the data satisfy the assumptions of a \\(\\chi^2\\) test?\nHow should you report the critical value, p-value and df in a scientific paper?\nBased on the p-value, do we accept or reject the null hypothesis?\nWrite a scientific (biological) conclusion based on the outcome.\n\nUse the markdown structure below to answer the questions (if you wish):\n## Exercise 2: hermit crabs\n### Hypothesis\n### Assumptions\n### Test statistic\n### P-value\n### Conclusions\nTake your time, and when you are ready, check your answers with your demonstrators.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#done",
    "href": "labs/Lab07.html#done",
    "title": "Lab 07 – Chi-squared test",
    "section": "Done!",
    "text": "Done!\nThis is the end of the lab. Remember to save your work, render your document and ask your demonstrators for feedback if you are unsure about your answers.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-1-national-rspca-statistics",
    "href": "labs/Lab07.html#exercise-1-national-rspca-statistics",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 1: National RSPCA statistics",
    "text": "Exercise 1: National RSPCA statistics\nThe RSPCA releases statistics on the number of animals they receive, reclaim and rehome every year. In the 2023-24 financial year, the RSPCA received 17468 dogs, 26704 cats, and 37497 other animals. The “other” category includes horses, small animals, livestock and wildlife.\nUsing the HATPC framework, test whether these animals were received in equal proportions.\n\nreceived &lt;- c(17468, 26704, 37497)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-2-uc-berkeley-admissions",
    "href": "labs/Lab07.html#exercise-2-uc-berkeley-admissions",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 2: UC Berkeley Admissions",
    "text": "Exercise 2: UC Berkeley Admissions\nFor the two exercises we’re going to use simplified versions of the inbuilt dataset ‘UCBAdmissions’, which has data on student admissions for Berkeley. The dataset shows how many students were rejected and admitted to the university by both department and gender.\n\n2.1 Admissions by department\nDid every department at UC Berkeley admit students in equal proportions?\n\ndept_admissions &lt;- c(601, 370, 322, 269, 147, 46)\n\n\n\n2.2\nAre male and female students admitted and rejected in the same proportion?\n\ngender &lt;- as.table( #Make a table with values for each row\n  rbind(\n    admitted = c(1198, 557),\n    rejected = c(1493, 1278)\n  )\n)\n\ncolnames(gender) &lt;- c(\"Male\", \"Female\") #Add column names to table",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab05.html",
    "href": "labs/Lab05.html",
    "title": "Lab 05 – Hypothesis testing",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 1-sample t-test\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#welcome",
    "href": "labs/Lab05.html#welcome",
    "title": "Lab 05 – Hypothesis testing",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 1-sample t-test\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#create-a-new-project",
    "href": "labs/Lab05.html#create-a-new-project",
    "title": "Lab 05 – Hypothesis testing",
    "section": "Create a new project",
    "text": "Create a new project\nReminder (skip to step 2 if you are going to use the directory you created in your tutorial)\nStep 1: Create a new project file for the practical put in your ENVX1002 Folder. File &gt; New Project &gt; New Directory &gt; New Project.\nStep 2: Download the data files from canvas or using above link and copy into your project directory.\nI recommend that you make a data folder in your project directory to keep things tidy! If you make a data folder in your project directory you will need to indicate this path before the file name.\nStep 3: Open a new Quarto file.\ni.e. File &gt; New File &gt; Quarto Document and save it immediately i.e. File &gt; Save.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#problems-with-your-personal-computer-and-r",
    "href": "labs/Lab05.html#problems-with-your-personal-computer-and-r",
    "title": "Lab 05 – Hypothesis testing",
    "section": "Problems with your personal computer and R",
    "text": "Problems with your personal computer and R\nNOTE: If you are having problems with R on your personal computer that cannot easily be solved by a demonstrator, please use the Lab PCs.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#installing-packages",
    "href": "labs/Lab05.html#installing-packages",
    "title": "Lab 05 – Hypothesis testing",
    "section": "Installing packages",
    "text": "Installing packages\nRemember All of the functions and data sets in R are organised into packages. There are the standard (or base) packages which are part of the source code - the functions and data sets that make up these packages are automatically available when R is opened. There are also many contributed packages. These have been written by many different authors, often to implement methods that are not available in the base packages. If you are unable to find a method in the base packages, you might be able to find it in a contributed package. The Comprehensive R Archive Network (CRAN) site (http://cran.r-project.org/) is where many contributed packages can be downloaded. Click on packages on the left hand side. We will download two packages in this class using the install.packages command and we then load the package into R using the library command.\nAlternatively, in RStudio click on the Packages tab &gt; Install &gt; type in package name &gt; click install.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#normally-you-choose-0.05-as-a-level-of-significance",
    "href": "labs/Lab05.html#normally-you-choose-0.05-as-a-level-of-significance",
    "title": "Lab 05 – Hypothesis testing",
    "section": "1. Normally you choose 0.05 as a level of significance:",
    "text": "1. Normally you choose 0.05 as a level of significance:\nThis value is generally accepted in the scientific community and is also linked to type 2 errors where choosing a lower significance increases the likelihood of a type 2 error occurring.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#write-null-and-alternative-hypotheses",
    "href": "labs/Lab05.html#write-null-and-alternative-hypotheses",
    "title": "Lab 05 – Hypothesis testing",
    "section": "2. Write null and alternative hypotheses:",
    "text": "2. Write null and alternative hypotheses:\n\n\nQuestion: Write down the null hypothesis and alternative hypotheses:\nH0: &lt; Type your answer here &gt;\nH1: &lt; Type your answer here &gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#check-assumptions-normality",
    "href": "labs/Lab05.html#check-assumptions-normality",
    "title": "Lab 05 – Hypothesis testing",
    "section": "3. Check assumptions (normality):",
    "text": "3. Check assumptions (normality):\n\na. load data:\nMake sure you set your working directory first\n\n# Type your R code here\n\nIt is always good practice to look at the data first to make sure you have the correct data, it loaded in correctly and know what the names of the columns are. This can be done by typing the name of the data Milk or for large datasets, use str() to show the first 6 lines:\n\n# Type your R code here\n\n\n\nb. Tests for normality:\nqqplots:\n\n# Type your R code here\n\nHistogram and boxplots:\n\n# Type your R code here\n\n\n\nQuestion: Do the plots indicate the data are normally distributed?\nAnswer: &lt; Type your answer here &gt;\n\n\nShapiro-Wilk test of normality:\n\n# Type your R code here\n\n\n\nQuestion: Does the Shapiro-Wilk test indicate the data are normally distributed? Explain your answer.\nAnswer: &lt; Type your answer here &gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#calculate-the-test-statistic",
    "href": "labs/Lab05.html#calculate-the-test-statistic",
    "title": "Lab 05 – Hypothesis testing",
    "section": "4. Calculate the test statistic",
    "text": "4. Calculate the test statistic\nIn R we achieve this via the command t.test(milk$Yield, mu = …) The R output first gives us the calculated t value, the degrees of freedom, and then the p-value, it then provides the 95% CI and the mean of the sample. Were mu = … is written enter in the hypothesised mean.\n\n# write your R code here\n\n\n5. Obtain P-value or critical value\n\n\nQuestion: Does the hypothesised economic threshold lie within the confidence intervals?\nAnswer: &lt; Type your answer here &gt;\n\n\n\n\n6. Make statistical conclusion\n\n\nQuestion:: Based on the P-value, do we accept or reject the null hypothesis?\nAnswer: &lt; Type your answer here &gt;\n\n\n\n\n7. Write a scientific (biological) conclusion\n\n\nQuestion:: Now write a scientific (biological) conclusion based on the outcome in 6.\nAnswer: &lt; Type your answer here &gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#thanks",
    "href": "labs/Lab05.html#thanks",
    "title": "Lab 05 – Hypothesis testing",
    "section": "Thanks!",
    "text": "Thanks!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#exercise-1-carrots",
    "href": "labs/Lab05.html#exercise-1-carrots",
    "title": "Lab 05 – Hypothesis testing",
    "section": "Exercise 1: Carrots",
    "text": "Exercise 1: Carrots\nA farmer is growing carrots for a restaurant. The restaraunt wants their carrots to be 10 cm long, so the farmer wants to check if the carrots in their field differ significantly from the needed length.\n\n#Read in data\n\ncarrots &lt;- c(7, 7, 13, 5, 13, 10, 11, 12, 10,  9)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#exercise-2-penguins",
    "href": "labs/Lab05.html#exercise-2-penguins",
    "title": "Lab 05 – Hypothesis testing",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nRey has just landed on earth and notived that penguins look really similar to porgs. Using weight as the point of comparison, she wants to know if two different penguin species weigh the same as her pet Porg Stevie, who weighs 4000g.\nWe will be using the Palmer penguin dataset to test if chinstrap and gentoo penguins weigh the same as Stevie.\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n\n2.1 Chinstrap\n\nchinstrap &lt;-  penguins%&gt;%\n  filter(species == \"Chinstrap\")%&gt;%\n  na.omit()\n\n\n\n2.2 Gentoo\n\ngentoo &lt;-penguins%&gt;%\n  filter(species == \"Gentoo\")%&gt;%\n  na.omit() \n\n\n\nAttribution\nThis lab was developed using resources that are available under a Creative Commons Attribution 4.0 International license, made available on the SOLES Open Educational Resources repository.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis testing"
    ]
  },
  {
    "objectID": "labs/Lab03.html",
    "href": "labs/Lab03.html",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "",
    "text": "Learning outcomes\n\n\n\nAt the end of this computer practical, students should be able to:\n\nImport and prepare data for visualisation\nCreate basic plot types using ggplot2 (histograms, boxplots, bar plots, scatterplots)\nCustomise plots with appropriate labels, colours, and themes\nIdentify and visualise distribution properties (normality, skewness, kurtosis)\nInterpret visualisations to draw meaningful conclusions\nExport and save plots for reports",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#before-we-begin",
    "href": "labs/Lab03.html#before-we-begin",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Before we begin",
    "text": "Before we begin\nFor this lab, you will need to:\n\nCreate a new Quarto document in your project folder (or create a new RStudio project) to simplify file management and reproducibility\nDownload the data files from the links provided in Exercise 1\nMake sure you have the following packages installed and loaded:\n\ntidyverse: For data manipulation and visualization (includes ggplot2)\nmoments: For calculating skewness and kurtosis\npatchwork: For combining multiple plots\n\n\nYou can install packages using install.packages(\"package_name\") if needed, and load them with library(package_name).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-1-dataset-exploration",
    "href": "labs/Lab03.html#exercise-1-dataset-exploration",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Exercise 1: Dataset exploration",
    "text": "Exercise 1: Dataset exploration\n\nExploring the pie_crab dataset\nIn this lab, we’ll work with two environmental datasets. They can be downloaded from the links below:\n\npie_crab.csv: Crab size measurements across different sites and latitudes\nhbr_maples.csv: Maple seedling measurements from different watersheds\n\nBy now, you should know how to use read_csv() to read these datasets into R. If you need a refresher, refer to the previous labs or ask for help from a demonstrator (if available).\nLet’s start by exploring the pie_crab dataset. We’ve done this before, but it’s always good to refresh our memory as these functions are extremely common to use.\n\nThe str() function shows us the structure of the dataset, including variable names, types, and a preview of the values.\nThe head() function shows the first six rows of the dataset, giving us a quick look at the data.\nThe summary() function provides descriptive statistics for each variable, including minimum, maximum, mean, and quartiles for numeric variables.\n\n\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : chr [1:392] \"GTM\" \"GTM\" \"GTM\" \"GTM\" ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : chr [1:392] \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" \"Guana Tolomoto Matanzas NERR\" ...\n\nhead(pie_crab)\n\n# A tibble: 6 × 9\n  date       latitude site   size air_temp air_temp_sd water_temp water_temp_sd\n  &lt;date&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 2016-07-24       30 GTM    12.4     21.8        6.39       24.5          6.12\n2 2016-07-24       30 GTM    14.2     21.8        6.39       24.5          6.12\n3 2016-07-24       30 GTM    14.5     21.8        6.39       24.5          6.12\n4 2016-07-24       30 GTM    12.9     21.8        6.39       24.5          6.12\n5 2016-07-24       30 GTM    12.4     21.8        6.39       24.5          6.12\n6 2016-07-24       30 GTM    13.0     21.8        6.39       24.5          6.12\n# ℹ 1 more variable: name &lt;chr&gt;\n\nsummary(pie_crab)\n\n      date               latitude         site                size      \n Min.   :2016-07-24   Min.   :30.00   Length:392         Min.   : 6.64  \n 1st Qu.:2016-07-28   1st Qu.:34.00   Class :character   1st Qu.:12.02  \n Median :2016-08-01   Median :39.10   Mode  :character   Median :14.44  \n Mean   :2016-08-02   Mean   :37.69                      Mean   :14.66  \n 3rd Qu.:2016-08-09   3rd Qu.:41.60                      3rd Qu.:17.34  \n Max.   :2016-08-13   Max.   :42.70                      Max.   :23.43  \n    air_temp      air_temp_sd      water_temp    water_temp_sd  \n Min.   :10.29   Min.   :6.391   Min.   :13.98   Min.   :4.838  \n 1st Qu.:12.05   1st Qu.:8.110   1st Qu.:14.33   1st Qu.:6.567  \n Median :13.93   Median :8.410   Median :17.50   Median :6.998  \n Mean   :15.20   Mean   :8.654   Mean   :17.65   Mean   :7.252  \n 3rd Qu.:18.63   3rd Qu.:9.483   3rd Qu.:20.54   3rd Qu.:7.865  \n Max.   :21.79   Max.   :9.965   Max.   :24.50   Max.   :9.121  \n     name          \n Length:392        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nWhat variables are in the dataset? What are the data types of each variable? Are there any missing values? What are the ranges of the numeric variables?\n\n\nFrom our exploration, we can see that the pie_crab dataset contains information about:\n\ndate: When the crabs were measured\nlatitude: The latitude where the crabs were collected\nsite: The specific collection site\nsize: The size of the crabs in millimeters\nair_temp: Air temperature in degrees Celsius\nwater_temp: Water temperature in degrees Celsius\n\n\n\nIdentifying factors in the data\nOne of the first steps in data exploration is to determine if your data types are recognised correctly by R. Looking at the output of str() can help you identify variables that might be better represented as different data types.\nIn our dataset, site and name are character variables with repeating values. When categorical variables like these have a limited number of possible values that repeat throughout the dataset, they’re good candidates to be converted to factors. Factors are R’s way of representing categorical data efficiently.\nWe can check how many unique values these variables have using the unique() function:\n\nunique(pie_crab$site)\n\n [1] \"GTM\" \"SI\"  \"NIB\" \"ZI\"  \"RC\"  \"VCR\" \"DB\"  \"JC\"  \"CT\"  \"NB\"  \"CC\"  \"BC\" \n[13] \"PIE\"\n\nunique(pie_crab$name)\n\n [1] \"Guana Tolomoto Matanzas NERR\"     \"Sapelo Island NERR\"              \n [3] \"North Inlet Winyah Bay NERR\"      \"Zeke's Island NERR\"              \n [5] \"Rachel Carson NERR\"               \"Virginia Coastal Reserve LTER\"   \n [7] \"Delaware Bay NERR\"                \"Jacques Cousteau NERR\"           \n [9] \"Sixpenny Island - Connecticut\"    \"Narragansett Bay NERR\"           \n[11] \"Cape Cod\"                         \"Bare Cove Park\"                  \n[13] \"Plum Island Estuary - West Creek\"\n\n\nFor a dataset with 392 observations, having only a few unique sites and names suggests that these variables should be factors. Let’s count exactly how many unique values we have:\n\nlength(unique(pie_crab$site))\n\n[1] 13\n\nlength(unique(pie_crab$name))\n\n[1] 13\n\n\nWe can convert these character variables to factors using the factor() or as.factor() functions:\n\n# Convert site and name to factors\npie_crab$site &lt;- factor(pie_crab$site)\npie_crab$name &lt;- factor(pie_crab$name)\n\n# Alternatively, use the as.factor() function:\n# pie_crab$site &lt;- as.factor(pie_crab$site)\n# pie_crab$name &lt;- as.factor(pie_crab$name)\n# We have intentionally commented out (#) the code to avoid running it\n\n# Check the structure again to confirm the conversion\nstr(pie_crab)\n\ntibble [392 × 9] (S3: tbl_df/tbl/data.frame)\n $ date         : Date[1:392], format: \"2016-07-24\" \"2016-07-24\" ...\n $ latitude     : num [1:392] 30 30 30 30 30 30 30 30 30 30 ...\n $ site         : Factor w/ 13 levels \"BC\",\"CC\",\"CT\",..: 5 5 5 5 5 5 5 5 5 5 ...\n $ size         : num [1:392] 12.4 14.2 14.5 12.9 12.4 ...\n $ air_temp     : num [1:392] 21.8 21.8 21.8 21.8 21.8 ...\n $ air_temp_sd  : num [1:392] 6.39 6.39 6.39 6.39 6.39 ...\n $ water_temp   : num [1:392] 24.5 24.5 24.5 24.5 24.5 ...\n $ water_temp_sd: num [1:392] 6.12 6.12 6.12 6.12 6.12 ...\n $ name         : Factor w/ 13 levels \"Bare Cove Park\",..: 4 4 4 4 4 4 4 4 4 4 ...\n\n\nNotice how we’ve converted the specific variables to factors and used the assignment operator &lt;- to update the dataset. This is a common pattern in R, where we update the dataset in place. The output of str() now shows these variables as factors with their levels (unique values) listed.\nConverting to factors is important because:\n\nIt makes R treat the data appropriately in statistical analyses\nIt can make visualisations more informative\nIt improves memory efficiency for large datasets",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-2-building-visualizations-with-ggplot2",
    "href": "labs/Lab03.html#exercise-2-building-visualizations-with-ggplot2",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Exercise 2: Building visualizations with ggplot2",
    "text": "Exercise 2: Building visualizations with ggplot2\n\nThe grammar of graphics\nThe ggplot2 package is based on the “grammar of graphics,” a framework that breaks visualisations into components, similar to how grammar breaks language into parts of speech. This approach makes it possible to create complex visualisations by combining simple elements.\nThe key components (or layers) include:\n\nData: The dataset being visualised\nAesthetics: Mappings from data variables to visual properties\nGeometries: The shapes used to represent the data\nFacets: Subplots that show different subsets of the data\nStatistics: Transformations of the data (e.g., counts, means)\nCoordinates: The space in which the data is plotted\nThemes: Visual styling of non-data elements\n\nLet’s learn how to create visualisations using this approach by building a plot step by step, explaining each component along the way. As you follow along, you are improving the code that produces the plot – not re-writing it – at each step.\n\n\nStep 1: The canvas\nEvery ggplot2 visualisation starts with a blank canvas:\n\n# Start with an empty canvas\nggplot()\n\n\n\n\n\n\n\n\nThis creates an empty plotting space. It doesn’t show anything yet because we haven’t specified any data or how to visualise it.\n\n\nStep 2: Adding data\nNext, we tell ggplot2 what data to use. Ideally the data is a data.frame or tibble object (which you will know by using str() previously):\n\n# Add data to the plot\nggplot(data = pie_crab)\n\n\n\n\n\n\n\n\nWe’ve now told ggplot2 to use the pie_crab dataset, but we still cannot see anything because we have not specified which variables to plot – or how to represent them.\n\n\nStep 3: Mapping aesthetics\nAesthetics map variables in your data to visual properties in the plot. They are a function aes() on their own. Think of aesthetics as how you woul define x, y and other dimensions in the plot.\n\n# Map variables to visual properties\nggplot(data = pie_crab, mapping = aes(x = size))\n\n\n\n\n\n\n\n\nHere, we’ve mapped the size variable to the x-axis. This variable is something that exists in the pie_crab data frame. We still cannot see any data points because we haven’t specified how to represent the data (e.g., as points, bars, or lines).\n\n\nStep 4: Adding a Geometry\nGeometries determine how the data is represented visually. They are functions that are prefixed by geom_* so that users know what they are meant to do. In most cases it is clear what type of plot is being create by reading the name of the geometry function(s) in the plot code.\n\n# Add a histogram geometry\nggplot(data = pie_crab, mapping = aes(x = size)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNow we can see the data! We’ve added a histogram geometry (geom_histogram()), which counts the number of observations falling into bins along the x-axis. The + operator adds layers to the plot.\nNotice the message about the default bin width. ggplot2 automatically chose 30 bins, but we can adjust this.\n\n\nStep 5: Customizing the Geometry\nLet’s customize our histogram:\n\n# Customize the histogram\nggplot(data = pie_crab, mapping = aes(x = size)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\")\n\n\n\n\n\n\n\n\nWe’ve made several changes:\n\nbins = 20: Changed the number of bins to 20\nfill = \"skyblue\": Set the fill color of the bars to sky blue\ncolor = \"black\": Set the outline color of the bars to black\n\nThese are fixed properties applied to all bars, not mappings from data variables. For colours you may search “r colours” in your web browser to see what available colours you can use (it’s a lot).\n\n\nStep 6: Adding Labels and Titles\nGood visualisations have clear labels. Titles are optional – but the option is available if you need it. We can add all of these in the next layer using the labs() function:\n\n# Add informative labels\nggplot(data = pie_crab, mapping = aes(x = size)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Distribution of Crab Sizes\",\n    x = \"Size (mm)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\nThe labs() function adds various text elements to the plot:\n\ntitle: The main title of the plot\nx: The x-axis label\ny: The y-axis label\n\n\n\nStep 7: Applying a Theme\nThemes control the overall appearance of the plot:\n\n# Add a theme for consistent styling\nggplot(data = pie_crab, mapping = aes(x = size)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Distribution of Crab Sizes\",\n    x = \"Size (mm)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe theme_minimal() function applies a minimalist theme with a white background and subtle grid lines. Other common themes include:\n\ntheme_classic(): No grid lines, simple axes\ntheme_light(): Light background with subtle grid lines\ntheme_dark(): Dark background for presentations\n\n\n\nBonus: Adding multiple geometries\nOne of the powerful features of ggplot2 is the ability to layer multiple geometries:\n\n# Add a density curve on top of the histogram\nggplot(data = pie_crab, mapping = aes(x = size)) +\n  geom_histogram(aes(y = after_stat(density)),\n    bins = 20,\n    fill = \"skyblue\", color = \"black\"\n  ) +\n  geom_density(color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Distribution of Crab Sizes\",\n    x = \"Size (mm)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this plot:\n\nWe’ve changed the y-axis of the histogram to show density instead of count using aes(y = after_stat(density))\nWe’ve added a density curve with geom_density()\nWe’ve set the density curve color to red and increased its line width\n\n\n\n\n\n\n\nQuestion 2\n\n\n\n\nWhat’s the difference between setting a fixed property (like fill = \"blue\") and mapping a variable to an aesthetic (like aes(fill = site))?\nHow would you modify the histogram to have more or fewer bins? Use ?geom_histogram to help you think of an answer.\nWhat would happen if you changed the order of the geom_histogram() and geom_density() layers?\n\n\n\n\n\nThese questions can typically be answered by making changes to the code to view the differences.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-3-analysing-environmental-variables",
    "href": "labs/Lab03.html#exercise-3-analysing-environmental-variables",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Exercise 3: Analysing environmental variables",
    "text": "Exercise 3: Analysing environmental variables\nNow that we understand the grammar of graphics approach, let’s analyse a different variable in our dataset.\n\nExamining water temperature distribution\nLet’s examine the distribution of water temperatures across our sampling sites:\n\n# Create a basic histogram of water temperatures\nggplot(pie_crab, aes(x = water_temp)) +\n  geom_histogram(bins = 15) +\n  labs(\n    title = \"Distribution of Water Temperatures\",\n    x = \"Water Temperature (°C)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\nThe histogram shows us the frequency distribution of water temperatures. We can see the shape of the distribution, including any skewness or unusual patterns.\n\n# Add a density curve\nggplot(pie_crab, aes(x = water_temp)) +\n  geom_histogram(aes(y = after_stat(density)),\n    bins = 15,\n    fill = \"lightblue\", colour = \"black\"\n  ) +\n  geom_density(colour = \"red\") +\n  labs(\n    title = \"Distribution of Water Temperatures\",\n    x = \"Water Temperature (°C)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\nAdding a density curve helps us see the overall shape of the distribution more clearly.\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat is the shape of the distribution of water temperatures? Does the distribution appear to be normal? Are there any outliers?\n\n\n\n\nSkewness and Kurtosis\nTo quantify the shape of the water temperature distribution, we can calculate skewness and kurtosis:\n\n# Calculate skewness and kurtosis for water temperature\nskewness_value &lt;- skewness(pie_crab$water_temp)\nkurtosis_value &lt;- kurtosis(pie_crab$water_temp)\nskewness_value\n\n[1] 0.4750277\n\nkurtosis_value\n\n[1] 1.888369\n\n\nInterpreting these values:\n\nSkewness measures the asymmetry of the distribution:\n\n0 = symmetric (like a normal distribution)\nMore than, &gt; 0 = right-skewed (tail extends to the right)\nLess than, &lt; 0 = left-skewed (tail extends to the left)\n\nKurtosis measures the “tailedness” of the distribution:\n\n3 = normal distribution (in the moments package, this is sometimes normalised to 0)\nMore than, &gt; 3 = leptokurtic (heavy-tailed, more outliers)\nLess than, &lt; 3 = platykurtic (light-tailed, fewer outliers)\n\n\nThe skewness value of approximately 0.5 confirms our visual observation that the water temperature distribution is moderately right-skewed. The kurtosis value of approximately 2.5 indicates the distribution has slightly lighter tails than a normal distribution.\nThese numerical measures help us quantify what we observe visually in the histograms and density plots. Now that we understand the overall distribution of our data, let’s explore how it varies across different groups.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-4-comparing-groups",
    "href": "labs/Lab03.html#exercise-4-comparing-groups",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Exercise 4: Comparing groups",
    "text": "Exercise 4: Comparing groups\nNow that we’ve examined the overall distribution of crab sizes, let’s compare sizes across different groups.\n\nCreating boxplots to compare sites\nBoxplots are excellent for comparing distributions between two or more groups:\n\n# Create boxplots of crab sizes by site\nggplot(pie_crab, aes(x = site, y = size)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"Crab Sizes by Site\",\n    x = \"Site\",\n    y = \"Size (mm)\"\n  )\n\n\n\n\n\n\n\n\nA boxplot shows:\n\nThe median (middle line)\nThe interquartile range (IQR) from the 25th to 75th percentile (the box)\nThe whiskers (typically extend to 1.5 × IQR)\nOutliers (points beyond the whiskers)\n\nTo see the actual data points alongside the boxplots we can add another geometric layer. You can see how it may or may not be useful to readers. In most cases adding the data points is not necessary, but in some cases it could be useful in the exploration phase (i.e. not the final plot for publication).\n\n# Add points to see the actual data\nggplot(pie_crab, aes(x = site, y = size)) +\n  geom_boxplot(fill = \"skyblue\", alpha = 0.5) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Crab Sizes by Site\",\n    x = \"Site\",\n    y = \"Size (mm)\"\n  )\n\n\n\n\n\n\n\n\nWe’ve added:\n\ngeom_jitter() to add individual data points with a slight horizontal jitter to avoid overplotting\nalpha = 0.5 to make both the boxplots and points semi-transparent\nwidth = 0.2 to control the amount of horizontal jittering\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nHow do crab sizes vary across different sites? Which site has the largest median crab size? Which site shows the most variability in crab sizes? Are there any outliers at specific sites? Answer these questions in a descriptive manner using the plot.\n\n\n\n\nHere we are exercising our ability to see patterns from data visualisations and using them to make certain observations about the data.\n\n\nExploring the relationship between latitude and size\nLet’s examine if there’s a relationship between latitude and crab size:\n\n# Create a scatterplot of size vs. latitude\nggplot(pie_crab, aes(x = latitude, y = size)) +\n  geom_point(alpha = 0.5) +\n  labs(\n    title = \"Crab Size vs. Latitude\",\n    x = \"Latitude\",\n    y = \"Size (mm)\"\n  )\n\n\n\n\n\n\n\n\nScatterplots show the relationship between two continuous variables. Each point represents a single observation.\nTo help visualise the trend, we can add a trend line (something that we will cover in greater detail once we look at linear models in Week 7):\n\n# Add a trend line\nggplot(pie_crab, aes(x = latitude, y = size)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Crab Size vs. Latitude\",\n    x = \"Latitude\",\n    y = \"Size (mm)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe’ve added:\n\ngeom_smooth(method = \"lm\") to add a linear regression line\nse = TRUE to include the standard error as a shaded confidence band\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nIs there a relationship between latitude and crab size?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-5-faceting-and-grouping",
    "href": "labs/Lab03.html#exercise-5-faceting-and-grouping",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Exercise 5: Faceting and grouping",
    "text": "Exercise 5: Faceting and grouping\nSo far, we’ve created separate plots for different analyses. Now, let’s explore techniques for comparing multiple groups or variables within a single plot.\n\nExploring the hbr_maples dataset\nLet’s switch to our second dataset, which contains measurements of maple seedlings from different watersheds:\n\n# Examine the structure of the maples dataset\nstr(hbr_maples)\n\ntibble [359 × 11] (S3: tbl_df/tbl/data.frame)\n $ year               : num [1:359] 2003 2003 2003 2003 2003 ...\n $ watershed          : Factor w/ 2 levels \"Reference\",\"W1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ elevation          : Factor w/ 2 levels \"Low\",\"Mid\": 1 1 1 1 1 1 1 1 1 1 ...\n $ transect           : Factor w/ 12 levels \"R1\",\"R2\",\"R3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ sample             : Factor w/ 20 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ stem_length        : num [1:359] 86.9 114 83.5 68.1 72.1 77.7 85.5 81.6 92.9 59.6 ...\n $ leaf1area          : num [1:359] 13.84 14.57 12.45 9.97 6.84 ...\n $ leaf2area          : num [1:359] 12.13 15.27 9.73 10.07 5.48 ...\n $ leaf_dry_mass      : num [1:359] 0.0453 0.0476 0.0423 0.0397 0.0204 0.0317 0.0382 0.0179 0.0286 0.0125 ...\n $ stem_dry_mass      : num [1:359] 0.03 0.0338 0.0248 0.0194 0.018 0.0246 0.0316 0.015 0.0291 0.0149 ...\n $ corrected_leaf_area: num [1:359] 29.1 33 25.3 23.2 15.5 ...\n\n\n\n# View the first few rows\nhead(hbr_maples)\n\n# A tibble: 6 × 11\n   year watershed elevation transect sample stem_length leaf1area leaf2area\n  &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;     &lt;fct&gt;    &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  2003 Reference Low       R1       1             86.9     13.8      12.1 \n2  2003 Reference Low       R1       2            114       14.6      15.3 \n3  2003 Reference Low       R1       3             83.5     12.5       9.73\n4  2003 Reference Low       R1       4             68.1      9.97     10.1 \n5  2003 Reference Low       R1       5             72.1      6.84      5.48\n6  2003 Reference Low       R1       6             77.7      9.66      7.64\n# ℹ 3 more variables: leaf_dry_mass &lt;dbl&gt;, stem_dry_mass &lt;dbl&gt;,\n#   corrected_leaf_area &lt;dbl&gt;\n\n\n\n# Get a summary of the variables\nsummary(hbr_maples)\n\n      year          watershed   elevation     transect       sample   \n Min.   :2003   Reference:179   Low :120   R1     : 40   1      : 18  \n 1st Qu.:2003   W1       :180   Mid :120   R2     : 40   2      : 18  \n Median :2003                   NA's:119   W1-1   : 40   3      : 18  \n Mean   :2003                              W1-2   : 40   4      : 18  \n 3rd Qu.:2004                              W1-3   : 40   5      : 18  \n Max.   :2004                              R3     : 39   6      : 18  \n                                           (Other):120   (Other):251  \n  stem_length       leaf1area        leaf2area     leaf_dry_mass    \n Min.   : 52.70   Min.   : 2.480   Min.   : 3.40   Min.   :0.01170  \n 1st Qu.: 75.65   1st Qu.: 8.818   1st Qu.: 8.95   1st Qu.:0.03975  \n Median : 85.70   Median :11.636   Median :11.28   Median :0.05590  \n Mean   : 86.86   Mean   :11.800   Mean   :11.75   Mean   :0.06368  \n 3rd Qu.: 97.05   3rd Qu.:14.016   3rd Qu.:14.30   3rd Qu.:0.07855  \n Max.   :132.30   Max.   :26.952   Max.   :25.79   Max.   :0.38700  \n                  NA's   :119      NA's   :119                      \n stem_dry_mass     corrected_leaf_area\n Min.   :0.00790   Min.   : 9.597     \n 1st Qu.:0.02360   1st Qu.:21.180     \n Median :0.03320   Median :25.950     \n Mean   :0.04506   Mean   :26.687     \n 3rd Qu.:0.05820   3rd Qu.:31.751     \n Max.   :0.15000   Max.   :55.874     \n                   NA's   :119        \n\n\nNow, let’s create histograms of stem length by watershed using faceting:\n\n# Create histograms of stem length by watershed\nggplot(hbr_maples, aes(x = stem_length)) +\n  geom_histogram(bins = 20, fill = \"forestgreen\", colour = \"black\") +\n  facet_wrap(~watershed) +\n  labs(\n    title = \"Distribution of Stem Lengths by Watershed\",\n    x = \"Stem Length (mm)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\nhbr_maples |&gt;\n  select(watershed, stem_length) |&gt;\n  group_by(watershed) |&gt;\n  summarise(median = median(stem_length, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  watershed median\n  &lt;fct&gt;      &lt;dbl&gt;\n1 Reference   80.6\n2 W1          90  \n\n\nThe facet_wrap() function creates separate panels for each value of the specified variable. This allows us to compare distributions across groups.\n\n\n\n\n\n\nQuestion 6\n\n\n\nHow do stem lengths differ between watersheds? Which watershed shows more variability in stem lengths? Are the distributions similarly shaped? Use only the visualisations to answer these questions.\n\n\n\n\nComparing leaf area and stem length\nLet’s examine the relationship between leaf area and stem length, comparing across watersheds:\n\n# Create a scatterplot of leaf area vs. stem length, coloured by watershed\nggplot(hbr_maples, aes(x = stem_length, y = corrected_leaf_area, colour = watershed)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Leaf Area vs. Stem Length\",\n    x = \"Stem Length (mm)\",\n    y = \"Corrected Leaf Area (cm²)\",\n    colour = \"Watershed\"\n  )\n\nWarning: Removed 119 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nHere, we’ve mapped the watershed variable to the colour aesthetic, which automatically creates a color-coded legend.\nLet’s add separate trend lines for each watershed:\n\n# Add separate trend lines for each watershed\nggplot(hbr_maples, aes(x = stem_length, y = corrected_leaf_area, colour = watershed)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Leaf Area vs. Stem Length\",\n    x = \"Stem Length (mm)\",\n    y = \"Corrected Leaf Area (cm²)\",\n    colour = \"Watershed\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 119 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 119 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWhen we include colour = watershed in the global aesthetics, ggplot2 automatically applies this grouping to all geometries, including geom_smooth(). This creates separate trend lines for each watershed.\n\n\n\n\n\n\nQuestion 7\n\n\n\nIs there a relationship between stem length and leaf area? Does this relationship differ between watersheds? What might explain these differences from an ecological perspective?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-6-bonus-take-home",
    "href": "labs/Lab03.html#exercise-6-bonus-take-home",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Exercise 6: Bonus take-home",
    "text": "Exercise 6: Bonus take-home\nThese exercises are designed for you to practice the visualisation techniques we’ve covered in this lab. You can complete them during the lab if you finish early, or at home for additional practice.\n\nBasic visualisation practice\n\nCreate a histogram of the air_temp variable in the crabs dataset. Calculate and interpret its skewness and kurtosis.\nCreate boxplots comparing the leaf_dry_mass between watersheds in the maples dataset. What do you observe?\nCreate a scatterplot examining the relationship between stem_dry_mass and leaf_dry_mass in the maples dataset, with points coloured by watershed.\n\n\n\nAdvanced challenge: patchwork\nThe patchwork package allows you to combine multiple plots into a single figure. This is useful for creating complex visualisations that tell a story about your data.\n\n# Load the patchwork package\nlibrary(patchwork)\n\nLet’s create a few plots and then combine them:\n\n# Create multiple plots\np1 &lt;- ggplot(pie_crab, aes(x = size)) +\n  geom_histogram(fill = \"skyblue\", colour = \"black\") +\n  labs(title = \"Distribution of Crab Sizes\")\n\np2 &lt;- ggplot(pie_crab, aes(x = latitude, y = size)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Size vs. Latitude\")\n\np3 &lt;- ggplot(pie_crab, aes(x = site, y = size)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(title = \"Size by Site\")\n\n# Combine plots (this is patchwork in action!)\np1 / (p2 + p3)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe patchwork syntax is intuitive:\n\n/ arranges plots vertically (one above the other)\n+ arranges plots horizontally (side by side)\nYou can use parentheses to control the layout\n\nNow, try these exercises:\n\nCreate a combined plot using patchwork that shows:\n\nA histogram of stem lengths\nA scatterplot of stem length vs. leaf area\nBoxplots of stem lengths by watershed\nArrange these plots in a 2x2 grid\n\nCreate a combined plot that tells a story about the relationship between temperature and crab size:\n\nA scatterplot of air temperature vs. crab size\nA scatterplot of water temperature vs. crab size\nA boxplot of crab sizes by site\nArrange the scatterplots side by side and the boxplot below them",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#summary",
    "href": "labs/Lab03.html#summary",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Summary",
    "text": "Summary\nIn this lab, we’ve explored how to create and customize various types of visualisations using ggplot2. We’ve learned:\n\nThe grammar of graphics approach to building visualisations layer by layer\nHow to create and interpret histograms, density plots, boxplots, and scatterplots\nHow to quantify and interpret distribution properties like skewness and kurtosis\nHow to compare groups using boxplots and faceting\nHow to examine relationships between variables using scatterplots and trend lines\nHow to combine multiple plots using the patchwork package\n\nThese skills will be valuable for exploring and presenting data in future labs and assignments.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab03.html#additional-resources",
    "href": "labs/Lab03.html#additional-resources",
    "title": "Lab 03 – Exploring and visualising data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR for Data Science - Data Visualisation chapter\nggplot2 documentation\npatchwork package documentation\nR Graph Gallery - Examples of various visualisations in R\nCookbook for R - Graphs - Recipes for common visualisation tasks",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Exploring and visualising data"
    ]
  },
  {
    "objectID": "labs/Lab01.html",
    "href": "labs/Lab01.html",
    "title": "Lab 01 – Getting started",
    "section": "",
    "text": "Learning Outcomes\n\n\n\nAt the end of this practical students should be able to:\n\nInstall and set up R and RStudio on their computer\nUnderstand the relationship between R and RStudio\nCreate and manage R projects effectively\nWrite and render basic Quarto documents\nImport data from CSV and Excel files\nPerform basic operations in R",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-1-setting-up",
    "href": "labs/Lab01.html#exercise-1-setting-up",
    "title": "Lab 01 – Getting started",
    "section": "Exercise 1: setting up",
    "text": "Exercise 1: setting up\n\nInstalling R and RStudio\nBefore we begin working with data, we need to set up our statistical computing environment. We’ll be using two main pieces of software:\n\nR: The statistical programming language (the engine)\nRStudio: The integrated development environment (IDE) that makes working with R easier (the interface)\n\n\n\nThink of R and RStudio like a car: R is the engine that does all the work, while RStudio is the dashboard and controls that make it easier to drive.\nThis week’s tutorial would have guided you through the installation process, but if you missed it, below are the steps to install R and RStudio on your personal computer.\n\n\n\n\n\n\nImportant\n\n\n\nYou must install R before installing RStudio, as RStudio needs R to function.\n\n\n\nInstalling R\n\nGo to the CRAN (Comprehensive R Archive Network) website\nClick on the link for your operating system\nFollow the installation instructions for your system\n\nFor Windows: Click “base” then download the latest version\nFor Mac: Choose the appropriate .pkg file for your system (Apple Silicon or Intel)\nFor Linux: Follow the instructions for your distribution\n\n\n\n\nInstalling RStudio\n\nVisit the Posit download page\nScroll down to “RStudio Desktop”\nClick the download button for your operating system\nRun the installer and follow the prompts\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re using a University computer, both R and RStudio should already be installed. However, it’s important to install them on your personal computer for working outside of class.\n\n\n\n\n\nCreating your first R project\nAn R project is like a container that keeps all your work organised and tidy. Think of it as a dedicated workspace for your course where everything stays together and works smoothly. Here’s why R projects are especially helpful for beginners:\n\n\nGood project organisation is crucial for reproducible research. It helps you and others find files easily and ensures your code works consistently.\n\nConsistent starting point: Every time you open your project, you’ll be in the right place with all your files readily available\nNo more lost files: Your data, code, and outputs stay together in one organised location\nEasier file paths: You don’t need to worry about complex file locations - R projects make it simple to find and use your files\nCollaboration ready: When sharing your work, everything stays organised and works on other computers\nBetter workflow: As you learn more complex analyses, having an organised project structure will save you time and prevent headaches\n\nLet’s create a project for this course:\n\nOpen RStudio\nClick File → New Project\nChoose “New Directory”\nClick “New Project”\nEnter “ENVX1002” as the directory name\nChoose a location on your computer (preferably in a cloud-synced folder)\nClick “Create Project”\n\nTo keep things simple but organised, let’s create one essential folder:\nENVX1002/\n├── ENVX1002.Rproj  # This file helps RStudio manage your project\n└── data/           # Store your datasets here\nTo create the data folder, any of the following works:\n\nIn RStudio’s Files pane (bottom-right), click “New Folder”, then name it “data”\nIn the console, run dir.create(\"data\") to create the folder\nManually create a folder named “data” in your project directory, using Finder(macOS), File Explorer(Windows), or similar\n\n\n\n\n\n\n\nTip\n\n\n\nAs you progress in the course, you can create more folders to organise your work. But for now, keeping it simple will help you focus on learning R without getting overwhelmed by complex folder structures.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-2-using-quarto",
    "href": "labs/Lab01.html#exercise-2-using-quarto",
    "title": "Lab 01 – Getting started",
    "section": "Exercise 2: using Quarto",
    "text": "Exercise 2: using Quarto\n\nWhat is Quarto?\nQuarto is a modern publishing system that allows you to:\n\n\nQuarto documents combine code, text, and output in one file, making your analysis reproducible and easy to share.\n\nCombine text, code, and output in one document\nCreate professional reports, presentations, and websites\nWork with multiple programming languages (including R)\nGenerate high-quality output in various formats (HTML, PDF, Word)\n\nHere are some examples of documents that have been created using Quarto:\n\nCVs (curriculum vitae) and resumes in PDF\nResearch papers for major academic journals\nAnd more – check the Quarto gallery\n\n\n\nCreating your first Quarto document\n\nIn RStudio, click File → New File → Quarto Document\nFill in the title and author\nClick “Create”\nSave the document (File → Save As…) with a .qmd extension\n\nTo render your document:\n\nClick the “Render” button (blue arrow) in the editor toolbar\nThe HTML output will automatically open in your default browser\n\nIf you are rendering your document for the first time, it will fail. Look out for a yellow box at the top of your source text that prompts you to install the rmarkdown package. Install it (by clicking on the install link), wait for the installation to complete, and then render your document again.\n\nBasic markdown formatting\nQuarto uses markdown for text formatting:\n\n\nMarkdown is a simple way to format text that’s easy to read and write. The syntax is designed to be intuitive. Quarto’s documentation on markdown can be found here.\n\nBold text: **bold**\nItalic text: *italic*\nHeaders: # Level 1, ## Level 2, ### Level 3\nLists: Use - or 1. for bullet or numbered lists\nLinks: [text](URL)\nImages: ![alt text](image.png)\n\n\n\nCode chunks\nCode chunks are where you write and execute R code. They keep your code separate from your text while showing both the code and its output.\nCode chunks in Quarto start with ```{r} and end with ```:\n\n```{r}\n2 + 2\nmean(c(1, 2, 3, 4, 5))\n```\n\n[1] 4\n[1] 3\n\n\nYou can control chunk behavior with options:\n\necho: false - Hide the code but show results\neval: false - Show code but don’t run it\nwarning: false - Hide warning messages\nmessage: false - Hide messages\n\nFor example:\n```{r}\n#| warning: false\n#| message: false\n\n10 / 5\n```\nQuarto does not run code by default. You need to put R code in code chunks to run them. This is a safety feature to prevent accidental code execution, as well as a means to control the output.\n\n\n\nBasic R operations\nNow that we have our environment set up, let’s try some basic R operations. Don’t forget to use code chunks to evaluate the code below.\n\n\nR uses standard mathematical operators. Remember that ^ means “to the power of” and * means multiplication.\n\n# Basic arithmetic\n5 + 5\n\n[1] 10\n\n10 - 3\n\n[1] 7\n\n4 * 2\n\n[1] 8\n\n8 / 2\n\n[1] 4\n\n2^3 # Exponentiation\n\n[1] 8\n\n\nThat’s it for now. We will look more into code chunks next week when we focus on statistical operations.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-3-r-packages",
    "href": "labs/Lab01.html#exercise-3-r-packages",
    "title": "Lab 01 – Getting started",
    "section": "Exercise 3: R packages",
    "text": "Exercise 3: R packages\n\nWhat are R packages?\nR packages are collections of functions, data, and documentation that extend R’s capabilities. Think of them as add-ons or extensions that provide additional functionality beyond what comes with base R. They are essential tools that make R incredibly versatile for different types of analysis.\n\n\nThink of R packages like apps on your phone - they add new features and capabilities to the base system.\n\n\n\n\n\n\nTip\n\n\n\nR comes with several built-in packages (called “base R”), but thousands more are available for specific tasks, from data manipulation to complex statistical analyses.\n\n\n\n\nInstalling packages\nThere are two main ways to install R packages:\n\n\nYou only need to install a package once, but you need to load it every time you start a new R session.\n\nUsing the install.packages() function:\n\n\n# Install a single package\ninstall.packages(\"readr\") # DO NOT PUT THIS IN YOUR QUARTO DOCUMENT\n\n# Install multiple packages at once\ninstall.packages(c(\"readr\", \"readxl\")) # DO NOT PUT THIS IN YOUR QUARTO DOCUMENT\n\n\nUsing the RStudio interface:\n\nTools → Install Packages…\nType the package name\nClick “Install”\n\n\nDo not include install.packages() in your Quarto document. This particular function is meant to be run in the R console. Including it in your document causes numerous issues, plus why would you want to install a package every time you render your document?\n\n\n\nDon’t install packages in your Quarto document!\n\n\n\n\n\nInstall packages in the console only. Do it once, and you don’t need to do it again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou only need to install a package once on your computer. However, you’ll need to load it each time you start a new R session.\n\n\n\n\nLoading packages\nTo use a package in your R session, you need to load it using the library() function:\nBelow, we load the readr and readxl packages because we need to use the read_csv() and read_excel() functions to import data and these functions are part of these packages.\n\n# Load individual packages\nlibrary(readr)\nlibrary(readxl)\n\n\n\nOptional: pacman\nA more efficient way to handle package management is using the pacman package. You can read more about it here. Please do not use pacman unless you have read the documentation and are comfortable with it!\n\n# Load multiple packages at once\npacman::p_load(readr, readxl)\n\n\n\n\n\n\n\nNote\n\n\n\nThe :: operator in R allows you to use a function from a package without loading the entire package. For example, readr::read_csv() uses the read_csv function from the readr package.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#exercise-4-importing-data",
    "href": "labs/Lab01.html#exercise-4-importing-data",
    "title": "Lab 01 – Getting started",
    "section": "Exercise 4: importing data",
    "text": "Exercise 4: importing data\nNow that we understand R packages, let’s use some specific ones for importing data. We’ll use:\n\nreadr for importing CSV files\nreadxl for importing Excel files\n\nNote: CSV files are simple text files that can be opened by many programs. Excel files are more complex but can store multiple sheets and formatting. When Excel opens a CSV file, it treats it like a spreadsheet.\n\nExample data files\nBefore we begin, download these example files that we’ll use throughout this exercise:\n\nStudent Scores (CSV)\nWeather Data (Excel)\n\nSave these files in your project’s data folder before proceeding.\n\n\nUnderstanding file paths\nFinding and using files on your computer is a bit like giving directions to your house. Let’s break down how R finds your files in a way that’s easy to understand:\n\n\nThink of your project folder as your home base. Relative paths are like giving directions from there: “Go to the kitchen, then the fridge.”\n\nWhat are file paths?\nA file path is simply the address of a file on your computer, telling R exactly where to find it. There are two types of file paths:\n\nAbsolute paths are like complete postal addresses:\n\nThey start from the very root of your computer\nThey work anywhere but are specific to YOUR computer\nExamples:\n\nWindows: C:/Users/YourName/Documents/ENVX1002/data/student_scores.csv\nMac/Linux: /Users/YourName/Documents/ENVX1002/data/student_scores.csv\n\n\nRelative paths are like giving directions from where you are now:\n\nThey start from your project folder (where your .Rproj file is)\nMuch shorter and more convenient\nExample: data/student_scores.csv (meaning “look in the data folder, then find student_scores.csv”)\n\n\n\n\nThe working directory concept\nYour working directory is simply the folder R considers as “here” right now:\n\nWhen you open an R project, R automatically sets the working directory to that project’s folder\nAll relative paths are based on this location\nYou can check your current location with getwd() (“get working directory”)\n\n\n# This shows your current \"location\" in the computer\ngetwd()\n\n[1] \"/Users/jhar8696/Documents/ENVX1002-handbook/labs\"\n\n\n\n\nHow to use paths in practice\nIf your project structure looks like this:\nENVX1002/\n├── ENVX1002.Rproj\n├── data/\n│   ├── student_scores.csv\n│   └── weather_data.xlsx\n├── images/\n│   └── quokka.png\n└── reports/\n  └── lab_report.qmd\nThen from any R code in your project:\n\nTo access student_scores.csv: use \"data/student_scores.csv\"\nTo access weather_data.xlsx: use \"data/weather_data.xlsx\"\nTo access quokka.png: use \"images/quokka.png\"\n\nIt is important to repeat that this works because you created a project in RStudio. If you were to run the same code outside of RStudio it would not work.\n\n\nTroubleshooting file paths\nIf R can’t find your file, try these steps:\n\nCheck the spelling and capitalisation (R is case-sensitive!)\nMake sure the file is actually in the location you think it is\nUse list.files(\"data\") to see all files in your data folder\nIf using an absolute path, double-check it’s correct for your computer\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen sharing your code with others or moving your project to a different computer, relative paths (like \"data/file.csv\") will still work, but absolute paths (that include your username and specific computer folders) will break. This is why we recommend always using relative paths in your R projects!\n\n\n\n\n\nImporting CSV files\nCSV files are simple text files where data is separated by commas. They’re widely used because they’re simple and can be read by most software.\n\n\nAlways check your data after importing it. A quick look at the structure and first few rows can catch common issues early.\nTo import a CSV file, we use the read_csv() function from the readr package:\n\n# Using a relative path from the project root\nstudent_data &lt;- read_csv(\"data/student_scores.csv\")\n\nRows: 10 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): student_id, quiz_score, homework_score, final_score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows\nhead(student_data)\n\n# A tibble: 6 × 4\n  student_id quiz_score homework_score final_score\n       &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1       1001         85             92          88\n2       1002         78             85          82\n3       1003         92             88          90\n4       1004         65             78          72\n5       1005         88             90          89\n6       1006         72             75          74\n\n# Get a summary of the data\nsummary(student_data)\n\n   student_id     quiz_score   homework_score   final_score   \n Min.   :1001   Min.   :65.0   Min.   :75.00   Min.   :72.00  \n 1st Qu.:1003   1st Qu.:78.0   1st Qu.:82.75   1st Qu.:80.50  \n Median :1006   Median :83.5   Median :86.50   Median :86.50  \n Mean   :1006   Mean   :82.5   Mean   :85.70   Mean   :84.30  \n 3rd Qu.:1008   3rd Qu.:89.5   3rd Qu.:89.50   3rd Qu.:88.75  \n Max.   :1010   Max.   :95.0   Max.   :94.00   Max.   :95.00  \n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways try to use relative paths within your R project. This makes your code:\n\nMore portable (works on different computers)\nEasier to share with others\nLess likely to break when files move\n\n\n\n\n\nImporting Excel files\nFor Excel files, we use the read_excel() function from the readxl package:\n\n# Import an Excel file\nweather_data &lt;- read_excel(\"data/weather_data.xlsx\")\n\n# View the first few rows\nhead(weather_data)\n\n# A tibble: 6 × 4\n  date                temperature rainfall humidity\n  &lt;dttm&gt;                    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 2024-01-01 00:00:00        25.5      0         65\n2 2024-01-02 00:00:00        27.8     12.5       78\n3 2024-01-03 00:00:00        24.2      8.2       82\n4 2024-01-04 00:00:00        26.1      0         70\n5 2024-01-05 00:00:00        28.4      0         68\n6 2024-01-06 00:00:00        23.9     15.6       85\n\n# Look at the structure of the data\nstr(weather_data)\n\ntibble [10 × 4] (S3: tbl_df/tbl/data.frame)\n $ date       : POSIXct[1:10], format: \"2024-01-01\" \"2024-01-02\" ...\n $ temperature: num [1:10] 25.5 27.8 24.2 26.1 28.4 23.9 22.8 25.6 27.2 26.8\n $ rainfall   : num [1:10] 0 12.5 8.2 0 0 15.6 22.3 0 0 5.4\n $ humidity   : num [1:10] 65 78 82 70 68 85 88 72 65 75\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen importing data:\n\nAlways check the first few rows using head()\nLook at the data structure using str()\nCheck for any missing values using summary()\n\n\n\n\n\nCommon importing issues\nHere are some common issues you might encounter when importing data:\n\n\nData import problems are common but can usually be fixed by specifying the correct options in your import function.\n\nFile path errors: Ensure you’re using the correct path relative to your project directory\nMissing values: R might interpret empty cells differently than expected\nColumn types: Sometimes R might guess the wrong data type for columns\nSpecial characters: Non-ASCII characters might not import correctly\n\nYou can handle these issues using additional arguments in the import functions:\n\n# Example with more options\nstudent_data &lt;- read_csv(\"data/student_scores.csv\",\n  na = c(\"\", \"NA\", \"missing\"), # Define missing value codes\n  col_types = cols( # Specify column types\n    student_id = col_character(),\n    quiz_score = col_double(),\n    homework_score = col_double(),\n    final_score = col_double()\n  )\n)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#summing-up",
    "href": "labs/Lab01.html#summing-up",
    "title": "Lab 01 – Getting started",
    "section": "Summing up",
    "text": "Summing up\nIn this lab, you’ve learned:\n\nBasic setup: Installing R/RStudio and creating projects\nCreating and formatting Quarto documents\nUsing R packages and basic operations\nImporting data from CSV and Excel files",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "labs/Lab01.html#take-home-exercises",
    "href": "labs/Lab01.html#take-home-exercises",
    "title": "Lab 01 – Getting started",
    "section": "Take-home exercises",
    "text": "Take-home exercises\n\nExercise 5: Creating a lab report template\nCreate a Quarto document that will serve as your template for future lab reports. Your template should include:\n\nA YAML header with:\n\nYour name and student ID\nThe unit of study code (ENVX1002)\nThe current date\nOutput format set to HTML\n\nThe following sections (using appropriate header levels):\n\nIntroduction\nMethods\nResults\nDiscussion\nReferences\n\nInclude at least one example of each of these formatting elements:\n\nBold text\nItalic text\nA bullet point list\nA numbered list\nA link to a relevant website\nAn empty R code chunk\n\n\nSave this template as lab_report_template.qmd in your ENVX1002 project folder.\n\n\nExercise 6: Data exploration practice\nCreate a new Quarto document called data_exploration.qmd and complete the following tasks:\n\nLoad the required packages (readr and readxl)\nCreate a simple data frame with two columns. You may need to use the data.frame() function to create this data frame, or you could do this manually in Excel and then import it using one of the read_*() functions. The data frame should have the following structure:\n\nsite_id: A to E\ntemperature: 15.2, 14.8, 15.6, 14.9, 15.3\n\nUse these functions to explore your data frame:\n\nhead() to view the first few rows\nstr() to examine the structure and data types\ndim() to check the dimensions (rows and columns)\nnames() to see the column names\n\n\nRemember to add text explanations between your code chunks describing what each function does and what information it provides about your data.\n\n\n\n\n\n\nTip\n\n\n\nThese exercises will help reinforce the skills you’ve learned today and create useful resources for future labs. Make sure to render your documents to check that everything works correctly.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Getting started"
    ]
  },
  {
    "objectID": "001-intro.html",
    "href": "001-intro.html",
    "title": "Introduction to statistics",
    "section": "",
    "text": "Welcome to ENVX1002! This unit introduces you to essential statistical methods that will help you understand and analyse data, regardless of your field of study. Whether you’re studying environmental science, agriculture, veterinary science, or any other discipline, the skills you’ll learn here will help you make sense of data and draw meaningful conclusions from it.\n\n\nBy the end of this unit, you will be able to:\n\nUnderstand different types of data and when to use them\nUse R and Excel for basic data analysis\nApply appropriate statistical methods to answer research questions\nCreate clear visualisations of your findings\nInterpret and communicate statistical results confidently\n\nThe above are, of course, simplified for your convenience. The actual learning objectives are more detailed and you can find them in the unit outline.\n\n\n\nENVX1002 is about learning to use statistical and computing methods to answer quantitative questions. It combines three key elements:\n\nApplied statistics: Tools and methods to analyse data\nData analysis: Practical skills to process and understand information\nResearch methods: Approaches to answer scientific questions\n\n\n\n\nStatistics bridges data analysis and scientific research\n\n\n\n\n\nThe University of Sydney has a strong research tradition, and throughout your degree, you’ll have opportunities to engage with research in your chosen field. This unit will equip you with the fundamental skills needed to understand and conduct research, regardless of your specific area of study.\n\n\n\nThe research cycle: from question formulation through data collection, analysis, and communication of results which (often) leads to more questions, and iterations thereof\n\n\n\n\nStatistics plays a vital role in modern research by providing rigorous methods to analyse and interpret data. In research, we use statistics to explore patterns in our data, from simple summaries to complex relationships between variables. For example, astronomers used statistical analysis to discover thousands of planets around other stars by detecting tiny, regular dips in star brightness - sometimes as small as 0.01%! Without statistics to separate these tiny signals from random variations in brightness, we would never have found these distant worlds.\nThese analytical methods go beyond just describing data - they give us the power to draw reliable conclusions. Using techniques like hypothesis testing, we can determine if observed differences between groups are truly meaningful or just due to chance. Perhaps most importantly, we can quantify our uncertainty. Rather than simply claiming “Treatment A works better than Treatment B,” we can state precisely how confident we are in our findings.\nMost importantly, these methods enable discoveries that change the world. From measuring the impact of renewable energy breakthroughs to tracking the recovery of endangered species, data analysis replaces guesswork with evidence-based confidence. This approach helps us tackle our planet’s biggest challenges.\n\n\n\n\n\n\nData comes in various forms. Understanding these different types helps us choose the right analysis methods:\n\nContinuous data (measurements on a continuous scale)\n\nHeight measurements\nTemperature readings\nWeight measurements\n\nDiscrete data (counts or whole numbers)\n\nNumber of items in a sample\nFrequency of events\nPopulation counts\n\nCategorical data\n\nYes/no responses\nClassifications (A, B, C)\nTypes or categories\n\n\n\n\n\nIn research, we often study samples to understand larger populations:\n\nA population includes all possible cases (e.g., all students at the university)\nA sample is a subset we actually measure (e.g., 100 randomly selected students)\n\n\n\n\nA sample of n = 20 units (v) taken from a population of N = 1,000 (O)\n\n\n\n\n\nWhen scientists measure things in nature, they never get exactly the same result twice. For example, if you measure a tree’s height several times, each measurement will be slightly different. This is called natural variability.\nIn science, we expect and plan for this variation. It happens because the natural world isn’t perfectly uniform - trees in a forest grow to different heights, temperatures change throughout the day, and no two soil samples are identical. Even our measuring tools add some variation to our results.\nStatistics gives us ways to work with this natural variation. It helps us understand what’s normal variation and what might signal a real difference that needs our attention. This understanding is essential for making reliable scientific conclusions.\n\n\n\n\n\n\nDon’t worry if you’ve never programmed before! We’ll start with the basics and build your skills gradually. You’ll learn to import and organize data, calculate summary statistics, create informative graphs, and perform statistical tests.\n\n\n\nExcel will complement your R skills by helping you organize data, create simple summaries, perform basic calculations, and prepare data for analysis.\n\n\n\n\nIn general, you can get help through:\n\nFace-to-face (lectures, tutorials, labs)\nEmail consultations\nEd Discussion forum (public or private posts)\nDrop-in sessions (Floris will organise these and announce them on Ed)\n\nRemember to ask questions early - we’re here to help you succeed.\n\n\n\nIn the coming weeks, we’ll work with real datasets and learn practical statistical methods that you can apply to data. The first step is to get comfortable with R and Excel, so make sure you’re ready to dive in…",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#learning-objectives",
    "href": "001-intro.html#learning-objectives",
    "title": "Introduction to statistics",
    "section": "",
    "text": "By the end of this unit, you will be able to:\n\nUnderstand different types of data and when to use them\nUse R and Excel for basic data analysis\nApply appropriate statistical methods to answer research questions\nCreate clear visualisations of your findings\nInterpret and communicate statistical results confidently\n\nThe above are, of course, simplified for your convenience. The actual learning objectives are more detailed and you can find them in the unit outline.",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#what-is-envx1002",
    "href": "001-intro.html#what-is-envx1002",
    "title": "Introduction to statistics",
    "section": "",
    "text": "ENVX1002 is about learning to use statistical and computing methods to answer quantitative questions. It combines three key elements:\n\nApplied statistics: Tools and methods to analyse data\nData analysis: Practical skills to process and understand information\nResearch methods: Approaches to answer scientific questions\n\n\n\n\nStatistics bridges data analysis and scientific research",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#introduction-to-research",
    "href": "001-intro.html#introduction-to-research",
    "title": "Introduction to statistics",
    "section": "",
    "text": "The University of Sydney has a strong research tradition, and throughout your degree, you’ll have opportunities to engage with research in your chosen field. This unit will equip you with the fundamental skills needed to understand and conduct research, regardless of your specific area of study.\n\n\n\nThe research cycle: from question formulation through data collection, analysis, and communication of results which (often) leads to more questions, and iterations thereof\n\n\n\n\nStatistics plays a vital role in modern research by providing rigorous methods to analyse and interpret data. In research, we use statistics to explore patterns in our data, from simple summaries to complex relationships between variables. For example, astronomers used statistical analysis to discover thousands of planets around other stars by detecting tiny, regular dips in star brightness - sometimes as small as 0.01%! Without statistics to separate these tiny signals from random variations in brightness, we would never have found these distant worlds.\nThese analytical methods go beyond just describing data - they give us the power to draw reliable conclusions. Using techniques like hypothesis testing, we can determine if observed differences between groups are truly meaningful or just due to chance. Perhaps most importantly, we can quantify our uncertainty. Rather than simply claiming “Treatment A works better than Treatment B,” we can state precisely how confident we are in our findings.\nMost importantly, these methods enable discoveries that change the world. From measuring the impact of renewable energy breakthroughs to tracking the recovery of endangered species, data analysis replaces guesswork with evidence-based confidence. This approach helps us tackle our planet’s biggest challenges.",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#understanding-data",
    "href": "001-intro.html#understanding-data",
    "title": "Introduction to statistics",
    "section": "",
    "text": "Data comes in various forms. Understanding these different types helps us choose the right analysis methods:\n\nContinuous data (measurements on a continuous scale)\n\nHeight measurements\nTemperature readings\nWeight measurements\n\nDiscrete data (counts or whole numbers)\n\nNumber of items in a sample\nFrequency of events\nPopulation counts\n\nCategorical data\n\nYes/no responses\nClassifications (A, B, C)\nTypes or categories\n\n\n\n\n\nIn research, we often study samples to understand larger populations:\n\nA population includes all possible cases (e.g., all students at the university)\nA sample is a subset we actually measure (e.g., 100 randomly selected students)\n\n\n\n\nA sample of n = 20 units (v) taken from a population of N = 1,000 (O)\n\n\n\n\n\nWhen scientists measure things in nature, they never get exactly the same result twice. For example, if you measure a tree’s height several times, each measurement will be slightly different. This is called natural variability.\nIn science, we expect and plan for this variation. It happens because the natural world isn’t perfectly uniform - trees in a forest grow to different heights, temperatures change throughout the day, and no two soil samples are identical. Even our measuring tools add some variation to our results.\nStatistics gives us ways to work with this natural variation. It helps us understand what’s normal variation and what might signal a real difference that needs our attention. This understanding is essential for making reliable scientific conclusions.",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#tools-well-use",
    "href": "001-intro.html#tools-well-use",
    "title": "Introduction to statistics",
    "section": "",
    "text": "Don’t worry if you’ve never programmed before! We’ll start with the basics and build your skills gradually. You’ll learn to import and organize data, calculate summary statistics, create informative graphs, and perform statistical tests.\n\n\n\nExcel will complement your R skills by helping you organize data, create simple summaries, perform basic calculations, and prepare data for analysis.",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#getting-help",
    "href": "001-intro.html#getting-help",
    "title": "Introduction to statistics",
    "section": "",
    "text": "In general, you can get help through:\n\nFace-to-face (lectures, tutorials, labs)\nEmail consultations\nEd Discussion forum (public or private posts)\nDrop-in sessions (Floris will organise these and announce them on Ed)\n\nRemember to ask questions early - we’re here to help you succeed.",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "001-intro.html#whats-next",
    "href": "001-intro.html#whats-next",
    "title": "Introduction to statistics",
    "section": "",
    "text": "In the coming weeks, we’ll work with real datasets and learn practical statistical methods that you can apply to data. The first step is to get comfortable with R and Excel, so make sure you’re ready to dive in…",
    "crumbs": [
      "Introduction to statistics"
    ]
  },
  {
    "objectID": "000-rguide.html",
    "href": "000-rguide.html",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "",
    "text": "This is your R guide to survive ENVX1002. This document was designed to help you with (some of) the most common R-related issues faced by past students. The goal was to make this document as succinct as possible to avoid overloading you with a ton of code and nonessential details/explanations. Pay close attention to the notes and tips that will help you write nicer code. The topics covered are available in the “On this page” section on the right side of the screen.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#R",
    "href": "000-rguide.html#R",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "1. What is R?",
    "text": "1. What is R?\nR is a statistical programming language that can be used to store, manipulate, visualize and analyse data. It contains a number of predefined functions (i.e. tools), but you can also program your own.\nR is open source which means that you can examine and modify the raw software code if needed. A worldwide collective of scientists, programmers and statisticians are working on improving and extending the capabilities of R.\nOf immediate value to you is that it is free so for the rest of your career you can keep using R without burdening future employers with software licence fees. \nNote: When we talk about R, we may refer to the programming language or the software.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Running",
    "href": "000-rguide.html#Running",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "2. Running R in Rstudio",
    "text": "2. Running R in Rstudio\nR is an open-source programming language. RStudio is a Graphical User Interface (GUI) for R. What does this mean? Well, imagine a car. The engine is what makes the car move. Without the engine, the car does not do anything. However, it would be quite challenging to use the engine without all the other parts of the car. The different parts of the car enables you to use the engine in a simplified and comfortable manner. Here, R is our engine and Rstudio is the car around the engine.\nAlways install R first, and then R studio. You can download R and R studio from: https://posit.co/download/rstudio-desktop/\n\n\n\nFigure1. RStudio interface. (Note: the layout may vary depending on your computer, packages and display settings)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Writing",
    "href": "000-rguide.html#Writing",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "3. Writing code",
    "text": "3. Writing code\nYou can communicate with R (the software) by writing lines of code in the R language. This code must follow certain rules and respect a predefined syntax otherwise R will not be able to execute what you are trying to do.\nIn this unit, most of the code will be written using a basic syntax, called ‘Base R’. We will also write some code following the ‘Tidyverse’ syntax, a slightly more elaborated and recent syntax.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Script",
    "href": "000-rguide.html#Script",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "4. Script",
    "text": "4. Script\nYou can enter a line of code directly in the Console, but this code will get lost. Writing the code in a script is the best way to save your work. It is then easy to re-open it, and modify it anytime you need to (in the top left corner, File &gt; Open File..). You can use a basic R script (the equivalent of a simple text file or note pad) or you can use a much nicer script: a Quarto Document.\nIn a Quarto document, you will find sections dedicated to writing code (a code chunk) and you can write text in between the code chunks.\nYou can then render your Quarto document to turn it into a .html or .pdf (which is not the case with a regular R script). You can create a new R script or Quarto document by pressing the “New File” button (top left corner above the Script Editor window).\nTip: the shortcut to insert a new code chunk is Ctrl+Alt+I (Windows) or Cmd+Option+I (Mac)\nTip: it is really important to write some comments in your code to keep track of what you do.\nIn a basic R script, you can use a # to comment out a line of code or write a comment for yourself. Any line of code starting by a # will be automatically considered as irrelevant by the software.\n\n# this is a comment\n2 + 3\n# 5-3\n8 / 7 # the result is 1.142857\n\nNote: to run a line of code from a script, simply highlight the line of code and press ctrl+enter (or command+return).\nAvoid running the whole script by pressing “Run”. Similarly, avoid running a whole chunk code if you are building your code or trying to debug.\nTip: to run a single line of code, simply place the cursor on the line of code, click only once and run it (ctrl+enter or command+return).\nYou can select a single word in the code by clicking twice on it, then run it (that is very convenient to check what an object is).\nYou can select an entire line of code by clicking three times.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Rprojects",
    "href": "000-rguide.html#Rprojects",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "5. R projects",
    "text": "5. R projects\nRStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents. Create a new project for each new Practical to avoid working directory issues.\nTo create a new project in RStudio, use the Create Project command (available on the Projects menu and on the global toolbar):\n\n\n\nFigure 2. Create a new project.\n\n\nWe can open a project by:\n\nUsing the Open Project command (available from both the Projects menu and the Projects toolbar) to browse for and select an existing project file (e.g. MyProject.Rproj).\nSelecting a project from the list of most recently opened projects (also available from both the Projects menu and toolbar).\nDouble-clicking on the project file within the system shell.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Objects",
    "href": "000-rguide.html#Objects",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "6. Defining objects",
    "text": "6. Defining objects\nR can do everything a calculator does… and much more! The power of R lies in its ability to define and store “objects”. An object is simply a storage space that we can create. We can name it whatever we want and this object can contain pretty much anything we want (e.g. a value, a word, a spreadsheet, a list of things, etc.).\nWe need to use the assignment operator ” &lt;- “ to define an object.\n\nmy_object &lt;- 2 # defining the object named \"my_object\" and storing the value \"2\" in it.\n\na &lt;- 5 + 3\nb &lt;- \"hello world!\"\n\nAfter an object has been defined, we can verify what it contains by calling it.\n\nmy_object\na\nb\n\nWe can update and overwrite an object.\n\na &lt;- 2 * a\nb &lt;- \"update: hello world!!\"\na\nb\n\nNote: every time we create or update an object, it can be seen in the Environment window.\nTip: R is case-sensitive, so my_object is totally different from my_Object (and x is different from X). So be careful when naming your objects.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Functions",
    "href": "000-rguide.html#Functions",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "7. Using functions",
    "text": "7. Using functions\nA function is a block of code that performs a specific task by taking in data, processing it, and returning a result. A function is a tool that will do only one specific task. A function only runs when it is called (i.e. by running the line of code calling this function).\nIn a script, a function can be identified by the presence of parentheses. Inside the parentheses of the function, we will supply the data on which we want to perform the task, and a number of arguments. Arguments are simply some settings or parameters we want to apply to a function.\nHere are some examples of basic functions in R:\n\n# the \"combine\" function (Combines Values into a Vector or List)\nmy_object &lt;- c(3, 1, 7, 12, 11.5)\nmy_object\n\n\n# the mean function\nmean(c(3, 1, 7, 12, 11.5))\n\n# or apply the function directly on my_object\nmean(my_object)\n\n\n# plot function\nplot(my_object)\n\nNote: even after using R for 10+ years, you will still come across new functions very frequently. Most of the time we have no idea what a function does, or what argument it needs, or which package it comes from… and that’s ok! Each function (or package) comes with a documentation to help us using it. The code to access the documentation is simply a “?” followed by the empty function (i.e. nothing within it’s parentheses).\n# access the documentation for the hist function\n?hist()\n\n# access the documentation for the median function\n?median()",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Packages",
    "href": "000-rguide.html#Packages",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "8. Packages",
    "text": "8. Packages\nA function is a tool that will do only one specific task. R comes loaded with a lot of functions directly available for us to use, but we often need to use some functions that are are not available in the basic R installation.\nWe achieve this by downloading a package. A package is simply a collection of functions designed to work on a specific topic. First we need to download a package and install it in our computer using install.packages(). Then we need to extract the functions from the package to be able to use them, using library(). We only need to install the package once, but we need to extract the functions every time we start a new R session.\nThis is a frequent source of mistake!\nStill not clear what a package is?\nLet’s imagine we are going camping. To survive this camping trip we will need a few tools/pieces of gear to make a fire (axe, matches, etc.), to prepare the food (knife, cooking pot, etc.) and to sleep (tent, sleeping bag, etc). In order to have access to all these tools every time we go camping, we first need to put all these things into a box/bag in the car’s trunk. Upon arrival at the camping site, we can not use any of these tools until we take to box out of the car and open it to take something from it. In this analogy, loading the car with a box full of tools/gear is like installing a package (you only need to do it once), and taking the box of the car to open it on the camping site is like loading the package (i.e. opening the box to release the tools).\n\n# download and install a package on the computer (run only once!)\ninstall.packages(\"dplyr\")\n\n# load the package (run in every session) \nlibrary(dplyr)\n\n# load the package\nrequire() # an alternative to library()\nThe “Packages” window (bottom right) shows the list of all the packages installed on the computer, and this list keeps growing every time we download a new package. All these packages will remain on the computer unless we decide to remove R & Rstudio and re-install it from scratch.\nAny package with the tick sign (check mark) on its left is a package which is already loaded in the current R session. Some packages get automatically loaded when we start a new R session, but most of them will need to be loaded manually using library().\nR, R studio and packages all need to get updated occasionally.\n\n# update a package\nupdate.packages(c(\"dplyr\", \"ggplot2\"))\nNote: Packages are often mentioned within “{ }”. For example, {readxl} would read as “the readxl package”.\nNote: Two functions coming from two different packages might have the same name. As a result, R struggles to pick the correct function, which may lead to Error messages or incorrect results. This can be avoided by specifying which package we want to use. For example, “dplyr::select()”, read as “the function select() from the dplyr package”.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Directory",
    "href": "000-rguide.html#Directory",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "9. Working directory",
    "text": "9. Working directory\nConducting data analysis will often start with reading a spreadsheet (or text file) in R and will end by writing/saving some documents (pdf, html, image, etc.). Unless we indicate the correct path to this file, R will search for a file (or save file) in a default location on our computer. This default location is called the working directory. We can check what is our working directory and change it manually.\n\n# check the working directory.\ngetwd()\n\n# set the working directory.\nsetwd() # provide a path to the function (e.g. \"C:/Desktop/R_documents)\nBy default, your script will be saved in the current working directory.\nTip: if you open an existing script (or R project) by double-clicking on it, your working directory will be automatically set to this location. So to avoid any working directory issue when starting a new script, simply save your script, close R Studio, navigate to your new script and re-open it by double clicking on it.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Spreadsheet",
    "href": "000-rguide.html#Spreadsheet",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "10. Reading a spreadsheet",
    "text": "10. Reading a spreadsheet\nThere is a number of functions available to read a spreadsheet in R. We may need to open .txt, .xlsx, .xls. or .csv files.\nCommon errors: the path provided does not lead to the location where the spreadsheet is, the file we are trying to read is not located in the working directory, or the name of the file is not provided correctly (R is case sensitive, remember? ).\nTip: check what the working directory is, and place the spreadsheet in that location. Make sure the name of the file is identical to the one provided in the code.\n\n# install.packages(readxl)\nlibrary(readxl)\nmy_data&lt;-read_excel(\"data/ENVX1002_Data.xlsx\", sheet = \"Sheet3\")",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#DataFrames",
    "href": "000-rguide.html#DataFrames",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "11. Data Frames",
    "text": "11. Data Frames\nA data frame is a 2 dimensional object, made of columns and rows. It is basically the R equivalent to a spreadsheet. Data frames can have different types of data inside it (i.e., character, numeric or logical).\nHere are some useful functions to work on data frames:\n\n#my_data&lt;-read_excel(\"data/ENVX1002_Data.xlsx\", sheet = \"Sheet3\")\n\nis.data.frame(my_data) # check if the object is a data frame\nhead() # print the first 6 rows of the data frame\ntail() # print the last 6 rows of the data frame\n\nstr() # print information about the variables (i.e. columns) \nsummary() # print baisc summary about the variables  \n\nnrow() # the number of rows \nncol() # the number of columns \nlength() # the number of columns \ndim() # the number of rows and columns\n\ncolnames() # the names of the variables (i.e. columns) \nnames() # the names of the variables (i.e. columns) \nTip: str() is a very useful function to get to know your data. It is a good habit to make sure you run str(your_data_frame) directly after you imported your spreadsheet.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#BuiltInDataSet",
    "href": "000-rguide.html#BuiltInDataSet",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "12. Built-in data sets",
    "text": "12. Built-in data sets\nR comes with many built-in data sets, ready for you to use. They are perfect for practicing your analytical skills. Some of the most used data sets include: iris, mtcars, PlantGrowth, etc.\n\ndata() # to see the list of of pre-loaded data sets. \n\ndata(mtcars) # Loading mtcars \n\nhead(mtcars) # Print the first 6 rows of mtcars\n\n?mtcars # access the documentation about this data set.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Subsetting",
    "href": "000-rguide.html#Subsetting",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "13. Subsetting a data frame and selecting specific cases",
    "text": "13. Subsetting a data frame and selecting specific cases\nNote: this is one of the most common source of error in ENVX. Make sure you master this section!\nWhen conducting analyses, we will need to subset the data frame (i.e. isolate part of it) or select specific cases. Our data set is made of variables containing many observations (in R, this translate to “a dataframe is made of columns and rows” ).\nWe will often need to take one variable at the time. We can do this by using the “$” between the name of our data frame and the name of the variable, such as my_dataframe$variable_name. Here are some examples using the “iris” built-in dataset:\n\ndata(iris) # load the iris dataset\n\ndf &lt;- iris # save the iris dataset in an object called 'df' (for dataframe)\n\ncolnames(df) # check the names of the variables\n\ndf$Sepal.Width # print the Sepal.Width variable\n\nplot(df$Sepal.Width) # plot the Sepal.Width variable\n\n\n# OR, save it into its own object\nmy_variable &lt;- df$Sepal.Width\n\nplot(my_variable)\nWe can do more than just selecting a single variable! We can include or exclude pretty much anything we want from a data frame by accessing the rows and columns.\nWe can achieve this by using the square brackets with a coma in the middle. Anything we write before the coma will control the rows, while anything we write after the coma will control the columns, such as:\nmy_dataframe[rows, columns]\nLet’s look at some simple examples using the mtcar built-in data set.\n\ndata(mtcars) # load the mtcars dataset\n\ndf &lt;- mtcars # save the mtcars dataset in an object called 'df' (for dataframe)\n\n# including/excluding rows\ndf[5,]# the 5th row only (across all columns)\ndf[-5,]# all the rows but the 5th one (across all variables)\ndf[3:5,]# the rows 3 to 5 (across all columns)\ndf[c(3,5,7),]# the rows 3, 5 and 7 using the c function (across all columns)\n\n# including/excluding columns\ndf[,3]# the 3rd column only (for all rows)\ndf[,-3]# exclude the 3rd column (for all rows)\ndf[,4:7]# the columns 4th to 7th (for all rows)\ndf[,c(1,3,7)]# the columns 1,3,and 7 (for all rows)\n\n# including/excluding columns AND rows\ndf[3:4,4:7]# the rows 3 and 4 for the columns 4th to 7th \nWe can also make a selection based on a given value within one (or two) variable(s).\n\n# filter based on one variable's values\ndf[df$hp &gt;100,] # only the rows with a hp &gt; 100 (across all variables)\n\n# filter based on two variables\ndf[which(df$hp &gt;100 & df$disp &gt; 400),] # only the rows with a hp &gt; 100 and mpg &lt; 20\nNote: “( )” are parentheses, “[ ]” are square brackets, “{ }” are curly rackets (or braces).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#DataType",
    "href": "000-rguide.html#DataType",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "14. Data type",
    "text": "14. Data type\nIn R, an object can store pretty much anything, but very often we need to know what type of data we are working with. There are 5 basic data types in R:\n\ncharacter: \"a\", \"day\"\nnumeric: 6, 3.14\ninteger: 2 or 2L (the L tells R to store this as an integer)\nlogical: TRUE, FALSE\ncomplex: 1+3i (complex numbers with real and imaginary parts)\n\nThere is a number of functions available to check what type of data we are working with:\n\nclass() # what kind of object is it?\ntypeof() # what is the object’s data type?\nlength() # how long is the object?\nis.character() # returns TRUE or FALSE\nis.numeric()\nis.integer()\nis.logical()\n\nas.character() # convert to character \nas.numeric()",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#DataStructure",
    "href": "000-rguide.html#DataStructure",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "15. Data structure",
    "text": "15. Data structure\nWhen we define an object by storing something inside it, we automatically assign a data structure to our object. The most common structures include:\n\nvector: the most common and basic data structure in R. There are two types: atomic vectors and lists. A vector is a collection of elements that are most commonly of mode character, logical, integer or numeric.\n\n# create a empty vector to store 4 elements of type character\nvector(\"character\", length=4)\ncharacter(5) # the same thing, but using the constructor directly\nnumeric(5)   # a numeric vector with 5 elements\nlogical(5)   # a logical vector with 5 elements\n\n# create vectors by directly specifying their content. \n# R will then guess the appropriate mode of storage for the vector. \nx &lt;- c(1, 2, 3)\nx &lt;- c(1L, 2L, 3L)\nx &lt;- c(TRUE, TRUE, FALSE, FALSE)\nx &lt;- c(\"John\", \"Paul\", \"George\", \"Ringo\")\n\n# examine a vector\nis.vector() # check if the object is a vector\ntypeof()\nlength()\nclass() \nstr()\n\n# count elements in a vector\nmy_vector&lt;-c(2,4,6,8,10,11,13,15)\nlength(my_vector[my_vector&gt;10])\nmatrix: a two dimensional data set, with rows and columns.\n\n# Create a matrix\nmy_matrix &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\n\n# access items in the matrix\nmy_matrix[, ] # [rows,columns]\n\ndata frame: a two dimensional data set, with rows and columns. Data frames can have different types of data inside it.\n\n# Create a data frame\ndf &lt;- data.frame (\n  Variable1 = c(\"Monday\", \"Tuesday\", \"Wednesday\"),\n  Variable2 = c(10, 15, 20),\n  Variable3 = c(6, 3, 5)\n)\n\nsummary(df) #summarise the df\ndim(df)\nnrow()\nncol()\nfactors: factors are used to categorize data. The “categories” inside a factor are called “levels” (i.e. the labels used to identify groups/categories).\n# Create a factor\nmy_factors &lt;- factor(c(\"high\", \"low\", \"medium\"))\n\nlevels() \nlength()\nmy_factor[2]\nis.factor()\nas.factor()\n\n# ordering the factors\nmy_factors_ordered &lt;- ordered(my_factors, levels = c(\"low\", \"medium\", \"high\"))\n\nNote: when testing the effect of a treatment (e.g. a drug) or any other categorical data (e.g. farm, location, etc.), we need to make sure to specify that the variable should be treated as categorical. This is how we do it:\ndf$Variable1 &lt;- as.factor(df$Variable1) \n\n# take the variable 1 from df,\n# convert it as factor and \n# feed it back into the column \"Variable1\" of df.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#MissingData",
    "href": "000-rguide.html#MissingData",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "16. Dealing with missing values (NA)",
    "text": "16. Dealing with missing values (NA)\nIt can happen that we have some missing data in a vector. Missing data are indicated as NA in R.\n# inspect if there are NAs\nis.na()\nanyNA()\nsum(is.na())\n\n# many functions have a special argument to ignore NAs\nx&lt;- c(4,6,8, NA)\nmean(x, na.rm=TRUE)\nNote: “NaN” means “Not a Number”. It’s an undefined value. Inf is infinity. You can have either positive or negative infinity.\n1/0 # Inf\n0/0 # NaN",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Tidyverse",
    "href": "000-rguide.html#Tidyverse",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "17. A brief introduction to tidyverse",
    "text": "17. A brief introduction to tidyverse\nTidyverse is a collection of packages designed to work well together. Each package covers a specific topic. For example:\n\n{dplyr} is a package for data manipulation (i.e. for working on the data frame).\n{ggplot2} is a package for data visualization (i.e., making all sort of plots and figures)\n\nSome of the functions coming from the tidyverse packages are quite similar to some functions from base R, while other enable you to do some sophisticated things with only 1 line of code (instead of 5 or 10 in base R).\nIn the tidyverse, we use this strange operator, %&gt;%, called the pipe. The pipe is used to connect different actions, creating a chain which will be processed as one single flow, as opposed to several lines of code processed one by one. For example:\nmydf_new &lt;- mtcars %&gt;% mutate(New_variable2 = Variable_2 +10 ) %&gt;% filter(New_variables2 &gt;10)\nThis line of code would translate into: take the object df, then use the function “mutate” to create a new variable (i.e. column) named New_Variable2 which contains the values from Variable_2 +10, then filter to keep only the values that are greater than 10, and store all of this inside an object named mydf_new.\nAs you can see, we can do two actions within a single line of code (create a new variable and filter the data) using the pipe sign.\nTip: the short cut for the pipe is Ctrl+Shift+M (Windows) or Cmd+Shift+M (Mac).\nExamples of functions from {dplyr}:\nutils::data(starwars) # load the built-in data set \ndplyr::glimpse(starwars) # take a look at the structure of the data  \n\n# filter the data \nstarwars %&gt;% dplyr::filter(skin_color == \"blue\") \n# filter to keep only the rows with \"blue\" in skin_color \n\nstarwars %&gt;% dplyr::filter(skin_color == \"blue\", eye_color == \"brown\") # filter to keep only the rows with \"blue\" skin_color and \"brown\" eye_color  \n\n# arrange (i.e. order) the rows \nstarwars %&gt;% dplyr::arrange(height) # order the rows using the mass values  \n\n# slice the rows to select specific rows \nstarwars %&gt;% dplyr::slice(3:6) # keep only the rows 3 to 6  \n\n# select columns \nstarwars %&gt;% dplyr::select(height, skin_color, eye_color) # keep only air_color, skin_color, eye_color  \n\n# create new columns \nstarwars %&gt;% dplyr::mutate(height_doubled = height * 2) \n\n# create more more than one column \nstarwars %&gt;%   \n  dplyr::mutate(height_m = height / 100,     \n         BMI = mass / (height_m^2)    \n         )  \n\n# collapse a data frame to a single row \nstarwars %&gt;% dplyr::summarise(height = mean(height, na.rm = TRUE)) # calculate the mean of height over all rows\nWe can then combine different functions and use them within a single chain. For example:\nstarwars_modified &lt;- starwars %&gt;%   \n  dplyr::group_by(species, sex) %&gt;%   \n  dplyr::select(height, mass) %&gt;%   \n  dplyr::summarise(\n    height = mean(height, na.rm = TRUE),    \n    mass = mean(mass, na.rm = TRUE)   \n    )  \n# The code above does to the following steps:   \n#1) take the starwars data frame  \n#2) group by species and sex, which correspond to \"process everything one species at a time, and one sex at a time\".\n#3) keep only the height and the mass\n#4) summarize the height (by calculating the mean, and remove the NAs) and the mass (again, by calculating the mean)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#ggplot2",
    "href": "000-rguide.html#ggplot2",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "18. Data visualization with ggplot2",
    "text": "18. Data visualization with ggplot2\nTo make a plot, you will need to pass some data to the ggplot() function, which stores the data to be used later by other parts of the plotting system. \nggplot(data = mpg)\nThe mapping of a plot is a set of instructions on how parts of the data are mapped onto aesthetic attributes. In other words, we will define which variable will be used on the x axis, y, etc.\nThe mapping is made by using the aes() function.\nggplot(mpg, mapping = aes(x = cty, y = hwy))\nTo customize the plot, we will use the layers. Consider that a “layer” is simply an extra layer of information which you apply to your plot. A layer affects anything, from the size of the data points, to the position of an axis, etc. Layers take the mapped data and display it in something humans can understand as a representation of the data.\n\nlibrary(ggplot2)\ndata(mpg)\nggplot2::ggplot(mpg, aes(cty, hwy)) + # to create a scatterplot\n  geom_point(size = 3, shape = 6) +\n  # to fit and overlay a loess trendline\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_classic()\n\nThis plot was made using ggplot2. In base R, the code to produce the same figure would have been:\n\ndata(mpg)\n\n# first define the model\nmy_model &lt;- lm(hwy ~ cty, data = mpg)\n# get teh prediction and the 95% confidence interval\nconf_interval &lt;- predict(my_model, interval = \"confidence\", level = 0.95)\n# plot the data\nplot(hwy ~ cty,\n  data = mpg,\n  pch = 6, # the shape of the points\n  ylim = c(10, 50)\n) # the ticks of the y axis\n\n# add the regression line\nabline(my_model, col = \"blue\", lwd = 3)\nlines(mpg$cty, conf_interval[, 2],\n  col = \"gray\",\n  lty = 3\n)\nlines(mpg$cty, conf_interval[, 3],\n  col = \"gray\",\n  lty = 3\n)\n\nThis is a good demonstration of why you should use {ggplot2} to make figures (4 lines of code using {ggplot2} versus 7+ lines of code in {base} R).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "000-rguide.html#Debugging",
    "href": "000-rguide.html#Debugging",
    "title": "A brief R guide for surviving ENVX1002",
    "section": "19. Debugging",
    "text": "19. Debugging\nHere is something important to keep in mind. Writing code leads to making mistakes. Forgetting a sign, errors in the syntax, providing the wrong path, typing a semicolon (;) instead of a colon (:), issues with a package… There is a million of ways in which we can make a mistake while writing code. In fact, your code will rarely (never?) run smoothly from the first try… and that’s ok!\nThe heart of coding comes from … DEBUGGING! Debugging means that we double check each line of code to find and fix issues until the code runs smoothly. It sometimes takes 20min (or more) to debug just a few lines of code.\nHere are some tips to help you debugging.\n1) look at the Console. This is where you can see if the code runs smoothly (or not). The sign &gt; means that R is ready to accept new code, which means that the previous line of code has been executed without issue.\n2) A “Warning” message does not mean that the code did not run. It simply provids you with some information about what the functions did. However, an “Error” message means that the code crashed (time to start debugging).\n3) Run each line one by one to identify which one makes the code crash.\n4) Check what the objects contain (remember that you can simply double click on the object and run it quickly to check what is in it).\n5) Access the documentation of a function using the “?” before an empty function (e.g. ?median() ). There, you will find explanations about the arguments used in the function, and some examples.\n6) Pay attention to the words used in the Error message. “Class” may indicate some issue with one of the object (you may be trying to feed a factor into a function that takes numerical data only); “Path” means that you may have provided the wrong path to your data frame, etc.\n7) There is a vast community of R users online. If you can not understand the Error message , simply copy/paste it in Google. You are certainly not the first person to have this error and it is likely that this issue has already been discussed (and hopefully solved) in a forum. There is a number of forums and blogs for R users that will likely have an answer to your issue. Here are a few of them: R-bloggers, StackOverflow, RStudio Community, Github R Topics, CrossValidated, etc.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "A brief R guide for surviving ENVX1002"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX1002 Handbook",
    "section": "",
    "text": "Welcome to the ENVX1002 lab handbook. This handbook is designed to be a useful companion to the lectures, labs and assessments."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "ENVX1002 Handbook",
    "section": "Quick links",
    "text": "Quick links\n\nENVX1002 Unit page – check assessment weights, due dates, and other important information\nCanvas – the main hub\nEd discussion forum – ask questions, share resources, and get help"
  },
  {
    "objectID": "index.html#how-to-use-this-handbook",
    "href": "index.html#how-to-use-this-handbook",
    "title": "ENVX1002 Handbook",
    "section": "How to use this handbook",
    "text": "How to use this handbook\nRead this handbook before, or after each lecture to better understand certain concepts. Computer lab exercises are also included, which you can use to prepare for the lab sessions.\nTo begin, we strongly suggest that you read our introduction chapter (which will align with Week 1 Lectures) to get an overview of why ENVX1002 is important for your degree and includes some introductory statistical concepts.\n\n\n\n\n\n\nAcknowledgements\n\n\n\n\n\nWe would like to acknowledge the work of previous generations of teaching staff who created the bulk of the teaching material within this manual. In particular, the work of:\n\nAssoc. Prof. Mick O’Neill\nDr. Kathryn Aufflick\nAssoc. Prof. Peter Thomson\nProf. Thomas Bishop\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\n\n\nThis handbook is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nWhen you are ready, check the sidebar to get started."
  },
  {
    "objectID": "labs/Lab02.html",
    "href": "labs/Lab02.html",
    "title": "Lab 02 – Statistical programming",
    "section": "",
    "text": "Learning Outcomes\n\n\n\nAt the end of this practical students should be able to:\n\nAcknowledge the importance of academic integrity\nImport, subset and inspect data in R\nCalculate simple summary statistics using both R and Excel\nGenerate dynamic reports in Quarto using inline R code\nUnderstand how to debug R code",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#before-we-begin",
    "href": "labs/Lab02.html#before-we-begin",
    "title": "Lab 02 – Statistical programming",
    "section": "Before we begin",
    "text": "Before we begin\nPlease create a new Quarto document in your project folder to practice R code and complete lab exercises. We will go through the process in Exercise 1.\nThe following packages are required for this practical:\n\ntidyverse – a collection of R packages designed for data science\nreadxl – a package for reading Excel files\nmodeest – a package for estimating the mode of a distribution\nlubridate – a package for working with dates and times\n\nIf you have not already installed these packages, you can do so by adding the following code into a code chunk in your Quarto document:\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(modeest)\nlibrary(lubridate)\n\nFinally, please download the data file used in this lab here: soil.xlsx.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#academic-integrity",
    "href": "labs/Lab02.html#academic-integrity",
    "title": "Lab 02 – Statistical programming",
    "section": "Academic integrity",
    "text": "Academic integrity\nThis exercise encourages students to discuss academic integrity, and in particular the grey areas often present. Your demonstrator will provide you with a number of scenarios to discuss with each other in smaller groups, and then with the class.\nIf you are interested in more information on Academic Integrity at the University of Sydney, see the following link: Academic Integrity. Also ensure you have completed the Academic Honesty Education Module (AHEM). This must be complete before your first assessment is due (next week for ENVX1002).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-1-setting-up-walk-through",
    "href": "labs/Lab02.html#exercise-1-setting-up-walk-through",
    "title": "Lab 02 – Statistical programming",
    "section": "Exercise 1: Setting up (walk-through)",
    "text": "Exercise 1: Setting up (walk-through)\n\nGetting Started - Important!\nBefore starting each lab session, you will need to create a new Quarto document to work on the exercises. Follow these steps to set up your lab environment:\n\nOpen RStudio Project\n\nAny of the following should work:\n\nIn RStudio, click on the project name in the top-right corner and select your project from a recent list. (It might already be open, otherwise it would show “Project: (None)”)\nIn RStudio, use File &gt; Open Project to navigate to your .Rproj file\nDouble-click the .Rproj file directly from your file explorer\n\n\n\n\n\nYour project name should appear in the top-right corner of RStudio.\n\nCreate a New Quarto Document\n\nSeveral ways to do it, the easiest is by clicking the “New File” button in the toolbar and selecting “Quarto Document…”\nGive your document a meaningful name (e.g., Lab 2 Exercises), leave other options as default and click “Create”\n\nSave and Render Your Document\n\nYou must save your document now. This acts as a simple check to see that you are working in an environment that is not restricted.\nClick File &gt; Save or press Ctrl+S (Windows) / Cmd+S (Mac)\nName it something meaningful like lab02_practice.qmd\nClick the “Render” button in the editor toolbar to generate the HTML output\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you encounter any issues, please ask your demonstrator for help.\n\n\n\n\nFollow our lead\n\n\n\n\n\n\nImportant\n\n\n\nWe’ve created this section to make sure you don’t get lost during the lab. Remember to read “A brief R guide for surviving ENVX1002” available in the Tool Kit section on Canvas for a better understanding of how to use R in this course.\n\n\n\nTwo golden rules\n\nText are formatted in Markdown (like this text)\nCode must be written inside code chunks\n\nA code chunk is what makes your document “dynamic” as it can execute code and perform all sorts of tasks.\nTo create a code chunk:\n\nQuick way: Use keyboard shortcuts\n\nWindows/Linux: Ctrl + Alt + I\nMac: Cmd + Option + I\n\nManual way: Type three backticks followed by {r}, then end with three backticks\n\nHere’s what a code chunk looks like:\n```{r}\nmean(c(1, 2, 3))\n```\nNote: the “fence” that we use (the three backticks plus the {r}) will only be visible in Source view. Your demonstrator will show you how to switch between Source and Preview views.\n\n\nUnderstanding Functions\nThis is a function:\n\n# Input → Process → Output\nmean(c(2, 4, 6)) # Takes numbers, calculates average, returns 4\n\n[1] 4\n\n\nYou can recognise it because it has:\n\nA name (mean)\nParentheses () to hold input data\nInput data inside the parentheses (e.g., c(2, 4, 6))\n\nA function will almost always return an output, which you can use in your code. In this case, the output is 4. Functions are the reason why we can do so much with R – they are like actions that someobe else has written for us to use so that we can complete our tasks with minimal programming.\nLet’s look at three common functions for central tendency, but at the same time look at what it means to run functions on data we often call “objects”:\n\nmean():\n\n# Create a vector of numbers and store it in an object called \"scores\"\nscores &lt;- c(85, 92, 78, 95, 8)\n# Calculate the mean of the scores\nmean(scores)\n\n[1] 71.6\n\n\nmedian():\n\nheights &lt;- c(160, 165, 168, 170, 185)\nmedian(heights)\n\n[1] 168\n\n\nmode():\n\nvotes &lt;- c(\"yes\", \"no\", \"yes\", \"maybe\", \"yes\")\nmfv(votes)\n\n[1] \"yes\"\n\n\n\nRemember: Every function needs:\n\nParentheses () to work\nInput data (inside the parentheses)\nSometimes extra options (like na.rm = TRUE to handle missing values)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-2-water-quality",
    "href": "labs/Lab02.html#exercise-2-water-quality",
    "title": "Lab 02 – Statistical programming",
    "section": "Exercise 2: Water quality",
    "text": "Exercise 2: Water quality\nSulphate (SO4) is a key indicator in water quality monitoring and can be used to assess environmental impacts from industry and agriculture. In this exercise, we will explore a dataset of SO4 concentrations (mg/kg) in water samples.\nThe data is stored in a sheet called “SO4” in the MS Excel file, soil.xlsx.\n\nso4 &lt;- read_excel(\"data/soil.xlsx\", sheet = \"SO4\")\n\n\n\nTry writing and running the code chunks in your own Quarto document to see their outputs. Results will appear below each chunk. See Lab 01 for more information on inserting and running code chunks in Quarto.\nWhen we load data into R for the first time, it is important to check what it looks like (and whether it loaded correctly). The str() function is a good way to quickly inspect the data:\n\nstr(so4)\n\ntibble [39 × 1] (S3: tbl_df/tbl/data.frame)\n $ SO4: num [1:39] 50.6 55.4 56.5 57.5 58.3 63 66.5 64.5 63.4 58.4 ...\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nWhat does the output of str() tell us about the data? You may want to look at the documentation ?str or search online for more information, but ask your demonstrator if you’re still unsure about why we use this function. Relate this to the data you are working with.\n\n\nSince the data is a data frame object, we have a good idea of what functions we can use to explore it. Let’s examine the first few rows of our data:\n\nhead(so4)\n\nLet’s calculate some basic descriptive statistics. Read the code and try to understand every part of it, including the text in #.\n\n# Calculate mean, median, and mode\nmean_so4 &lt;- mean(so4$SO4)\nmedian_so4 &lt;- median(so4$SO4)\nmode_so4 &lt;- mfv(so4$SO4)[1] # Most frequent value using modeest package\n\n# Calculate measures of spread\nrange_so4 &lt;- range(so4$SO4)\niqr_so4 &lt;- IQR(so4$SO4)\nvar_so4 &lt;- var(so4$SO4)\nsd_so4 &lt;- sd(so4$SO4)\n\n\nReporting your data\nWhen reporting statistics in a Quarto document, there are two approaches we could take:\n\n1. Basic R output:\n\nmean_so4\n\n[1] 61.92308\n\nsd_so4\n\n[1] 5.241558\n\nmedian_so4\n\n[1] 62.1\n\n\n\n\n2. Inline reporting (recommended):\nConsider the following reporting:\n\nThe mean SO4 concentration in our samples is 61.92 mg/kg, with a standard deviation of 5.24 mg/kg. The median value is 62.1 mg/kg.\n\nUsing inline R code (approach 2) has several advantages:\n\nStatistics are seamlessly integrated into your text\nNumbers are automatically updated if your data changes\nResults are presented in a reader-friendly format\n\nTo create inline R code, use backticks with r, like this:\nThe mean SO~4~ concentration in our samples is `{r} round(mean_so4, 2)` mg/kg...\nTry to recreate the report above in your Quarto document, or use other objects like mode_so4 and var_so4 in your report.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-3-using-ms-excel",
    "href": "labs/Lab02.html#exercise-3-using-ms-excel",
    "title": "Lab 02 – Statistical programming",
    "section": "Exercise 3: Using MS Excel",
    "text": "Exercise 3: Using MS Excel\nWhy use Excel when you have R? Well, Excel is an ubiquitous tool in many industries and can be a useful complement to R for quick data exploration and analysis. Sometimes it may even be easier to use Excel for simple tasks.\nLet’s calculate the same statistics using Excel to compare approaches. First:\n\nOpen the soil.xlsx file in Excel\nNavigate to the “SO4_excel” sheet (we’ll use this sheet to avoid modifying the original data)\nEnsure the data is properly displayed in columns\n\n\n\nMS Excel remains a popular tool for quick and dirty data analysis and can be a valuable resource in just about any field.\n\n\n\nImported data in Excel. Make sure to use the “SO4_excel” sheet.\n\n\nFor any calculation in Excel:\n\nSelect an empty cell\nType “=” followed by the function name\nSelect your data range\nPress Enter\n\nFor example, to calculate the mean:\n\nClick an empty cell\nType =AVERAGE(\nSelect all SO4 values\nType ) and press Enter\n\n\n\n\nUsing the =AVERAGE() formula in Excel. Note that this screenshot demonstrates the selection of a specific range of data.\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nCalculate these statistics for the SO4 data in Excel:\n\nFor central tendency:\n\nMean: Use =AVERAGE()\nMedian: Use =MEDIAN()\nMode: Use =MODE()\n\nFor spread:\n\nRange: Use =MAX() and =MIN()\nIQR: Use =QUARTILE.INC() for Q3 and Q1\nVariance: Use =VAR.S()\nStandard Deviation: Use =STDEV.S()\n\n\nWrite notes on how you used these formulas if necessary. You may also want to compare the results with those obtained in R and write down your thoughts on the efficiency and ease of use for each method.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-4-soil-data",
    "href": "labs/Lab02.html#exercise-4-soil-data",
    "title": "Lab 02 – Statistical programming",
    "section": "Exercise 4: Soil data",
    "text": "Exercise 4: Soil data\nIn agricultural science, soil characteristics are essential for understanding soil health and fertility. The soil sheet in the soil.xlsx file contains data on soil properties at different depths, as well as lithology and land use information.\nIn this exercise, we’ll explore different ways to subset data in R using the soil characteristics data. First, let’s load the data:\n\nsoil &lt;- read_excel(\"data/soil.xlsx\", sheet = \"soil\")\n\n\nUnderstanding data structure\nLet’s examine our data structure:\n\nstr(soil)\n\ntibble [55 × 7] (S3: tbl_df/tbl/data.frame)\n $ id       : chr [1:55] \"AT1\" \"AT2\" \"AT3\" \"BM1\" ...\n $ clay0    : num [1:55] 21.26 21.43 4.52 19.37 40.64 ...\n $ clay60   : num [1:55] 30.4 38.2 42.6 24.6 75.6 ...\n $ ec0      : num [1:55] 52.4 34.9 52.8 35.1 46.4 ...\n $ ec60     : num [1:55] 32.6 25.1 38.5 26.4 35.2 ...\n $ lithology: chr [1:55] \"Siliceous Mid\" \"Siliceous Mid\" \"Siliceous Lower\" \"Siliceous Lower\" ...\n $ land_use : chr [1:55] \"Grazing\" \"Grazing\" \"Grazing\" \"Grazing\" ...\n\nhead(soil)\n\n# A tibble: 6 × 7\n  id    clay0 clay60   ec0  ec60 lithology       land_use\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;   \n1 AT1   21.3    30.4  52.4  32.6 Siliceous Mid   Grazing \n2 AT2   21.4    38.2  34.8  25.1 Siliceous Mid   Grazing \n3 AT3    4.52   42.6  52.8  38.5 Siliceous Lower Grazing \n4 BM1   19.4    24.6  35.2  26.4 Siliceous Lower Grazing \n5 BM2   40.6    75.6  46.4  35.2 Mafic           Grazing \n6 BM3   42.7    75.1  50.0  59.7 Mafic           Grazing \n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWhat do you notice about the data structure that is different from the SO4 data? How does this affect the way we subset the data?\n\n\nThe soil data frame contains the following columns:\n\nclay0: Clay content at 0 cm depth\nclay60: Clay content at 60 cm depth\nec0: Electrical conductivity at 0 cm depth\nec60: Electrical conductivity at 60 cm depth\nlithology: Type of soil composition\nland_use: Land usage type\n\n\n\nBasic subsetting in R\nThere are two main ways to subset data in R:\n\nUsing the $ operator to select columns\nUsing square brackets [] to select rows and columns\n\n\nUsing the $ operator\nThe $ operator allows us to select a specific column from our data frame. For example:\n\n# Get the land use column\nsoil$land_use\n\n [1] \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\" \n [7] \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\" \n[13] \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\" \n[19] \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Cropping\" \"Cropping\"\n[25] \"Cropping\" \"Cropping\" \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Grazing\" \n[31] \"Grazing\"  \"Grazing\"  \"Cropping\" \"Cropping\" \"Cropping\" \"Grazing\" \n[37] \"Cropping\" \"Grazing\"  \"Grazing\"  \"Grazing\"  \"Cropping\" \"Grazing\" \n[43] \"Grazing\"  \"Grazing\"  \"Cropping\" \"Grazing\"  \"Grazing\"  \"Grazing\" \n[49] \"Grazing\"  \"Grazing\"  \"Natural\"  \"Natural\"  \"Grazing\"  \"Cropping\"\n[55] \"Cropping\"\n\n\nCombined with functions, the $ operator can be used to calculate statistics on specific columns.\n\n# Calculate the mean clay content at 0 cm depth\nmean_clay0 &lt;- mean(soil$clay0)\nmean_clay0\n\n[1] 23.198\n\n\n\n\nNotice how we saved the result of the mean() function in a new object mean_clay0. This is useful for storing results and using them later in your code (e.g. for inline reporting). However, it means that the result will not be displayed in the console unless you explicitly print it by typing mean_clay0 again in a new line.\n\n\n\n\n\n\nNote\n\n\n\nThe $ operator is particularly useful when you want to:\n\nAccess a single column quickly\nUse column values in calculations\nCreate plots with specific variables\n\n\n\n\n\nUsing square brackets []\nSquare brackets allow more flexible subsetting using the format: dataframe[rows, columns]. It also works with vectors, lists, and matrices. Try the following examples:\n\nsoil[1:5, ] # First 5 rows, all columns\n\n# A tibble: 5 × 7\n  id    clay0 clay60   ec0  ec60 lithology       land_use\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;   \n1 AT1   21.3    30.4  52.4  32.6 Siliceous Mid   Grazing \n2 AT2   21.4    38.2  34.8  25.1 Siliceous Mid   Grazing \n3 AT3    4.52   42.6  52.8  38.5 Siliceous Lower Grazing \n4 BM1   19.4    24.6  35.2  26.4 Siliceous Lower Grazing \n5 BM2   40.6    75.6  46.4  35.2 Mafic           Grazing \n\nsoil[, c(\"clay0\", \"clay60\")] # Clay columns\n\n# A tibble: 55 × 2\n   clay0 clay60\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 21.3    30.4\n 2 21.4    38.2\n 3  4.52   42.6\n 4 19.4    24.6\n 5 40.6    75.6\n 6 42.7    75.1\n 7 28.1    50.5\n 8 41.3    72.6\n 9 48.7    78.7\n10 26.3    50.5\n# ℹ 45 more rows\n\nsoil[1:3, c(\"ec0\", \"ec60\")] # First 3 rows, electrical conductivity columns\n\n# A tibble: 3 × 2\n    ec0  ec60\n  &lt;dbl&gt; &lt;dbl&gt;\n1  52.4  32.6\n2  34.8  25.1\n3  52.8  38.5\n\n\n\n\nWhen using [], leaving the row or column section empty (with just a comma) means “select all”\nBasic subsetting: Use square brackets to extract soil samples at 0cm depth that have clay content greater than 40%.\n\nsoil[soil$clay0 &gt; 40, ]\n\nMultiple conditions: Find samples where clay content at 60cm is greater than 30% AND electrical conductivity at 0cm is less than 0.5.\n\nsoil[soil$clay60 &gt; 30 & soil$ec0 &lt; 0.5, ]\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nPractice: Subset the data to find samples where:\n\nclay content at 0cm depth is less than 50%\nelectrical conductivity at 60cm depth is between 25 and 30, inclusive\nclay content at 0cm is less than 10% OR electrical conductivity at 60cm is greater than 25",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-5-soil-statistics",
    "href": "labs/Lab02.html#exercise-5-soil-statistics",
    "title": "Lab 02 – Statistical programming",
    "section": "Exercise 5: Soil statistics",
    "text": "Exercise 5: Soil statistics\nWe will continue to work on the soil dataset and practice calculating some basic summary statistics.\n\n\n\n\n\n\nQuestion 5\n\n\n\n\nCalculate the mean, median, and mode for clay content at 0cm depth\nCalculate the range, IQR, variance, and standard deviation for electrical conductivity at 60cm depth\nReport these statistics in your Quarto document using inline R code",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#done",
    "href": "labs/Lab02.html#done",
    "title": "Lab 02 – Statistical programming",
    "section": "Done!",
    "text": "Done!\nThis is the end of Lab 02. Remember to save your Quarto document and submit it to Canvas when you’re done. If you have any questions, feel free to ask your demonstrator for help.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#bonus-take-home-exercises",
    "href": "labs/Lab02.html#bonus-take-home-exercises",
    "title": "Lab 02 – Statistical programming",
    "section": "Bonus: Take-home exercises",
    "text": "Bonus: Take-home exercises\nTake-home exercises are optional but recommended for further practice. You can complete these exercises in your own time.\n\nExercise 5: Mario Kart statistics\nWe’re not going to go too much about the game – but here is a dataset of character attributes from the popular Mario Kart.\nDownload it here: mario_kart.csv\nThe data is stored in a CSV file, which MS Excel can open directly. To read it into R, use the read_csv() function from the readr package.\nThe dataset contains the following variables:\n\ncharacter: Character name\nweight: Character’s weight class (1-5 scale)\nspeed: Maximum speed rating (1-5 scale)\nacceleration: How quickly they reach top speed (1-5 scale)\nhandling: Steering control rating (1-5 scale)\ntraction: Grip on different surfaces (1-5 scale)\ndrift_rating: Skill at power-sliding around corners (1-5 scale)\n\nComplete the following exercises using both R and Excel:\n\nData Import and Inspection\n\nLoad the data into R using read_csv()\nExamine the structure using str()\nView the first few rows using head()\nWhat do you notice about the data?\n\nCentral Tendency Calculate for the speed attribute:\n\nMean\nMedian\nMode\n\nWhich measure best represents the “typical” speed rating? Why?\nSpread Analysis For the weight attribute, calculate:\n\nRange\nIQR\nVariance\nStandard deviation\n\nWhat do these tell you about the variation in character weights?\nCharacter Comparison\n\nWhich characters have the highest and lowest acceleration?\nFind all characters with above-average handling\nIdentify characters with both speed and weight above 4.0\n\nAdvanced Challenge\n\nCalculate the mean of all attributes for each character\nWho is the most “well-rounded” character (closest to average in all stats)?\nCreate a report comparing Mario and Luigi’s stats using inline R code",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab02.html#attribution",
    "href": "labs/Lab02.html#attribution",
    "title": "Lab 02 – Statistical programming",
    "section": "Attribution",
    "text": "Attribution\nThis lab document is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Statistical programming"
    ]
  },
  {
    "objectID": "labs/Lab04.html",
    "href": "labs/Lab04.html",
    "title": "Lab 04 – Central limit theorem",
    "section": "",
    "text": "Project 1 is due in week 5 and for many of you this may be your first university assignment; some may be nervous, while others may be more relaxed. Your demonstrators will use the start of this practical to discuss how you may be feeling for this first assessment and share ways you might want to approach and prepare for the assessment.\nThis is the last reflective activity for now, thank you all for contributing so far and we hope you have found some benefit in the activities. Now for some probability!\n\n\n\n\n\n\nLearning outcomes\n\n\n\nAt the end of this computer practical, students should be able to:\n\ncalculate tail, interval and inverse probabilities associated with the Normal distribution\ncalculate probabilities associated sampling distribution of the sample mean by using simulation in R and using R commands.\n\n\n\nLink to data is below:\n\nENVX1002_Data4.xlsx\nAlternatively download from Canvas",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "href": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "title": "Lab 04 – Central limit theorem",
    "section": "Exercise 1 - Class activity - How tall is ENVX1002??",
    "text": "Exercise 1 - Class activity - How tall is ENVX1002??\n\nMake sure you have an existing or new project for Lab 4 and create a quarto document called Lab_4.qmd (suggestion). Save it in your project directory.\nThis is an anonymous exercise. Your demonstrator will create a google sheet (or similar) for you to enter your height and sex (M or F only so that the data is easy to analyse).\nOnce all data is collected, manually enter the data into R or export the data as a .csv to be read into R, for example:\n\n\nfemale &lt;- data.frame(heights = c(176, 180, 187, 168, 160, 170))\nmale &lt;- data.frame(heights = c(175, 183, 163, 190, 179))\n\n\nCalculate the mean and standard deviation using R and graph the distribution for both genders, for example:\n\n\n# calculate the mean and standard deviation of males\nm_mean &lt;- mean(male$heights)\nm_sd &lt;- sd(male$heights)\n\nlibrary(tidyverse) # needed for ggplot2\n\n# create a histogram of the data\nggplot(male, aes(x = heights)) +\n  geom_histogram(binwidth = 10, fill = \"lightblue\", color = \"black\")\n# now do the same for females\n\n\nDiscuss with your neighbour and class why the normal distribution is a good model for height data. Is it a good model for all data?\n\n\nHow does your class compare to the Australian statistics. For this we will look at the mean and standard deviation of measured heights for men and women aged 18 - 24 from the ABS for 1995 see page 13 of the pdf\n\nNote that both reported and measured heights are provided and (not surprisingly) reported heights are bigger that the measured heights. What do you think the reason for this is?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-2---milkfat-example",
    "href": "labs/Lab04.html#exercise-2---milkfat-example",
    "title": "Lab 04 – Central limit theorem",
    "section": "Exercise 2 - Milkfat example",
    "text": "Exercise 2 - Milkfat example\n\nPart 1\nThe milkfat content in milk (in %) for 120 cows are presented in the worksheet called ENVX1002_Data4.xlsx. Copy the file into your project directory and:\n\nImport the data into R.\n\n\nlibrary(readxl)\nmilkfat &lt;- read_excel(\"data/ENVX1002_Data4.xlsx\", sheet = \"Milkfat\")\n\n\nCalculate the summary statistics of Milkfat (mean, median and sd)\n\nNote that we use $ColumnName to select a column from the data\n\nmean(milkfat$Milkfat)\n\n\nWhat type of cows could they be? Compare your data to the table in the following link:\n\nhttps://lactalis.com.au/info-center/different-breeds-of-cows/\n\nWhat state could they be from? Check some of the recent Milk Production reports from Dairy Australia. The data can be found in the Average Milkfat & Protein (%) section of the PDF report: The reports can be found at the following link:\n\nhttps://www.dairyaustralia.com.au/resource-repository/2020/09/25/milk-production-report\n\nCould the data be normally distributed?\n\n\nCreate a histogram and boxplot of the milk fat data. Is the data “normally distributed”?\n\n\nrequire(ggplot2)\nggplot(milkfat, aes(x = Milkfat)) +\n  geom_histogram(binwidth = 0.1, fill = \"lightblue\", color = \"black\") +\n  xlab(\"Milkfat (%)\")\n\n\nIn the UK, breakfast milk' (orChannel Island milk’) has 5.5% fat content. What percentage of the cows in this data set is yielding breakfast milk with \\(\\ge\\) 5.5%?\n\n\ns &lt;- sort(milkfat$Milkfat) # Sorts the data\ns # Look at the sorted data\nlength(s[s &gt;= 5.5]) # Counts how many are &gt;= 5.5\n\n\nIn Australia, full cream milk has greater than 3.2% milk fat content. What percentage of these cows is yielding full cream milk?\n\n\n## Your turn\n\n\n\nPart 2\nLet \\(X\\) represent the milk fat content for the population of this breed of cows.\n\nAssuming the population is normal, use the sample mean and standard deviation from the previous question as estimates of the population parameters. So \\(X\\sim (\\mu =..., \\sigma^2 = ...)\\).\nDraw a picture of the curve representing \\(X\\). The below example uses ggplot2 to draw the curve for \\(N(4.16,0.30^2)\\).\n\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(4.16 - 4 * 0.3, 4.16 + 4 * 0.3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.16, sd = 0.30)) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16, 0.30^2) ~ pdf))\n\n\nWhat is the probability that 1 cow has a fat content less than 4%? We will adapt the ggplot command above a picture of this probability and then use R to find the probability.\n\nHint: You may need to use the stat_function command to draw the curve and then use the pnorm command to find the probability.\n\nggplot(data.frame(x = c(4.16 - 4 * 0.3, 4.16 + 4 * 0.3)), aes(x = x)) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = 0.30),\n    geom = \"area\", fill = \"white\"\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = 0.30),\n    xlim = c(4.16 - 4 * 0.3, 4), geom = \"area\", fill = \"red\"\n  ) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16, 0.30^2) ~ pdf))\n\n\npnorm(4, 4.16, 0.30)\n\n\nWhat is the probability that 1 cow (randomly sampled) has a fat content greater than 4.5%? Try and adapt the ggplots above to draw a picture of this probability and then use R to find the probability.\nFor a sample of 10 cows (randomly sampled), what is the probability that the sample mean milk fat content is greater than 4.2%?\n\nHint: First find the distribution of the sample mean \\(\\overline{X}\\). Then find \\(P(\\overline{X}&gt;4.2)\\)\n\nWhat is the probability that the sample mean milk fat content is greater than 4.2%?\n\n\n# For a sample of 10 cows, we need to find P(X_bar &gt; 4.2)\n# Mean of X_bar = population mean = 4.16\n# Standard error of X_bar = population sd / sqrt(n) = 0.30 / sqrt(10)\n\n# Calculate the standard error\nse &lt;- 0.30 / sqrt(10)\n\n# Calculate the probability\npnorm(4.2, 4.16, se, lower.tail = FALSE)\n\n# Visualization\nggplot(data.frame(x = c(4.16 - 4 * se, 4.16 + 4 * se)), aes(x = x)) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = se),\n    geom = \"area\", fill = \"white\"\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = se),\n    xlim = c(4.2, 4.16 + 4 * se), geom = \"area\", fill = \"blue\"\n  ) +\n  xlab(\"Sample mean milk fat content\") +\n  ylab(\"Probability density\") +\n  ggtitle(\"Distribution of sample mean (n=10)\")",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-3---skin-cancer",
    "href": "labs/Lab04.html#exercise-3---skin-cancer",
    "title": "Lab 04 – Central limit theorem",
    "section": "Exercise 3 - Skin cancer",
    "text": "Exercise 3 - Skin cancer\nA dermatologist investigating a certain type of skin cancer induced the cancer in nine rats and then treated them with a new experimental drug. For each rat she recorded the number of hours until remission of the cancer. The rats had a mean remission time of 400 hours and a standard deviation of 30 hours. From this data, calculate the standard error of the mean.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-4---soil-carbon",
    "href": "labs/Lab04.html#exercise-4---soil-carbon",
    "title": "Lab 04 – Central limit theorem",
    "section": "Exercise 4 - Soil carbon",
    "text": "Exercise 4 - Soil carbon\nAn initial soil carbon survey of a farm based on 12 observations found that the sample mean \\(\\overline{X}\\) was 1.2% and the standard deviation s was 0.4%. How many observations would be needed to estimate the mean carbon value with a standard error of 0.1%?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "href": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "title": "Lab 04 – Central limit theorem",
    "section": "Exercise 5 - What’s in the media - looming state election",
    "text": "Exercise 5 - What’s in the media - looming state election\nAn article was published in the Sydney Morning Herald on Saturday 20.3.2010 about statistics related to opinion polls. Read it and find the sentences related to (i) populations versus samples (ii) standard error formula (iii) the effect of sample size on standard errors.\nhttp://www.smh.com.au/national/demystifying-the-dark-art-of-polling-20100319-qmai.html",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-6---extra-practice",
    "href": "labs/Lab04.html#exercise-6---extra-practice",
    "title": "Lab 04 – Central limit theorem",
    "section": "Exercise 6 - Extra practice",
    "text": "Exercise 6 - Extra practice\nThe average Australian woman has height (in cms) of 161.8 with a standard deviation of 6.\n\nThe Australian Institute of Sport ran a netball training camp for the best Australian young players. How tall were the goal position players? http://www.abc.net.au/news/2015-06-14/tall-athletes-get-support-at-ais-to-stand-as-proud-netballers/6544642\nWhat is the probability of finding an Australian woman of this height or taller?\n\nHints:\nStep 1: Using ggplot, draw a sketch of the Normal curve with the probability identified. You may need to draw a section of the right tail as the probability is small! We have provided the solution for the plotting to assist you.\nStep 2: Calculate the probability in R.\n\n1 - pnorm(189, 161.8, 6)\n\nggplot() +\n  stat_function(\n    fun = dnorm, args = list(mean = 161.8, sd = 6),\n    geom = \"area\", fill = \"white\", xlim = c(180, 161.8 + 4 * 6)\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 161.8, sd = 6),\n    geom = \"area\", fill = \"red\", xlim = c(161.8 + 4 * 6, 189)\n  ) +\n  xlab(\"x\") +\n  ylab(expression(N(161.8, 6^2) ~ pdf)) +\n  scale_x_continuous(breaks = 189)\n\n\nDharshani Sivalingam is the tallest netball player in the world. How tall is Dharshani? https://en.wikipedia.org/wiki/Tharjini_Sivalingam What is the probability of finding an Australian woman of Dharshani’s height?\n\n\nMadison Brown is one of the the shortest Australian International players. How tall is Madision? https://en.wikipedia.org/wiki/Madison_Browne What percentage of Australian women are between Madison and Dharshani’s heights?\n\n\nIf 80% of Australian women are above a certain height, what is that height?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Central limit theorem"
    ]
  },
  {
    "objectID": "labs/Lab06.html",
    "href": "labs/Lab06.html",
    "title": "Lab 06 – Two-sample t-test",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 2-sample t-test\n\nindependent samples with constant variance\nindependent samples with unequal variance\npaired samples\ndata transformations\n\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#before-you-begin",
    "href": "labs/Lab06.html#before-you-begin",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-06.Rmd or similar. The following data files are required:\n\nBarley.csv\nPlant_growth.csv\nTurbidity.csv\n\nThe following external packages are used in this lab. Install them if you have not done so already.\ninstall.packages(c(\"tidyverse\", \"car\"), \n  repo = \"https://cloud.r-project.org\")\nFinally, try to complete today’s lab exercises in pairs and try out pair programming, where one person writes the code and the other person reviews each line as it is written. You can swap roles every 10 minutes or so. This is a great way to learn from each other and to improve your coding skills.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-1-barley-walk-through",
    "href": "labs/Lab06.html#exercise-1-barley-walk-through",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 1: barley (walk-through)",
    "text": "Exercise 1: barley (walk-through)\n\nBackground\nAn experiment was designed to compare two varieties of spring barley. Thirty four plots were used, seventeen being randomly allocated to variety A and seventeen to variety B. Unfortunately five plots were destroyed. The yields (t/ha) from the remaining plots were as they appear in the file Barley.csv.\n\n\nInstructions\nFirst, quickly explore the data; then, utilise the HATPC process and test the hypothesis that the two varieties give equal yields, assuming that the samples are independent.\n\n\nHATPC:\n\nHypothesis\nAssumptions\nTest (statistic)\nP-value\nConclusion\n\n\n\n\n\n\n\nLevel of significance\n\n\n\nThe level of significance is usually set at 0.05. This value is generally accepted in the scientific community and is also linked to Type 2 errors, where choosing a lower significance increases the likelihood of failing to reject the null hypothesis when it is false.\n\n\n\n\nData exploration\nFirst we load the data and inspect its structure to see if it needs to be cleaned or transformed. The glimpse() function is a tidy version of str() that provides a quick overview of the data that focuses on the variables, ignoring data attributes.\n\n\nTry to compare str() and glimpse() to see what the differences are.\n\nbarley &lt;- readr::read_csv(\"data/Barley.csv\") # packagename:: before a function lets you access a function without having to load the whole library first\ndplyr::glimpse(barley)\n\nRows: 29\nColumns: 2\n$ Variety &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ Yield   &lt;dbl&gt; 4.6, 4.3, 3.8, 3.4, 3.9, 3.9, 3.9, 4.4, 3.6, 3.6, 4.7, 3.9, 3.…\n\n\nThe Variety column is a factor with two levels, A and B, but it is defined as a character. We can convert it to a factor using the mutate() function from the dplyr package, but it is not necessary for the t-test since R will automatically convert it to a factor.\n\nlibrary(tidyverse)\nbarley &lt;- mutate(barley, Variety = as.factor(Variety))\n\nQuickly preview the data as a plot to see if there are any trends or unusual observations.\n\nbarley %&gt;%\n  ggplot(aes(x = Variety, y = Yield)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nA trained eye will anticipate that the data may not meet the assumption of equal variance; however, we will test this assumption later. Otherwise, there appear to be no unusual observations in the data.\n\n\nHypothesis\nWhat are the null and alternative hypotheses? We can use the following notation:\n\\[H_0: \\mu_A = \\mu_B\\] \\[H_1: \\mu_A \\neq \\mu_B\\]\nwhere \\(\\mu_A\\) and \\(\\mu_B\\) are the population means of varieties A and B, respectively.\n\n\nIt is important that when using mathematical symbols to denote the null and alternative hypotheses, you should always define what the symbols mean. Otherwise, the reader may not understand what you are referring to.\nThe equations above are written in LaTeX, a typesetting system that is commonly used in scientific writing. You can learn more about LaTeX here. The raw syntax used to write the equations are shown below:\n$$H_0: \\mu_A = \\mu_B$$\n$$H_1: \\mu_A \\neq \\mu_B$$\nWhy do we always define the null and alternative hypotheses? In complex research projects or when working in a team, it is important to ensure that everyone is on the same page. By defining the hypotheses, you can avoid misunderstandings and ensure that everyone is working towards the same goal as the mathematical notation is clear and unambiguous.\n\n\nAssumptions\n\nNormality\nThere are many ways to check for normality. Here we will use the QQ-plot. Use of ggplot2 is preferred (as a means of practice) but since we are just exploring data, base R functions are not a problem to use.\n\nUsing ggplot2Using base R\n\n\n\nggplot(barley, aes(sample = Yield)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Variety) #facet_wrap ensures there are separate plots for each variety rather than one plot with all the data in Yield \n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\nqqnorm(barley$Yield[barley$Variety == \"A\"], main = \"Variety A\") # square brackets to subset the data by variety\nqqline(barley$Yield[barley$Variety == \"A\"])\nqqnorm(barley$Yield[barley$Variety == \"B\"], main = \"Variety B\")\nqqline(barley$Yield[barley$Variety == \"B\"])\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Do the plots indicate the data are normally distributed?\nAnswer: Yes, the data appear to be normally distributed as the QQ-plot shows that the data points are close to the line.\n\n\nHomogeneity of variance\nFrom the boxplot, we can see that there is some indication that the variances are not equal. We can test this assumption using Bartlett’s test or Levene’s test; here we will just use Bartlett’s test.\n\nbartlett.test(Yield ~ Variety, data = barley)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Yield by Variety\nBartlett's K-squared = 14.616, df = 1, p-value = 0.0001318\n\n\nQuestion: Does the Bartlett’s test indicate the two groups have equal variances? What effect will that have on the analysis?\nAnswer: The two groups have unequal variance (Bartlett’s test: \\(X^2 = 14.6\\), \\(p &lt; 0.01\\)). This means that we will need to use the Welch’s t-test, which does not assume equal variances.\n\n\n\nTest statistic\nWe can now calculate the test statistic using the t.test() function in R. Since the variances are unequal, we do not have to specify the var.equal argument – the default test for t.test() is the Welch’s t-test which does not assume equal variances.\n\nt.test(Yield ~ Variety, data = barley)\n\n\n    Welch Two Sample t-test\n\ndata:  Yield by Variety\nt = -4.9994, df = 19.441, p-value = 7.458e-05\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -0.9293569 -0.3814274\nsample estimates:\nmean in group A mean in group B \n       4.052941        4.708333 \n\n\n\n\nP-value\nSince the p-value is &lt; 0.05, we can reject the null hypothesis that the mean yield of both varieties is equal.\n\n\nConclusion\nThe conclusion needs to be brought into the context of the study. In a scientific report or paper, you would write something like this:\n\nThe mean yield of barley variety A was significantly different from that of variety B (\\(t = -5.0\\), \\(df = 19.4\\), \\(p &lt; 0.01\\)).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-2-plant-growth",
    "href": "labs/Lab06.html#exercise-2-plant-growth",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 2: plant growth",
    "text": "Exercise 2: plant growth\n\nBackground\nIn a test of a particular treatment aimed at inducing growth, 20 plants were grouped into ten pairs so that the two members of each pair were as similar as possible. One plant of each pair was chosen randomly and treated; the other was left as a control. The increases in height (in centimetres) of plants over a two-week period are given in the file Two week plant heights. We wish to compare whether the treatment is actually inducing improved growth, as compared to the control.\n\n\nInstructions\nHere, we have two samples, and the samples are paired as it is a before-after experiment. So we’d like to conduct a paired t-test.\nFor paired t-tests the analysis is performed as a 1-sample t-test on the difference between each pair so the only assumption is the normality assumption.\nCopy the structure below and perform your analysis in your document.\n## Exercise 2: plant growth\n### Data exploration\n### Hypothesis\n### Assumptions\n#### Normality\n#### Homogeneity of variance\n### Test statistic\n### P-value\n### Conclusion\nNote that the data is not tidy. The code below will convert the data to the long format and assign it to tidy_plant.\n\nplant_growth &lt;- readr::read_csv(\"data/Plant_growth.csv\")\n\ntidy_plant &lt;- plant_growth %&gt;%\n  pivot_longer(cols = c(Treated, Control), names_to = \"Treatment\", values_to = \"Height\")\n\nYou may also need to perform a Shapiro-Wilk test to check for normality. To do this for each group, you can use the tapply() function.\n\ntapply(tidy_plant$Height, tidy_plant$Treatment, shapiro.test)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-3-turbidity",
    "href": "labs/Lab06.html#exercise-3-turbidity",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 3: turbidity",
    "text": "Exercise 3: turbidity\n\nBackground\nA new filtering process was installed at a dam which provided drinking water for a nearby town. To check on its success, a number of water samples were taken at random times and locations in the weeks before and after the process was installed. The following are the turbidity values (units = NTU) of the water samples.\n\n\nInstructions\nNow we consider further examples of a two-sample t-test, but where the assumption of equal variance and normality may not be met for the raw data. Sometimes after applying a data transformation the analysis can proceed assuming equal variances – but always check after a data transformation.\nThe data can be read with the code below:\n\nturbidity &lt;-read_csv(\"data/Turbidity.csv\")\n\nFor data transformation, you may need to create a new variable in your dataset to store the transformed data. For example, to create a new variable TurbLog10 that stores the log10 transformed turbidity values, you can use the following code:\n\nturbidity$TurbLog10 &lt;- log10(turbidity$Turbidity)\n\nTo interpret the results for your conclusions, you may need to back-transform the mean and/or confidence interval values. To back transform log10 data you use:\n\\[10^{\\text{mean or CI}}\\]\nTo back-transform natural log, loge, you use:\n\\[e^{\\text{mean or CI}}\\]",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-1-tooth-growth",
    "href": "labs/Lab06.html#exercise-1-tooth-growth",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 1: Tooth growth",
    "text": "Exercise 1: Tooth growth\nTooth GRowth is an inbuilt dataset that shows the effect of vitamin c in Guinea pig tooth growth. It has three variables:\n\nlen = tooth length\nsupp = type of supplement (Orange juice or ascorbic acid)\ndose = mg/day gieven to the guinea pigs\n\n\nhead(ToothGrowth)\nstr(ToothGrowth)\n\nUsing the HATPC framework, test whether the type of supplent affects tooth length.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-2-adelie-penguin-bill-length",
    "href": "labs/Lab06.html#exercise-2-adelie-penguin-bill-length",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 2: Adelie penguin bill length",
    "text": "Exercise 2: Adelie penguin bill length\nFor this exercise, we will be using a subset of the palmer penguins dataset.\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nadelie &lt;-penguins%&gt;%\n  filter(species == \"Adelie\")%&gt;%\n  na.omit()%&gt;%\n  droplevels()\n\nhead(adelie)\n\nUsing the HATPC framework, test whether male and female penguins have the same length bill",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-3-penguin-body-mass",
    "href": "labs/Lab06.html#exercise-3-penguin-body-mass",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 3: Penguin body mass",
    "text": "Exercise 3: Penguin body mass\nFor this exercise, we will use a subset of the palmer penguins data set again. This time, we will be comparing two different penguin species.\n\nlibrary(palmerpenguins)\n\npenguins2&lt;- penguins%&gt;%\n  filter(species != \"Adelie\")%&gt;%\n  na.omit()%&gt;%\n  droplevels()\n\nUsing the HATPC framework, test whether chinstrap and gentoo penguins have different body masses.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab08.html",
    "href": "labs/Lab08.html",
    "title": "Lab 08 – 🚫 No exercises",
    "section": "",
    "text": "There are no exercises this week to make room for the evaluation task.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 08 -- 🚫 No exercises"
    ]
  },
  {
    "objectID": "labs/Lab10.html",
    "href": "labs/Lab10.html",
    "title": "Lab 10 – Linear functions",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nFit simple linear models and obtain associated model summaries in R\nOverlay fitted models onto scatterplots in R\nUndertake hypothesis testing to determine if slope \\(\\neq\\) 0\nCheck assumptions are met prior to assessing model output\nAssess model summary in terms of fit and P-values",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#before-you-begin",
    "href": "labs/Lab10.html#before-you-begin",
    "title": "Lab 10 – Linear functions",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-10.Rmd or similar. The following data files are required:\n\nENVX1002_wk10_practical_data_Regression.xlsx\n\nLast week you fitted models in R, now it is time to understand what the output means.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon’t forget to save as you go!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-1-walkthrough---fertiliser",
    "href": "labs/Lab10.html#exercise-1-walkthrough---fertiliser",
    "title": "Lab 10 – Linear functions",
    "section": "Exercise 1: Walkthrough - Fertiliser",
    "text": "Exercise 1: Walkthrough - Fertiliser\nLike last week, we will start off our R modelling journey by fitting a model to the fertiliser data.\n\nRead the following code into R:\n\n\n# add the data to R Studio\nfert &lt;- c(0, 1, 2, 3, 4, 5)\nyield &lt;- c(2, 13, 19, 18, 25, 33)\n\n\n1.1 Scatterplot and correlation\nTo visually identify any trends or relationships, last week we created a scatterplot of the data. This is helps us visually understand our points so we know what we might expect from the model, and possibly identify if the relationship is looking non-linear.\n\n# Create a scatterplot\nplot(fert, yield)\n\n\n\n\n\n\n\n\nRemembering back to last week, we then calculated the correlation coefficient to numerically determine whether there was a relationship between fertiliser and yield.\nUsing the code below, we found there was quite a strong relationship between fertiliser and yield (0.964):\n\n# Correlation coefficient\ncor(fert, yield)\n\n[1] 0.9636686\n\n\n\n\n1.2 State hypotheses\nRemembering back to the lecture and tutorial, the general equations for our hypotheses are:\n\\[\nH_0 : \\beta_1 = 0\n\\]\n\\[\nH_1 : \\beta_1 \\neq 0\n\\]\nIn the context of our data, the hypotheses would be:\n\\(H_0\\): Slope = 0; fertiliser is not a significant predictor of yield.\n\\(H_1\\): Slope \\(\\neq\\) 0; fertiliser is a significant predictor of yield.\nIf P &gt; 0.05, we fail to reject the null hypothesis that the true slope (\\(\\beta_1\\)) is equal to 0. If this is the case, it means our model does not predict better than the mean of our observations, and so there is no advantage to using our model over the mean of y (\\(\\bar{y}\\)).\nIf we find there is a high probability of the slope not being equal to 0 (P &lt; 0.05), we can reject the null hypothesis and conclude our model is better at predicting than the mean of our observations.\nNow we understand what we are testing for, we can fit the model.\n\n\n1.3 Fit the model\nAfter checking the correlations and scatterplot, we need to fit the model using the lm() function. Remember to tell R the name of the object you want to store it as (in this case, model.lm &lt;-), then state the name of the function. The arguments within the function (i.e. between the brackets) will be yield ~ fert, with yield being the response variable and fert being the predictor.\n\n# Run your model\n## yield = response variable (x)\n## fert = predictor variable (y)\nmodel.lm &lt;- lm(yield ~ fert)\n\n\n\n1.4 Check assumptions\nThis time, before obtaining our model summary, we need to check our assumptions.\nSmaller sample size (n = 6) makes it harder to check whether the assumptions have been met, but we will still run through the check.\nLooking at each plot, we can see that the residual plots don’t look the best;\n\nResiduals vs fitted: Will tell us if the relationship is linear. We are looking for an even scatter around the mean, and red line should be reasonably straight. In this case the red line is not too straight, but the scatter seems even.\nNormal Q-Q: If the residuals are normally distributed, most of the points should lie along the dotted line. Our points follow the line, but do not lie on it.\nScale-Location: This is for testing whether the variance is equal in the residuals at each value of x. If the variance is equal, then we would expect to see an even scatter and no fanning. In this case, there is no fanning.\nResiduals vs Leverage: This will help us identify whether there are any single points influencing the slope or intercept of the model. We can see in the output plot there is a point sitting in the bottom-right corner, outside the dotted line, indicating that it may be having an influence on the model.\n\nThese plots are only useful as an example of how to obtain and interpret output. If we wanted to obtain a more reliable check of our assumptions (and a more reliable model), we would need a larger sample size (n &gt; 10).\n\n# Check your assumptions!!\npar(mfrow = c(2, 2)) # sets plots to show as 2x2 grid\nplot(model.lm)\n\n\n\n\n\n\n\n\nIn this case, we will assume the assumptions have been met and continue to assess the model output.\n\n\n1.5 Model output\nUse the summary() function to obtain output for your model:\n\n# Obtain model summary\nsummary(model.lm)\n\n\nCall:\nlm(formula = yield ~ fert)\n\nResiduals:\n     1      2      3      4      5      6 \n-2.762  2.810  3.381 -3.048 -1.476  1.095 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   4.7619     2.2778   2.091  0.10476   \nfert          5.4286     0.7523   7.216  0.00196 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.147 on 4 degrees of freedom\nMultiple R-squared:  0.9287,    Adjusted R-squared:  0.9108 \nF-statistic: 52.07 on 1 and 4 DF,  p-value: 0.001956\n\n\nIn the model output obtained from summary(model.lm) the model parameters will be listed under ‘Estimate’ for the intercept and ‘fert’. Last week we concluded the equation to be:\n\\[\nYield = 4.7619 + 5.4286*fert\n\\]\nFurthermore, from our model estimate, we can say that as fertiliser increases by 1, yield will increase by 5.4286.\n\n\n1.6 Is the model useful?\nWhen looking at the model summary output, we obtain the p-value from the coefficients table. We are interested in the P-value for fert and not the intercept.\nThe significance of the intercept P-value depends on our scientific question. We only really look at our intercept P-value when we want to extrapolate our line to the intercept, and know if the intercept is equal to zero (\\(H_0\\)) or not (\\(H_1\\)). This depends on your dataset and whether it makes sense to do so.\nAlso notice how the p-values for the F-test at the bottom of the summary output, and the t-test p-values we are using are the same. The F-test gives us an idea whether our overall model is significant and in this case, as we are only using a single predictor, the P-values will be the same.\nTherefore we can say the following:\nObserving the model output, we can see that the P-value for fert is significant (P = 0.00196) and we can say that as P &lt; 0.05, we reject the null hypothesis. We can conclude our slope is not 0 and our model is a better way to predict yield than the mean of our observations.\n\n\n1.7 How good is the model?\nTo assess how well the model fits the data, we need to look at the Residual standard error (3.147) and the r-squared value (0.9287).\n\nWe can say that our residual standard error is relatively low in terms of our response variable.\nOur r-squared indicates that fertiliser accounts for 92.9% of variation in yield. That’s pretty good!\n\nNote how Multiple R-squared and Adjusted R-squared are similar. For simple linear models we can opt for the multiple r-squared, but when using multiple predictors we need to use adjusted r-squared.\nFinally, to visually present our results, we can provide a scatterplot with the model overlaid.\n\n# Add the linear model to your scatterplot\nplot(fert, yield, xlab = \"fertiliser applied\", ylab = \"Yield\")\nabline(model.lm, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n1.8 Our conclusions\nNow we can put our interpretations together to form the conclusion:\nObserving the model output, we can see that the P-value for fert is significant (P = 0.00196) and we can say that as P &lt; 0.05, we reject the null hypothesis. We can conclude our slope is not 0 and our model is a better way to predict yield than the mean of our observations.\nWe can therefore conclude that fertiliser is a significant predictor of crop yield as the slope is not equal to zero (P &lt; 0.05), and it accounts for 92.9% of the variation in yield.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-2-toxicity-in-peanuts",
    "href": "labs/Lab10.html#exercise-2-toxicity-in-peanuts",
    "title": "Lab 10 – Linear functions",
    "section": "Exercise 2: Toxicity in peanuts",
    "text": "Exercise 2: Toxicity in peanuts\nData: Peanuts spreadsheet\nThe data comprise of, for 34 batches, the average level of the fungal contaminant aflatoxin in a sample of 120 pounds of peanuts and the percentage of non-contaminated peanuts in the whole batch.\nThe data were collected with the aim of being able to predict the percentage of non-contaminated peanuts (Peanuts$percent) from the aflatoxin level (Peanuts$toxin) in a sample. We will now investigate whether this is the case.\nFirst thing’s first! Let’s read in the data using read_xlsx command:\n\nlibrary(readxl)\nPeanuts &lt;- read_xlsx(\"data/ENVX1002_wk10_practical_data_Regression.xlsx\", sheet = \"Peanuts\")\nhead(Peanuts)\n\n# A tibble: 6 × 2\n  Percent Toxin\n    &lt;dbl&gt; &lt;dbl&gt;\n1    100.   3  \n2    100.   4.7\n3    100.   8.3\n4    100.   9.3\n5    100.   9.9\n6    100.  11  \n\n\n\n2.1 Scatter plot\nMake a scatter plot of the data.\n\nplot(Percent ~ Toxin, data = Peanuts)\n\n\n\n\n\n\n\n#Alternate syntax:\n#plot(Peanuts$Percent, Peanuts$Toxin)\n\n\nDescribe the relationship between the two variables.\n\n\nWould you say that the percentage of non-contaminated peanuts in a batch could be predicted accurately from the level of aflatoxin in a sample via a linear relationship?\n\n\n\n2.2 State Hypotheses\n\nWhat are the hypotheses we are testing? State them as the formulae and in the context of the study.\n\n\n\n2.3 Fit a linear model\nUse fit a linear model (lm()) to the Peanut data.\n\n# fit a linear model using lm()\nmod &lt;- lm(Percent ~ Toxin, data = Peanuts)\n\n\n\n2.4 Check assumptions\n\nInspect and comment on the residual plots- have the assumptions been met?\n\n\npar(mfrow = c(2, 2))\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n2.5 Observe model output\nOnce you are certain the assumptions are met, you can proceed to look at the regression output.\n\nComment on the overall fit of the regression, i.e. Is the model fit good? Is the model significant, and how much variation in percentage of non-contaminated peanuts does aflatoxin level account for?\n\n\n# Look at output with summary\nsummary(mod)\n\n\nCall:\nlm(formula = Percent ~ Toxin, data = Peanuts)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.076516 -0.020012 -0.004806  0.027094  0.073747 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.000e+02  1.089e-02 9184.91  &lt; 2e-16 ***\nToxin       -2.903e-03  2.335e-04  -12.44 8.54e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03933 on 32 degrees of freedom\nMultiple R-squared:  0.8285,    Adjusted R-squared:  0.8232 \nF-statistic: 154.6 on 1 and 32 DF,  p-value: 8.538e-14\n\n\n\nIs toxin a significant predictor of percentage non-contaminated peanuts? If so, how can we tell?\n\n\nInterpret the slope parameter in terms of quantifying the relationship between toxin and percent.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-3-dippers",
    "href": "labs/Lab10.html#exercise-3-dippers",
    "title": "Lab 10 – Linear functions",
    "section": "Exercise 3: Dippers",
    "text": "Exercise 3: Dippers\nData: Dippers spreadsheet\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\nTwenty-two sites were included in the survey. For the purpose of fitting a simple linear model, the dataset has been reduced to two variables:\n\nThe number of breeding pairs of Dippers per 10 km of river\nThe numbers of caddis fly larvae (Log(Number+1) transformed)\n\nNow it is your turn to work through the steps as above. Does the number of caddis fly larvae influence the number of breeding pairs of Dippers?\n\nRead in the data from today’s Excel sheet, the corresponding sheet name is “Dippers”\n\n\nObtain a scatterplot, are there signs of a relationship between breeding pair density and caddis fly larvae?\n\n\nWhat are the hypotheses we are testing? State them as the formulae and in the context of the study.\n\n\nLet’s investigate further. Run the model, but before looking at our model output, are the assumptions ok?\n\n\n# Run model\ndipper.lm &lt;- lm(Br_Dens ~ LogCadd, data = Dippers)\n\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(dipper.lm)\n\n\n\n\n\n\n\n\n\nOnce you are happy assumptions are good, you can use summary() to interpret the model output.\n\n\nWhat is the equation for our model, incorporating our coefficients?\n\n\nBased on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd?\n\n\nIs LogCadd a significant predictor of Dipper breeding pair density? How can we tell?\n\n\nHow good is the fit of our model?\n\n\nWhat conclusions can we make from this model output?\n\n\nA final thought; Does our result make sense within the context? i.e. why might the Dipper breeding pair density be related to LogCadd?\n\nGreat work fitting simple linear models! Next week we step it up with multiple linear regression.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-1-cars-stopping-distance",
    "href": "labs/Lab10.html#exercise-1-cars-stopping-distance",
    "title": "Lab 10 – Linear functions",
    "section": "Exercise 1: Cars stopping distance",
    "text": "Exercise 1: Cars stopping distance\nUse the cars dataset from last week to test if speed (mph) is a predictor of dist (stopping distance, fft).\n\nhead(cars)\n\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-2-penguins",
    "href": "labs/Lab10.html#exercise-2-penguins",
    "title": "Lab 10 – Linear functions",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nUse the palmer penguins dataset to test if flipper_length is a significant predictor of bill_length.\n\n#Load libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#Clean data\npenguins &lt;- penguins%&gt;%\n  na.omit()#remove missing data\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           36.7          19.3               193        3450\n5 Adelie  Torgersen           39.3          20.6               190        3650\n6 Adelie  Torgersen           38.9          17.8               181        3625\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-3-old-faithful-geyser-data",
    "href": "labs/Lab10.html#exercise-3-old-faithful-geyser-data",
    "title": "Lab 10 – Linear functions",
    "section": "Exercise 3: Old Faithful Geyser Data",
    "text": "Exercise 3: Old Faithful Geyser Data\nUsing the inbuilt faithful dataset, test whether waiting time (waiting) is a significant predictor of eruption time (eruption).\n\nhead(faithful)\n\n  eruptions waiting\n1     3.600      79\n2     1.800      54\n3     3.333      74\n4     2.283      62\n5     4.533      85\n6     2.883      55",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 -- Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab12.html",
    "href": "labs/Lab12.html",
    "title": "Lab 12 - Non-linear models",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nCalculate “by hand” the initial estimates of the parameters of a non-linear model\nInterpret tables of regression coefficients for polynomials to perform hypothesis testing\nFit polynomials and non-linear models to data using least-squares fitting using the SOLVER add-in in Excel\nFit polynomials and non-linear models to data in R, and interpret the outputs",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#before-we-begin",
    "href": "labs/Lab12.html#before-we-begin",
    "title": "Lab 12 - Non-linear models",
    "section": "Before we begin",
    "text": "Before we begin\nCreate your Quarto document and save it as Lab-12.qmd or similar.\nThe following data files are required:\n\neast_creek.csv\n\nOver the past few weeks you have explored linear models and how to interpret model summary output. Again we have stepped up the complexity, now venturing into the world of non-linear models.\nThis practical focuses on fitting non-linear models to data with an emphasis on 3 important classes of functions that all budding biologists and environmental scientists should know\n\npolynomials,\nexponential models, and\nlogistic models.\n\nA question before we begin:\nWhat are some advantages and disadvantages of non-linear models as compared to polynomials?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#polynomials",
    "href": "labs/Lab12.html#polynomials",
    "title": "Lab 12 - Non-linear models",
    "section": "Polynomials",
    "text": "Polynomials\n\nQuadratic\n\\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nwhere the parameters are the y-intercept (b0), the linear component (b1) and the quadratic component (b2).\nIf b2 is negative then the shape of the function is convex upwards, i.e. y increases with x until reaches a peak and then y decreases.\nIt is easy to understand so it has been commonly used for modelling the response of yield to inputs such as fertiliser, seeding rates. This is despite much criticism for being unrealistic.\nLimitations:\n\nrate of increase to peak is same as rate of decrease past peak\ndoes not level off as x becomes small or very large, y just keeps increasing or decreasing.\n\nCubic\n\\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nCompared to the quadratic model which has 1 turning point, a cubic model has 2 turning points.\n\n\n\nExercise 1: Interpreting polynomials\nA study was performed to examine the soil properties that control the within-field variation in crop yield. The focus of this question is on soil pH which (among other things) controls the availability of nutrients to plants.\nThis exercise does not require you to read in any data, but rather focus on interpreting the model output and comparing the models.\nThe figure below shows the raw observations of yield plotted against pH with three models fitted; a linear model, quadratic polynomial and a cubic polynomial.\n\nwhich line corresponds to which model?\n\n\n\n\n\n\n\n\n\n\n\n\nbased on the output from the 3 models below, which model fits the data best? Note: no hypothesis testing yet, just how well the model fits the data (R2).\n\n\n\nLinear model:\n\n\n\n\nCall:\nlm(formula = yield ~ ph, data = soil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0027 -0.5540  0.2189  0.7643  1.9424 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  7.08842    3.05279   2.322   0.0221 *\nph          -0.05249    0.40438  -0.130   0.8970  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.137 on 108 degrees of freedom\nMultiple R-squared:  0.000156,  Adjusted R-squared:  -0.009102 \nF-statistic: 0.01685 on 1 and 108 DF,  p-value: 0.897\n\n\n\nQuadratic model:\n\n\n\n\nCall:\nlm(formula = yield ~ ph + I(ph^2), data = soil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5010 -0.4380 -0.0099  0.5871  1.7064 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -299.9031    54.2430  -5.529 2.30e-07 ***\nph            83.1948    14.6954   5.661 1.27e-07 ***\nI(ph^2)       -5.6336     0.9942  -5.667 1.24e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.002 on 107 degrees of freedom\nMultiple R-squared:  0.2309,    Adjusted R-squared:  0.2166 \nF-statistic: 16.07 on 2 and 107 DF,  p-value: 7.921e-07\n\n\n\nCubic model:\n\n\n\n\nCall:\nlm(formula = yield ~ ph + I(ph^2) + I(ph^3), data = soil)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3734 -0.4540  0.0526  0.5171  1.9561 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 1719.020   1082.451   1.588   0.1152  \nph          -745.066    443.767  -1.679   0.0961 .\nI(ph^2)      107.461     60.570   1.774   0.0789 .\nI(ph^3)       -5.140      2.752  -1.867   0.0646 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9908 on 106 degrees of freedom\nMultiple R-squared:  0.2554,    Adjusted R-squared:  0.2344 \nF-statistic: 12.12 on 3 and 106 DF,  p-value: 6.971e-07\n\n\n\n\nUse the R output to perform hypothesis testing to find the best model. Write out the hypotheses you are testing.\n\n\n\n\nExercise 2: Fitting polynomials in R\nThis exercise will use real data from a yield-fertiliser trial in Bedfordshire, United Kingdom.\nFirst thing we can do is fit a linear model to the fertiliser data:\n\n# create fertiliser and yield objects\nfert &lt;- c(0, 100, 170, 225)\nyield &lt;- c(3.32, 5.23, 5.41, 5.02)\n\n# Fits a linear model and saves it to an object called lin.mod\nlin.mod &lt;- lm(yield ~ fert)\n\n# Summarises key features of model\nsummary(lin.mod)\n\n\nCall:\nlm(formula = yield ~ fert)\n\nResiduals:\n      1       2       3       4 \n-0.4469  0.6727  0.2995 -0.5252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 3.766929   0.634765   5.934   0.0272 *\nfert        0.007904   0.004243   1.863   0.2035  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7134 on 2 degrees of freedom\nMultiple R-squared:  0.6344,    Adjusted R-squared:  0.4515 \nF-statistic:  3.47 on 1 and 2 DF,  p-value: 0.2035\n\n\n\nWhat is the model fit like in this model?\n\nFit and plot a quadratic polynomial in R. In R a quadratic polynomial can be fitted using the following lines of code:\n\n# create a new variable which is the square of the fertilizer rates\nfert2 &lt;- fert^2\n\n# fit the quadratic model incorporating fert2\nquad.mod &lt;- lm(yield ~ fert + fert2)\n\nsummary(quad.mod)\n\n\nCall:\nlm(formula = yield ~ fert + fert2)\n\nResiduals:\n        1         2         3         4 \n-0.005611  0.024528 -0.032791  0.013874 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  3.326e+00  4.324e-02   76.92  0.00828 **\nfert         2.786e-02  9.014e-04   30.91  0.02059 * \nfert2       -9.064e-05  3.921e-06  -23.12  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0436 on 1 degrees of freedom\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.998 \nF-statistic: 731.7 on 2 and 1 DF,  p-value: 0.02613\n\n\n\nWhat is the fit like for our quadratic model? is it better than our linear model?\n\nIn Excel it is easy to fit a line, by creating a scatterplot, then add Trendline… and selecting Polynomial (2nd order).\nTo fit our polynomial line in R, we need to obtain model predictions first.\nTo plot model predictions you first need to predict at fine intervals of the predictor to make a continuous plot that is not jagged or stepped. To create a new prediction dataset you can use the following code:\n\n# creates a sequence of numbers from 0 to 225 going up in increments of 1\nnew.fert &lt;- seq(0, 225, 1)\n\nWe can use our model to predict at the values in the new prediction dataset, in this case new.fert.\n\nnew.pred &lt;- predict(quad.mod, list(fert = new.fert, fert2 = new.fert^2))\n\nThe general form of the predict function is predict(model object, list object).\nThe list object tells R what object contains the data we will use to predict. For example in our case the model was built on fert and fert2 so we have to tell the predict function what object contains the new values for each of these, in our case new.fert.\nNow we plot the raw observation as points and add an overlay of the model fit as lines:\n\nplot(fert, yield, xlab = \"Fertilizer\", ylab = \"Yield\")\nlines(new.fert, new.pred) # Adds lines to original plot",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#exponential-function",
    "href": "labs/Lab12.html#exponential-function",
    "title": "Lab 12 - Non-linear models",
    "section": "Exponential function",
    "text": "Exponential function\n\n\\(y=y_0e^{kx}\\)\nwhere the parameters are y0 which is the multiplier which expresses the starting or final value, and k which is negative for exponential decay and positive for the exponential growth.\nThe half life (for decay) or doubling time (for growth) can be calculated as\n\\(\\frac{log_e 2}{k}\\)\nLimitations:\n\nharder to fit than polynomials\nexponential growth has no horizontal asymptote; keeps going up.\n\n\n\n\nExercise 3: Initial estimates for exponential growth function\nIn this exercise we will find initial estimates of the parameters of an exponential growth model by visual assessment of plots of the data and/or rough calculations. The initial estimates of the parameters are needed as starting points for the iterative fitting methods we will use in the practicals, e.g. SOLVER in Excel and the nls() function in R.\nThe plot and table below presents the population of the world from 1650-1965.\nWe wish to model the data with an exponential growth function of the form;\n\\(y=y_0e^{kx}\\)\nwhere\n\ny is the population in the year x,\ny0 is the population in 1650 and\nk is the rate constant.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n1650\n1750\n1804\n1850\n1900\n1927\n1950\n1955\n1960\n\n\nPop (billions)\n0.5\n0.7\n1\n1.2\n1.6\n2\n2.55\n2.88\n3\n\n\n\n\n\n\n\nProvide an initial estimate of y0.\n\nThe parameter k can be estimated from a linear model fitted to loge population against year.\nRather than formally fitting a linear model you could estimate the slope approximately by using the smallest and largest value to estimate the slope and therefore k.\n\nUse this approach to estimate k.\n\nHint: \\[\nslope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}\n\\]\n\nFor an exponential growth model the doubling time of a population can be estimated by loge2 /k.\n\nExamine the graph and/or table to estimate the doubling time and use this to estimate k. You will have to make k the subject in the equation for estimating the doubling time.\n\nHow similar were the estimates of k?\n\n\n\n\nExercise 4 : Exponential growth models\nThis data is from Jenkins & Adams (2010) who studied soil respiration rates against temperature for different vegetation communities in the Snowy Mountains. They fitted an exponential growth model to the data.\nThe purpose of this exercise is to illustrate the dangers of using Excel’s in-built functions for statistics more complex than calculating means and fitting simple models.\nPlot the data in Excel and using the Add Trendline… option. Make sure tick the option for displaying the equation in the graph.\n\nThe researchers performed the experiment up to a temperature of 40 degrees C, would you expect exponential growth in the respiration rate to continue if high temperatures were considered? Is there a better model?\n\nNow fit the same model in R using the nls function. Code to get you started is:\n\ntemp&lt;-c(5,10,20,25,30,35,40)\nrespiration&lt;-c(1,2,4,6,8,11,18)\n\n##Initial parameters\nexp.mod&lt;-c(y0=1.0,k=0.1)\n\n##Fits exponential model\nres.exp&lt;-nls(respiration ~ y0 * exp(k*temp), start=exp.mod,trace=T)\n\n2025.224    (3.23e+01): par = (1 0.1)\n90.64319    (7.34e+00): par = (0.6559559 0.09162898)\n2.841421    (9.49e-01): par = (0.7302258 0.08091595)\n1.488431    (5.63e-02): par = (0.7944381 0.07731867)\n1.483685    (1.52e-03): par = (0.8033809 0.07707711)\n1.483681    (4.12e-05): par = (0.8036903 0.07706805)\n1.483681    (1.68e-06): par = (0.8037011 0.07706768)\n\n##Summarise model\nsummary(res.exp)\n\n\nFormula: respiration ~ y0 * exp(k * temp)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \ny0 0.803701   0.118632   6.775  0.00107 ** \nk  0.077068   0.004048  19.037 7.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5447 on 5 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 1.681e-06\n\n\n\nNow you can fit a line to the plot. Does this look similar to your trendline in Excel?\n\n\n#Plots raw data\nplot(temp,respiration,xlab='Temperature',ylab='Respiration')\n\n#Creates new dataset for predictions ( 5 to 40 at an interval of 1)\ntemp.new&lt;-seq(5,40,1)\n\n#Makes predictions onto temp.new\npred.exp&lt;-predict(res.exp,list(temp=temp.new))\n\n#Adds model fit to existing plots\nlines (temp.new,pred.exp)\n\n\n\n\n\n\n\n\nCompare the parameters values between Excel and R. You can extract the RSS value from an nls object by using the code below:\n\ndeviance(res.exp)\n\n[1] 1.483681\n\n\n\nCalculate the RSS value for the Excel exponential model. Based on this, which is the better model?\n\nWhen faced with the need to fit an exponential function, one approach that was used before computing power became readily accessible was to log the y values which linearises the relationship with x, enabling the modeller to use a simple linear model.\nIf we linearise, model would be \\(log_e(y) = b_0 + b_1x\\), where \\(e^{b_0}\\) is the y0 parameter in an exponential model, and b1 is the k parameter in the exponential model. This is similar to what was demonstrated in the lecture this week.\nIn Excel, log the soil respiration data and fit a linear model. You will see that the fitted model gives the same values as the exponential model fitted to the untransformed data.\nIf you compare the R2 values for both you will see they are the same. This means that Excel reports the R2 of the linear model fitted to logged respiration as the R2 of the exponential model fitted to the raw data. This is naughty of Excel.\n\nFor the dataset used here the exponential model fits it so well the Excel approach is only slightly different to the correct approach used in R.\nIn cases where the model does not fit the data so well the differences would be larger. Logarithm makes smaller values larger and larger values smaller.\nWHY IS THIS SUB-OPTIMAL?\n\nRegression modelling assumes that the residuals are normally distributed so logging normally distributed data will change the distribution to a non-normal one – it is best to analyse the data without transformation.\nModelling data on the logged scale reduces the impact that larger values have on minimising the RSS but when you plot the fitted model with the original data you may observe large discrepancies for larger values. In other words, using a linear model on the log(data) can result in a higher discrepancy for larger values when plotting the fitted model on the original data. Therefore, the model fitted to the logged data is not necessarily the best on the original data.\nThe reporting of the R-squared on the logged data on a model purported to be fitted to untransformed data is just wrong as the R-squared on the log scale will be better as the variation in the data has been reduced but we really want to know how well an exponential model fits the raw data.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#logistic-function",
    "href": "labs/Lab12.html#logistic-function",
    "title": "Lab 12 - Non-linear models",
    "section": "Logistic function",
    "text": "Logistic function\n\nThere are several versions of the logistic function. We will use the following:\n\\[ y = \\frac{Asym}{1+e^{\\frac{xmid-x}{scal}}} \\]\nwhere\n\n\\(Asym\\) is the maximum value of \\(y\\) (upper limit, horizontal asymptote).\n\\(xmid\\) is the value of \\(x\\) when \\(y\\) is halfway between the lower and upper limits (inflection point, \\(y = 0.5 \\times Asym\\)).\n\\(scal\\) is the rate of change: the rate at which \\(y\\) approaches the upper limit.\n\nCommonly used to model growth that has a sigmoid shape, i.e. where growth is initially slow, then picks up to a maximum, then slows down again as the system reaches a maximum.\nLimitations:\nHarder to fit than polynomials.\n\n\n\nExercise 5: Logistic models\nIn this exercise you will model the yield of pasture over time (since sowing).\nNote, we will assume the yield at sowing = 0 is 0 t/ha, which allows us to use the equation above (that is pre-defined in SSlogis()).\nIf this were not the case, we would need to use a slightly different equation; \\(y = y_0 + \\frac{Asym}{1+e^{-scal(x-xmid)}}\\), where \\(y_0\\) is the yield at sowing.\n\nFit the model in R using the nls function. Estimate the starting\nparameters manually (using the previous exercise and the equation above as a guide). Then use the SSlogis() function to automatically estimate the parameters (see L12, Tut12 or the handbook).\n\n\nPlot the fitted model with the observations.\n\n\nCompare the final model parameters when we provide starting estimates, and when we use SSlogis(). Are they the same?\n\n\nSTARTING VALUES\nWhen fitting non-linear functions (i.e. logistic or exponential) using iterative procedures such as nls or SOLVER the starting estimates of the parameters need to be approximated. If the values are too far, the model will not run and return an error.\nThe best way to ensure that you have suitable starting values is to plot the data with the predictions overlaid for your starting parameters. You can then see how close your initial model is to the data.\nThe reason we go through the process of estimating parameters is because nls and SOLVER can fit any nonlinear equation. They are more versatile\nRealistically, most nonlinear relationships you would fit are covered by a self-starting function; i.e. SSlogis(), SSasymp(), or nlraa::SSexpf(). These estimate the parameters for you and are more efficient to run. We recommend using these when suitable.\n\n\nThat’s it for Module 3! Great work exploring non-linear models!\nThank you all (students and demonstrators!) for your hard work and enthusiasm throughout this Module. Good luck with Project 3 and the final exam!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html",
    "href": "module02/01-ttest1.html",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.\n\n\n\n(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci\n\n\n\n\nThere are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\nqt(0.975, 24)\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).\n\n\n\n\nIf an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.\n\n\n\nAs you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\nkurtosis(bcc_df$BloodCellCount)\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\nkurtosis(bcc_df$log_BloodCellCount)\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\nkurtosis(tccb_df$log_TcCB_ppb)\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.\n\n\n\n\nSometimes research questions are framed not as “What is a plausible range of values for such and such a parameter?” but rather “Are the data consistent with this particular value for the parameter?”. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the “health” of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don’t - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause , the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g. in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou’ll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).\n\n\n\n\n\n\nThe liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e. (population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e. more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let’s say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).\n\n\n\n\nOften researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case…\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!\n\n\n\n\n\nFor the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e. we assume we don’t know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs. \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the “old school” t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R’s t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has “heavier” tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).\n\n\n\n\nHypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI’s.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance – the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI’s as they are more informative than the p-value alone. For example, the width of the CI’s says something about the precision of the estimate.\n\n\n\nExample\nThere is evidence that total nitrogen levels in the river – like many other environmental quality data – are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical “smiley” shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n\nshapiro.test(tn$TN)\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\nWe can use the mean of this transformed data i.e. the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nshapiro.test(tn$log10_TN)\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet’s test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-intervals-for-mu",
    "href": "module02/01-ttest1.html#confidence-intervals-for-mu",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "href": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "href": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "There are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\nqt(0.975, 24)\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#what-is-meant-by-confidence-interval",
    "href": "module02/01-ttest1.html#what-is-meant-by-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "If an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "href": "module02/01-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "As you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\nkurtosis(bcc_df$BloodCellCount)\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\nkurtosis(bcc_df$log_BloodCellCount)\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\nkurtosis(tccb_df$log_TcCB_ppb)\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#hypothesis-testing",
    "href": "module02/01-ttest1.html#hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Sometimes research questions are framed not as “What is a plausible range of values for such and such a parameter?” but rather “Are the data consistent with this particular value for the parameter?”. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the “health” of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don’t - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause , the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g. in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou’ll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-z-tests",
    "href": "module02/01-ttest1.html#one-sample-z-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "The liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e. (population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e. more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let’s say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#general-notes-on-hypothesis-testing",
    "href": "module02/01-ttest1.html#general-notes-on-hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Often researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case…\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-t-tests",
    "href": "module02/01-ttest1.html#one-sample-t-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "For the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e. we assume we don’t know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs. \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the “old school” t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R’s t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has “heavier” tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "href": "module02/01-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Hypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI’s.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance – the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI’s as they are more informative than the p-value alone. For example, the width of the CI’s says something about the precision of the estimate.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-t-test-with-data-transformation",
    "href": "module02/01-ttest1.html#one-sample-t-test-with-data-transformation",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Example\nThere is evidence that total nitrogen levels in the river – like many other environmental quality data – are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical “smiley” shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n\nshapiro.test(tn$TN)\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\nWe can use the mean of this transformed data i.e. the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nshapiro.test(tn$log10_TN)\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet’s test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/03-nonparametric1.html",
    "href": "module02/03-nonparametric1.html",
    "title": "Chi-squared tests",
    "section": "",
    "text": "The chi-squared distribution (where chi is pronounced ‘ky’) is a very widely used distribution in statistics. Its symbol is \\(\\chi^2\\). It has MANY applications. Here we will consider only two of these applications – tests of agreement with expected outcomes, and contingency tables.\n\n\nThe density function for a \\(\\chi^2\\) distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the \\(\\chi^2\\) density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g. 1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the \\(\\chi^2\\) distribution approaches the shape (and properties) of a normal distribution.\n\n\n\nFigure 7.1 The shape of the \\(\\chi^2\\) density function for various degrees of freedom.\n\n\nThe mean and variance of a \\(\\chi^2\\) variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as \\(\\nu\\) (Greek n), then\n\nMean of \\(\\chi^2\\) variable = \\(\\nu\\) (i.e. mean = df)\nVariance of \\(\\chi^2\\) variable = \\(2\\nu\\) (i.e. variance = twice the df)\n\nCritical values of a \\(\\chi^2\\) distribution are given in the \\(\\chi^2\\) probability table that appears as Appendix A.5.\n\n\n\n\n\nThe process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\nChoose an appropriate hypothesis test for the type of data you have, and the type of question you’re asking.\nChoose the level of significance for the test.\nWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\nCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\nCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic – however there are alternative tests designed to accommodate these situations.\nCalculate the test statistic and degrees of freedom.\nObtain the P-value.\nDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n\n\n\nEXAMPLE 1\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a “fair” coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a “fair” coin.\nEXAMPLE 2 (from Mead et al, 2003)\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\n\n\n\nCheese\nFirst preference\n\n\n\n\nA\n5\n\n\nB\n7\n\n\nC\n18\n\n\nD\n10\n\n\nTotal\n40\n\n\n\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n\\[\\chi^2_{obs}=\\frac{(5-10)^2}{10}+ \\frac{(7-10)^2}{10}+ \\frac{(10-10)^2}{10}  =9.80\\]\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the \\(\\chi^2\\) distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the \\(\\chi^2_{obs}\\) value of 9.80 is greater than the \\(\\chi^2_{crit}\\) value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since \\(P &lt; 0.05\\))\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with\n\\[ \\chi^2_{obs}=\\frac{(18-10)^2}{10}+ \\frac{(22-30)^2}{30}  =…\\text{etc.}\\]\n\n\n\n(from Mead et al, 2003)\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen).\nThe figures obtained for the primula plants follow.\n\n\n\n\nNormal eye\nPrimrose Queen eye\nTotal\n\n\n\n\nFlat leaves\n328\n122\n450\n\n\nCrimped leaves\n77\n33\n110\n\n\nTotal\n405\n115\n560\n\n\n\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, 3/4 of the total 560 observation in the first class of the characteristic and the remaining 1/4 in the second class. Further, this model predicts that 3/4 of the flat-leaved plants should have normal eyes, resulting in 3/4 \\(\\times\\) 3/4 of all the plants or 9/16 with flat leaves and normal eyes; the remaining 3/4 of the flat-leaved plants, which is 1/4 \\(\\times\\) 3/4or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes.\nThe calculation of these expected or predicted proportions is shown below.\n\n\n\n\nNormal eye\nPrimrose Queen eye\n\n\n\n\nFlat leaves\n3/4 x 3/4 = 9/16\n1/4 x 3/4 = 3/16\n\n\nCrimped leaves\n3/4 x 1/4 = 3/16\n1/4 x 1/4 = 1/16\n\n\n\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35.\nThe observed and expected frequencies are summarized in the table below.\n\n\n\n\nNormal eye\nPrimrose Queen eye\n\n\n\n\nFlat leaves\n328 (315)\n122 (105)\n\n\nCrimped leaves\n77 (105)\n33 (35)\n\n\n\n\\[\n\\begin{split}\n\\chi^2_{obs} &= \\frac{(328-315)^2}{315}+ \\frac{(122+105)^2}{105}+ \\frac{(77-105)^2}{105}+ \\frac{(33-35)^2}{35}  \\\\\n&= 0.54 + 2.75 + 7.47 + 0.11 \\\\\n&= 10.77.\n\\end{split}\n\\] We compare 10.77 with the 5% point of the \\(\\chi^2\\) distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable.\nSee pp. 332-333 of Mead et al, 2003 for what to do next… after rejecting the model.\n\n\n\n\n\n\nConsider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\n\nRecovered\n\n\n\nYes\nNo\nTotal\n\n\n\n\nProcedure 1\n2.05\n6\n20\n\n\nProcedure 2\n1.37\n12\n20\n\n\nTotal\n20\n18\n40\n\n\n\nThis is one form of a 2x2 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\nSolution: We will perform a statistical hypothesis test:\nH0: There is no difference in the true recovery rates for animals on either procedure.\nH1: The recovery rates do differ.\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\n\\[       \nH_0: p_1 = p_2\nH_1: p_1 \\neq p_2\n\\]\nEstimates of individual recovery rates are \\(\\hat p = 14/20 = 0.7\\) and \\(\\hat p = 8/20 = 0.4\\). Is this difference due to chance?\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n\\[\n\\hat p = \\frac{14 +8}{20+20}= \\frac{22}{40}=0.55\n\\]\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be $20 /40 = 20 = 11 $ animals. In general this can be written as: \\[\n\\text{Expected Frequency} = \\frac{( \\text{row total})\\times (\\text{Column total})}{\\text{Grand Total}}\n\\] So the expected frequencies for the cells in the table are: \\[\n\\begin{split}\n\\text{Procedure 1, Recovered:}\\ \\frac{20 \\times 22}{40} &= 11\\\\\n\\text{Procedure 1, Not recovered:}\\ \\frac{20 \\times 18}{40} &=9 \\\\\n\\text{Procedure 2, Recovered:}\\ \\frac{20 \\times 22}{40} &= 11\\\\\n\\text{Procedure 2, Not recovered:}\\ \\frac{20 \\times 18}{40} &=9\n\\end{split}\n\\] Expected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\n\nRecovered\n\n\n\nYes\nNo\nTotal\n\n\n\n\nProcedure 1\n14 (11)\n6 (9)\n20\n\n\nProcedure 2\n8 (11)\n12 (9)\n20\n\n\nTotal\n20\n18\n40\n\n\n\nThe table shows observed (and expected) frequencies. The \\(\\chi^2\\) test statistic is then calculated using: \\[\n\\begin{split}\n\\chi^2 &=\\sum \\frac{\\text{(Observed - expected)}^2}{\\text{Expected}}\\\\\n&=\\frac{(14-11)^2}{11}+\\frac{(6-9)^2}{9}+ \\frac{(8-11)^2}{11}+\\frac{(12-9)^2}{9}\\\\\n&= 3.64\n\\end{split}\n\\] Large values of \\(\\chi^2\\) indicate discrepancies between observed and expected frequencies, i.e. large values indicate that H0 should be rejected in favour of H1.\nThe df of this \\(\\chi^2\\) test is 1 for a \\(2 \\times 2\\) contingency table. In general, &gt;df = (number of rows - 1) x (number of columns - 1)\nIf H0 is true, the observed \\(\\chi^2\\) is just one observation from a \\(\\chi^2\\) distribution with 1 df:\nSince \\(P =P(\\chi^2_1&gt;3.64)=0.056\\), there is (just) not sufficient evidence to reject H0. Thus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n\n\n\nThe second example is a 43 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n\nDisease Status\n\n\nVaccine\nNo\nMild\nSevere\nTotal\n\n\n\n\nControl\n100 (137.3)\n71 (42.6)\n29 (20.1)\n200\n\n\nA\n146 (133.9)\n32 (41.6)\n17 (19.6)\n195\n\n\nB\n149 (132.5)\n28 (41.2)\n16 (19.3)\n193\n\n\nC\n146 (137.3)\n37 (42.6)\n17 (20.1)\n200\n\n\nTotal\n541\n168\n79\n788\n\n\n\nThe table shows observed (and expected) frequencies.\nWe test H0 that there is no association between disease status and vaccination given, i.e. all vaccinations have equal effectiveness.\nAssuming H0 is true, the expected frequencies are calculated as follows:\ne.g. Expected frequency for an animal in the Control, No disease group: \\[\n\\frac{\\text{row total}\\times \\text{column total}}{\\text{grand total}}\n= \\frac{200 \\times 541}{788} =137.3\n\\]\nAs before, the test statistic is $^2 = $ and this will have (4-1)\\(\\times\\)(3-1) = 6 df.\nPearson chi-squared value is 45.22 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\nLikelihood chi-squared value is 43.34 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe ML \\(\\chi^2\\) is calculated as follows: \\[\n\\chi^2=2\\sum Observed \\times ln(\\frac{Observed}{Expected} )\n\\]\nThe degrees of freedom are (r – 1)(c – 1) as before, where r and c are the numbers of rows and columns respectively.\nIn general, the \\(\\chi^2\\) approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\n\nDisease Status\n\n\nVaccine\nNo\nMild\nSevere\nTotal\n\n\n\n\nControl\n10\n7\n3\n20\n\n\nA\n15\n3\n2\n20\n\n\nB\n15\n3\n2\n20\n\n\nC\n15\n4\n2\n21\n\n\nTotal\n55\n17\n9\n81",
    "crumbs": [
      "**📗 Module 2**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "module02/03-nonparametric1.html#notes-on-the-chi2-distribution",
    "href": "module02/03-nonparametric1.html#notes-on-the-chi2-distribution",
    "title": "Chi-squared tests",
    "section": "",
    "text": "The density function for a \\(\\chi^2\\) distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the \\(\\chi^2\\) density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g. 1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the \\(\\chi^2\\) distribution approaches the shape (and properties) of a normal distribution.\n\n\n\nFigure 7.1 The shape of the \\(\\chi^2\\) density function for various degrees of freedom.\n\n\nThe mean and variance of a \\(\\chi^2\\) variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as \\(\\nu\\) (Greek n), then\n\nMean of \\(\\chi^2\\) variable = \\(\\nu\\) (i.e. mean = df)\nVariance of \\(\\chi^2\\) variable = \\(2\\nu\\) (i.e. variance = twice the df)\n\nCritical values of a \\(\\chi^2\\) distribution are given in the \\(\\chi^2\\) probability table that appears as Appendix A.5.",
    "crumbs": [
      "**📗 Module 2**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "module02/03-nonparametric1.html#testing-agreement-of-frequency-data-with-expectation-models",
    "href": "module02/03-nonparametric1.html#testing-agreement-of-frequency-data-with-expectation-models",
    "title": "Chi-squared tests",
    "section": "",
    "text": "The process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\nChoose an appropriate hypothesis test for the type of data you have, and the type of question you’re asking.\nChoose the level of significance for the test.\nWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\nCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\nCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic – however there are alternative tests designed to accommodate these situations.\nCalculate the test statistic and degrees of freedom.\nObtain the P-value.\nDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n\n\n\nEXAMPLE 1\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a “fair” coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a “fair” coin.\nEXAMPLE 2 (from Mead et al, 2003)\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\n\n\n\nCheese\nFirst preference\n\n\n\n\nA\n5\n\n\nB\n7\n\n\nC\n18\n\n\nD\n10\n\n\nTotal\n40\n\n\n\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n\\[\\chi^2_{obs}=\\frac{(5-10)^2}{10}+ \\frac{(7-10)^2}{10}+ \\frac{(10-10)^2}{10}  =9.80\\]\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the \\(\\chi^2\\) distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the \\(\\chi^2_{obs}\\) value of 9.80 is greater than the \\(\\chi^2_{crit}\\) value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since \\(P &lt; 0.05\\))\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with\n\\[ \\chi^2_{obs}=\\frac{(18-10)^2}{10}+ \\frac{(22-30)^2}{30}  =…\\text{etc.}\\]\n\n\n\n(from Mead et al, 2003)\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen).\nThe figures obtained for the primula plants follow.\n\n\n\n\nNormal eye\nPrimrose Queen eye\nTotal\n\n\n\n\nFlat leaves\n328\n122\n450\n\n\nCrimped leaves\n77\n33\n110\n\n\nTotal\n405\n115\n560\n\n\n\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, 3/4 of the total 560 observation in the first class of the characteristic and the remaining 1/4 in the second class. Further, this model predicts that 3/4 of the flat-leaved plants should have normal eyes, resulting in 3/4 \\(\\times\\) 3/4 of all the plants or 9/16 with flat leaves and normal eyes; the remaining 3/4 of the flat-leaved plants, which is 1/4 \\(\\times\\) 3/4or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes.\nThe calculation of these expected or predicted proportions is shown below.\n\n\n\n\nNormal eye\nPrimrose Queen eye\n\n\n\n\nFlat leaves\n3/4 x 3/4 = 9/16\n1/4 x 3/4 = 3/16\n\n\nCrimped leaves\n3/4 x 1/4 = 3/16\n1/4 x 1/4 = 1/16\n\n\n\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35.\nThe observed and expected frequencies are summarized in the table below.\n\n\n\n\nNormal eye\nPrimrose Queen eye\n\n\n\n\nFlat leaves\n328 (315)\n122 (105)\n\n\nCrimped leaves\n77 (105)\n33 (35)\n\n\n\n\\[\n\\begin{split}\n\\chi^2_{obs} &= \\frac{(328-315)^2}{315}+ \\frac{(122+105)^2}{105}+ \\frac{(77-105)^2}{105}+ \\frac{(33-35)^2}{35}  \\\\\n&= 0.54 + 2.75 + 7.47 + 0.11 \\\\\n&= 10.77.\n\\end{split}\n\\] We compare 10.77 with the 5% point of the \\(\\chi^2\\) distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable.\nSee pp. 332-333 of Mead et al, 2003 for what to do next… after rejecting the model.",
    "crumbs": [
      "**📗 Module 2**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "module02/03-nonparametric1.html#contingency-tables",
    "href": "module02/03-nonparametric1.html#contingency-tables",
    "title": "Chi-squared tests",
    "section": "",
    "text": "Consider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\n\nRecovered\n\n\n\nYes\nNo\nTotal\n\n\n\n\nProcedure 1\n2.05\n6\n20\n\n\nProcedure 2\n1.37\n12\n20\n\n\nTotal\n20\n18\n40\n\n\n\nThis is one form of a 2x2 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\nSolution: We will perform a statistical hypothesis test:\nH0: There is no difference in the true recovery rates for animals on either procedure.\nH1: The recovery rates do differ.\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\n\\[       \nH_0: p_1 = p_2\nH_1: p_1 \\neq p_2\n\\]\nEstimates of individual recovery rates are \\(\\hat p = 14/20 = 0.7\\) and \\(\\hat p = 8/20 = 0.4\\). Is this difference due to chance?\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n\\[\n\\hat p = \\frac{14 +8}{20+20}= \\frac{22}{40}=0.55\n\\]\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be $20 /40 = 20 = 11 $ animals. In general this can be written as: \\[\n\\text{Expected Frequency} = \\frac{( \\text{row total})\\times (\\text{Column total})}{\\text{Grand Total}}\n\\] So the expected frequencies for the cells in the table are: \\[\n\\begin{split}\n\\text{Procedure 1, Recovered:}\\ \\frac{20 \\times 22}{40} &= 11\\\\\n\\text{Procedure 1, Not recovered:}\\ \\frac{20 \\times 18}{40} &=9 \\\\\n\\text{Procedure 2, Recovered:}\\ \\frac{20 \\times 22}{40} &= 11\\\\\n\\text{Procedure 2, Not recovered:}\\ \\frac{20 \\times 18}{40} &=9\n\\end{split}\n\\] Expected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\n\nRecovered\n\n\n\nYes\nNo\nTotal\n\n\n\n\nProcedure 1\n14 (11)\n6 (9)\n20\n\n\nProcedure 2\n8 (11)\n12 (9)\n20\n\n\nTotal\n20\n18\n40\n\n\n\nThe table shows observed (and expected) frequencies. The \\(\\chi^2\\) test statistic is then calculated using: \\[\n\\begin{split}\n\\chi^2 &=\\sum \\frac{\\text{(Observed - expected)}^2}{\\text{Expected}}\\\\\n&=\\frac{(14-11)^2}{11}+\\frac{(6-9)^2}{9}+ \\frac{(8-11)^2}{11}+\\frac{(12-9)^2}{9}\\\\\n&= 3.64\n\\end{split}\n\\] Large values of \\(\\chi^2\\) indicate discrepancies between observed and expected frequencies, i.e. large values indicate that H0 should be rejected in favour of H1.\nThe df of this \\(\\chi^2\\) test is 1 for a \\(2 \\times 2\\) contingency table. In general, &gt;df = (number of rows - 1) x (number of columns - 1)\nIf H0 is true, the observed \\(\\chi^2\\) is just one observation from a \\(\\chi^2\\) distribution with 1 df:\nSince \\(P =P(\\chi^2_1&gt;3.64)=0.056\\), there is (just) not sufficient evidence to reject H0. Thus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n\n\n\nThe second example is a 43 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n\nDisease Status\n\n\nVaccine\nNo\nMild\nSevere\nTotal\n\n\n\n\nControl\n100 (137.3)\n71 (42.6)\n29 (20.1)\n200\n\n\nA\n146 (133.9)\n32 (41.6)\n17 (19.6)\n195\n\n\nB\n149 (132.5)\n28 (41.2)\n16 (19.3)\n193\n\n\nC\n146 (137.3)\n37 (42.6)\n17 (20.1)\n200\n\n\nTotal\n541\n168\n79\n788\n\n\n\nThe table shows observed (and expected) frequencies.\nWe test H0 that there is no association between disease status and vaccination given, i.e. all vaccinations have equal effectiveness.\nAssuming H0 is true, the expected frequencies are calculated as follows:\ne.g. Expected frequency for an animal in the Control, No disease group: \\[\n\\frac{\\text{row total}\\times \\text{column total}}{\\text{grand total}}\n= \\frac{200 \\times 541}{788} =137.3\n\\]\nAs before, the test statistic is $^2 = $ and this will have (4-1)\\(\\times\\)(3-1) = 6 df.\nPearson chi-squared value is 45.22 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\nLikelihood chi-squared value is 43.34 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe ML \\(\\chi^2\\) is calculated as follows: \\[\n\\chi^2=2\\sum Observed \\times ln(\\frac{Observed}{Expected} )\n\\]\nThe degrees of freedom are (r – 1)(c – 1) as before, where r and c are the numbers of rows and columns respectively.\nIn general, the \\(\\chi^2\\) approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\n\nDisease Status\n\n\nVaccine\nNo\nMild\nSevere\nTotal\n\n\n\n\nControl\n10\n7\n3\n20\n\n\nA\n15\n3\n2\n20\n\n\nB\n15\n3\n2\n20\n\n\nC\n15\n4\n2\n21\n\n\nTotal\n55\n17\n9\n81",
    "crumbs": [
      "**📗 Module 2**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html",
    "href": "module03/041-linear_functions.html",
    "title": "Why do we create models?",
    "section": "",
    "text": "Often in a scientific experiment, we collect data for a response variable (\\(y\\)) and one or more predictor variables (\\(x\\)).\nSome interchangeable terms:\n\n\\(y\\) – independent variable, response variable, target, outcome, etc.\n\\(x\\) – dependent variable, predictor variable, feature, input etc.\n\nThere are several reasons why we would ‘model’ or ‘create a model’ for the data we have collected:\n\nTo describe the relationship between \\(x\\) and \\(y\\) (e.g. weak/moderate/strong, positive/negative, linear/non-linear, etc.)\nTo explain the relationship between \\(x\\) and \\(y\\) in terms of an equation\nTo predict the value of \\(y\\) for a given value of \\(x\\)\n\nThe simplest form of a model is a linear model.",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#analytical-method",
    "href": "module03/041-linear_functions.html#analytical-method",
    "title": "Why do we create models?",
    "section": "Analytical method",
    "text": "Analytical method\nFor simple linear regression, we can calculate the values of \\(\\beta_0\\) and \\(\\beta_1\\) with equations\nFirst we calculate the slope \\(\\beta_1\\):\n\\[ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} =  \\frac{Cov(x,y)}{Var(x)} = \\frac{SS_{xy}}{SS_{xx}} \\] Which we then substitute into the equation below to get the intercept \\(\\beta_0\\):\n\\[ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} \\]\nUsing R to do our calculations:\n\ncov_xy &lt;- sum((x - mean(x)) * (y - mean(y)))\nvar_x &lt;- sum((x - mean(x))^2)\nb1 &lt;- cov_xy / var_x\n\nb0 &lt;- mean(y) - b1 * mean(x)\n\nSo the analytical method determines that our linear model is \\(y = -0.363076 + 0.415755 \\cdot x\\).",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#numerical-method",
    "href": "module03/041-linear_functions.html#numerical-method",
    "title": "Why do we create models?",
    "section": "Numerical method",
    "text": "Numerical method\nThe numerical method is a trial and error method. We start with some initial values of \\(\\beta_0\\) and \\(\\beta_1\\), and then update the values to minimize the sum of squares. This is the most common method that computer programs (e.g. Excel, R, Python, etc.) will use as it is more computationally efficient with very large dataset (e.g. millions of rows).\nFitting the model in R is very simple:\n\nmod &lt;- lm(y ~ x) # fit a linear model between x and y\nsummary(mod)     # model output in a neat summary table\n\nThe estimate of our parameters (or coefficients) are in the Estimate column. The Intercept Estimate is our y-intercept \\(\\beta_0\\) and the x Estimate is our slope \\(\\beta_1\\).\nSo the numerical method run by R determines that our linear model is \\(y = -0.363076 + 0.415755 \\cdot x\\). This is exactly the same result as the analytical method (at least to 6 decimal places).",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#exploratory-data-analysis",
    "href": "module03/041-linear_functions.html#exploratory-data-analysis",
    "title": "Why do we create models?",
    "section": "1. Exploratory data analysis",
    "text": "1. Exploratory data analysis\nThis is a basic step in any data analysis. We need to understand the data we are working with. As with previous modules, we can look at summary statistics, distributions, and visualise the data.\nFor linear regression, we also need to look at the relationship between the predictor and response. We first look at a scatter plot to determine if we have linear data, and then we choose a suitable correlation coefficient.\n\nggplot() +\n  geom_point(aes(x = x, y = y)) +\n  labs(title = \"Petal length vs Petal width\",\n       x = \"Petal length\",\n       y = \"Petal width\")\n\nThere appears to be a strong, positive linear relationship between petal length and petal width.\n\ncor(x, y) |&gt; round(2) # calculate correlation |&gt; round to 2 decimal places\n\nThe correlation coefficient is 0.96, which is very close to 1. This is almost a perfect positive linear relationship!",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#assumptions",
    "href": "module03/041-linear_functions.html#assumptions",
    "title": "Why do we create models?",
    "section": "2. Assumptions",
    "text": "2. Assumptions\nTo use linear regression, there are several assumptions (LINE) that need to be met:\n\nLinearity – is there a linear relationship?\nIndependence – are the residuals independent of each other?\nNormality – are the residuals normally distributed?\nEqual variance – is the variance of the residuals constant?\n\nIf assumptions are met, then we can be confident that the model is a good fit for the data. If they are not met, then the hypothesis test results are unreliable, the standard error estimate is unreliable, and the estimates of the coefficients will not fit the model well.\nIf the assumptions are not met, then we either need to transform our data (e.g. \\(x\\), \\(y\\), or both) with a function (e.g. square root, natural log etc.) or use a non-linear model.\nAn important point to remember is that the assumptions are about the residuals, not the data itself. The equation for a linear model is:\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\] where \\(\\epsilon\\) is the error term, or residual. This is the only term which adds variation to an otherwise straight line, so this is what we need to check our assumptions with.\n\nLinearity\nLinear regression assumes that there is a linear relationship between \\(x\\) and \\(y\\). It does not make logical or statistical sense to fit a linear model to data that does not have a linear relationship. With non-linear data, the other assumptions will not be met either. The easiest method to check for linearity with a scatter plot.\n\n\nIndependence\nThe independence of errors is the assumption that the residuals for one observation are independent of another observation. This is a difficult assumption to check, but it is often assumed that the data is collected in a way that the residuals are independent.\nFor example, if we are measuring the height of children in a class, the height of one child should not affect the height of another child. However, if all the children were siblings or identical octuplets, then the residuals would not be independent. Another case where this assumption could be broken is in time series data – the height of a child this year is not independent from the height last year.\n\n\nNormality\nA linear model assumes the residuals are normally distributed around the line of best fit. This is important for hypothesis testing and confidence intervals. We can check this assumption with a Q-Q plot or a histogram of the residuals.\n\n\n\n\n\n\n\nEqual variance\nThis is also known as the assumption of constant variance or homoscedasticity. It assumes that the variance of the residuals is constant across all levels of the predictor variable (i.e. no fanning). Again, this is important for hypothesis testing and confidence intervals. We can check this assumption with a scatter plot of Residuals vs Fitted values and the Scale-Location plot.\n\n\nAssumption Plots\nIn R, plots of the residuals can be made with the plot() function and the model object (in this case mod) as the input. The four plots produced are 1) Residuals vs Fitted (linearity, equal variances), 2) the Normal Q-Q plot (normality), 3) a Scale-Location plot (equal variances), and 4) the Residuals vs Leverage plot (extreme outliers).\n\nmod &lt;- lm(y ~ x)  # fit a linear model between x and y\n\npar(mfrow=c(2,2)) # set up a 2x2 grid of plots\nplot(mod)         # create the plots\n\nThese same plots can also be made with autoplot() function (from the ggfortify package). It does not require the par() function to set up the grid and is more aesthetically customisable.\n\nlibrary(ggfortify)\nggplot2::autoplot(mod)\n\n\nThe Residuals vs Fitted plots the residuals against the ‘predicted’ values (i.e. points on the line). If met, the points will be randomly scattered around the horizontal 0 line. If there is a pattern (e.g. a curve, a quadratic), then the assumption of linearity is not met. If there is a fan shape, then the assumption of equal variance is not met.\nThe Normal Q-Q plot compares the residuals to a normal distribution. If the residuals are normally distributed, the points will fall on the straight line. If the points deviate from the line, then the residuals are not normally distributed.\nThe Scale-Location plot is used to check the assumption of equal variance. If the line is essentially horizontal and points are randomly scattered, then the assumption is met.\nThe Residuals vs Leverage plot is used to identify extreme outliers. Although not an official assumption, a very extreme outlier has the potential to skew the line of best fit. These influential points should be kept track of or removed from the dataset.\n\nThe examples above are for the iris dataset with petal length and petal width. The assumptions are met.\nBelow is an example of plots from data that has an exponential relationship with unequal variances (it fans in) with a couple of extreme outliers (in red).\n\nmod_exp &lt;- lm(y_exp ~ x_exp)\npar(mfrow=c(2,2))\nplot(mod_exp)\n\nWe can see that:\n\nThe Residuals vs Fitted plot looks curved, indicating unequal variances. The scatter of points is also not very even because of the extreme outliers.\nThe Normal Q-Q plot shows the points are not on the line at the tails and hence the residuals are not normally distributed.\nThe Scale-Location plot shows a ‘W’ shape, indicating unequal variances.\nThe Residuals vs Leverage plot shows the two extreme outliers past the dotted lines, which confirms they are influential points.",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#model-evaluation",
    "href": "module03/041-linear_functions.html#model-evaluation",
    "title": "Why do we create models?",
    "section": "Model evaluation",
    "text": "Model evaluation\nSo now that we have confirmed the assumptions are met, the next step is to determine how good the model actually is. If the model was not a good fit to the data, then we wouldn’t want to interpret it – we’d try and improve it first.\nLet’s break down the summary(mod) output. The first few lines are the model formula lm(formula = y ~ x), so this is our linear model. The Residuals section gives some summary statistics of the residuals.\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\n\nHypothesis Test\nThe null hypothesis for a simple linear regression is that the slope is zero (\\(\\beta_1 = 0\\)), and the model does not perform better than just using the mean. There is thus no relationship between \\(x\\) and \\(y\\). The alternative hypothesis is that the slope is not zero (\\(\\beta_1 \\neq 0\\)), which means there is a relationship between \\(x\\) and \\(y\\), and the model is better than using the mean.\nThe Coefficients section gives the estimates of our y-intercept and slope, as well as the standard error, the t-value, and the p-value when determining the estimate. The p-value is the most easily interpreted – we care the p-value of the (\\(x\\)). If the p-value is less than 0.05, then we can reject the null hypothesis that the slope \\(\\beta_1\\) is zero.\nThe Signif. codes section gives a visual representation of the p-value. The more stars, the smaller the p-value. The *** means the p-value is less than 0.001, ** means less than 0.01, and * means less than 0.05. This is useful when we have many predictors.\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.363076   0.039762  -9.131  4.7e-16 ***\nx            0.415755   0.009582  43.387  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n\n\nModel Fit\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\nThe Residual standard error is the standard deviation of the residuals. The smaller the value, the better the model fits the data. It is in the same units as the response variable. Considering the \\(y\\) ranges between 0.1 and 2.5, a standard error of 0.2065 is quite small.\nThe Multiple R-squared is the proportion of the variance in the response variable that is explained by the model. The more variation explained by the model, the better the fit. A value of ‘1’ indicates a perfect fit, and a value of ‘0’ suggests the model explained no variation at all. A value of 92.71% is very good. In simple linear regression, the R2 is actually equivalent to the correlation coefficient squared - which is also where the term comes from.\nThe Adjusted R-squared is the same as the Multiple R-squared, but adjusted for the number of predictors in the model. We use the Adjusted R-squared in multiple linear regression, and the Multiple R-squared in simple linear regression.\n\\[ R^2 = \\frac{SS_{reg}}{SS_{tot}} = 1 - \\frac{SS_{res}}{SS_{tot}} \\]\n\\[ R^2_{adj} = 1 - \\frac{SS_{res}}{SS_{tot}} \\frac{n-1}{n-p-1} \\] Lastly, the F-statistic is a statistical test of the overall significance of the model. In simple linear regression (1 predictor, 1 response) it is the same as the p-value in the Coefficients table. The F-statistic value is 1882 in the example, and the degrees of freedom (1, 148) are the number of predictors (1) and the number of observations minus two (150-2=148) because there are two parameters. This is covered in more detail in ENVX2001.",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#interpretation",
    "href": "module03/041-linear_functions.html#interpretation",
    "title": "Why do we create models?",
    "section": "Interpretation",
    "text": "Interpretation\nThe assumptions for simple linear regression were met. The model explains 92.71% of the variation in the petal width with petal length with a residual standard error of 0.2065, and petal length is a significant predictor (p &lt; 2e-16).\nThe equation of the model is \\(y = -0.36 + 0.42 \\cdot x\\). This means that for every unit increase in petal length, petal width increases by 0.42 cm.\nThe y-intercept is -0.36, which is the value of petal width when petal length is zero. This is not a meaningful value in this context, as petal length cannot be zero. This is often the case with the y-intercept.\nLast but not least, the result can be plotted on a scatterplot with the line of best fit.\n\n### Base R\nplot(x, y, pch = 19,\n     xlab = \"Petal Length\", ylab = \"Petal Width\", main = \"Base R\"\n)\nabline(mod, col = \"red\") # using our model `mod` object\n# abline(a = -0.363076, b = 0.415755, col = \"red\") # manually inputting coefficients\n\n### ggplot\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") + # ggplot will fit the model for us\n  # geom_abline(intercept = -0.363076, slope = 0.415755, color = \"red\") + # manually inputting coefficients\n  labs(x = \"Petal Length\", y = \"Petal Width\", title = \"ggplot2\")",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#explaining-backtransformation",
    "href": "module03/041-linear_functions.html#explaining-backtransformation",
    "title": "Why do we create models?",
    "section": "Explaining backtransformation",
    "text": "Explaining backtransformation\n\nSquare root\nThe backtransformation of the coefficients is not as straightforward as the interpretation of the coefficients. For example, if we had used a square root transformation sqrt(Ozone), then if we wanted to get the effect of increasing Temp by one unit:\n\\[ \\sqrt{Ozone} = \\beta_0 + \\beta_1 \\cdot Temp \\] \\[ Ozone = (\\beta_0 + \\beta_1 \\cdot Temp)^2 \\] \\[ Ozone = \\beta_0^2 + 2 \\beta_0 \\beta_1 Temp + \\beta_1^2 Temp^2 \\] So if Temp increases by one unit, the increase in Ozone is not just \\(\\beta_1^2\\)! We end up with \\(2\\beta_1(\\beta_0+\\beta_1 \\cdot Temp) + \\beta_1^2\\).\n\n\nNatural log\nFor natural logs, however, the backtransformation is much simpler because of log laws.\n\\[ log(Ozone) = \\beta_0 + \\beta_1 \\cdot Temp \\] \\[ Ozone = e^{\\beta_0 + \\beta_1 \\cdot Temp} = e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp} \\] If Temp increases by one unit, then the increase in Ozone is:\n\\[ Ozone = e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot (Temp+1)} = e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp} \\cdot e^{\\beta_1} \\] The ratio between the two is:\n\\[ \\frac{e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp} \\cdot e^{\\beta_1}}{e^{\\beta_0} \\cdot e^{\\beta_1 \\cdot Temp}} = e^{\\beta_1} \\] So for a one unit increase in Temp, Ozone increases by \\(e^{\\beta_1}\\) times.",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html#percentage-change",
    "href": "module03/041-linear_functions.html#percentage-change",
    "title": "Why do we create models?",
    "section": "Percentage Change",
    "text": "Percentage Change\nInterpreting as a percent change can be more meaningful - it can be done with any log transformation (substitute \\(e\\) below for 10 or any other base), but the quick approximation only works with natural log transformations.\nIf \\(y\\) has been transformed with a natural log (log(y)), for a one-unit increase in \\(x\\) the percent change in \\(y\\) (not log(y)) is calculated with:\n\\[\\Delta y \\% = 100 \\cdot (e^{\\beta_1}-1)\\] If \\(\\beta_1\\) is small (i.e. \\(-0.25 &lt; \\beta_1 &lt; 0.25\\)), then: \\(e^{\\beta_1} \\approx 1 + \\beta_1\\). So \\(\\Delta y \\% \\approx 100 \\cdot \\beta_1\\). Below are some examples – when \\(\\beta_1\\) is 2, the ‘quick estimate’ is off by 439%!\n\n\n\n\n\n\n\n\nβ\nExact \\((e^{\\beta} - 1)\\)%\nApproximate \\(100 \\cdot \\beta\\)\n\n\n\n\n-0.25\n-22.13\n-25\n\n\n-0.1\n-9.52\n-10\n\n\n0.01\n1.01\n1\n\n\n0.1\n10.52\n10\n\n\n0.25\n28.41\n25\n\n\n0.5\n64.87\n50\n\n\n2\n638.91\n200\n\n\n\nSo for our linear model \\(log(\\text{Ozone}) = -1.849 + 0.068 \\cdot \\text{Temp}\\), a one unit increase in Temp results in approximately a 6.8% increase in Ozone.",
    "crumbs": [
      "**📘 Module 3**",
      "Why do we create models?"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html",
    "href": "module03/043-nonlinear.html",
    "title": "Non-linear regression",
    "section": "",
    "text": "Linear relationships are simple to interpret since the rate of change is constant – i.e. as \\(x\\) changes, \\(y\\) changes at a constant rate. For non-linear relationships – as \\(x\\) changes, \\(y\\) changes at an unproportional rate. To simplify interpretation and enable the use of linear models, it is often recommended to transform non-linear data to make it approximately linear.\nTransformation is usually possible for monotonic relationships (i.e. relationships that are always increasing or decreasing) such as exponential growth curves. The most common transformations are:\n\n\n\nName\nTransformation\n\n\n\n\nInverse\n\\(y = \\frac{1}{x}\\)\n\n\nRoot\n\\(y = \\sqrt[a]{x}\\)\n\n\nExponential\n\\(y = e^x\\)\n\n\nLogarithmic\n\\(y = \\log_a(x)\\)\n\n\n\nHowever, in the case of non-monotonic relationships (e.g. polynomials, asymptotic, logistic) transformation may not be enough to make the data meet the assumptions. In this section, we look at fitting non-linear models.\n\\[ Y_i = f(x_i, \\beta) + \\epsilon_i \\] where \\(f(x_i, \\beta)\\) is a nonlinear function of the parameters \\(\\beta\\).\nThe assumptions for non-linear regression are INE (Independence, Normality, Equal variance).\n\n\nA polynomial equation is an extension to linear regression and can still be fitted using least squares. Typically, a polynomial equation has multiple terms of the same predictor (i.e. one \\(x\\)). The more terms in the polynomial, the more complex the model and the less likely it follows an actual biological relationship. A complex model may ‘fit’ the data well, but fail to represent the true relationship between the variables (overfitting).\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i \\] where \\(k\\) is the degree of the polynomial.\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nEach level increases the power of the predictor by 1.\n\n\n\nTo fit a polynomial, we still use the lm() function. To create the polynomial terms, we use the poly(x, n) function, where x is the predictor and n is the degree of the polynomial.\nBelow is an example with asymptotic data – it increases rapidly to a certain point and then levels off. The linear model (blue) does not capture the complexity of the relationship. The polynomial model with 10 terms (green) fits the data well, but it is too complex between \\(x\\) = 5-10. In this case, the polynomial model with 2 terms (red), aka the quadratic model, fits the data the best.\n\n\nCode\n# Generate some data with an asymptotic relationship\nset.seed(442) # set seed\n\nasymptotic &lt;- tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100 * (1 - exp(-0.5 * predictor)) + rnorm(length(predictor), mean = 0, sd = 10)\n)\n\n\n\nmod_lin &lt;- lm(response ~ predictor, asymptotic) # blue\nmod_poly2 &lt;- lm(response ~ poly(predictor, 2), asymptotic) # red\nmod_poly10 &lt;- lm(response ~ poly(predictor, 10), asymptotic) # green\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point(size = 2) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_lin)), color = \"slateblue\", size = 1.5, linetype = 2) +\n  geom_line(aes(y = predict(mod_poly2)), color = \"brown\", size = 1, linetype = 1) +\n  geom_line(aes(y = predict(mod_poly10)), color = \"seagreen\", size = 1.5, linetype = 2) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nThis is also evident when comparing the summary() of the models. The linear model explains the least amount of variation in \\(y\\) (Multiple R-squared = 57.01%). The 2-term polynomial model explains 81.95% of variation (Adjusted R-squared), whereas the 10-term polynomial model explains 86.21% of variation (Adjusted R-squared). The first three terms of the polynomial model are significant, the fourth is almost significant, but the remainder are not. Comparing the 2 and 10-term polynomial models, are the extra 8 terms worth an extra 4.26% of variation explained? Considering the 10-term polynomial is very complex and overfitted – the principle of parsimony would say ‘no’.\n\nsummary(mod_lin)\n\n\nCall:\nlm(formula = response ~ predictor, data = asymptotic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.838 -11.028  -1.038  16.376  31.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.9163     5.4567   7.682 5.88e-10 ***\npredictor     7.5804     0.9404   8.061 1.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.77 on 49 degrees of freedom\nMultiple R-squared:  0.5701,    Adjusted R-squared:  0.5613 \nF-statistic: 64.97 on 1 and 49 DF,  p-value: 1.544e-10\n\nsummary(mod_poly2)\n\n\nCall:\nlm(formula = response ~ poly(predictor, 2), data = asymptotic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.056  -7.199  -0.193   8.959  22.482 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           79.818      1.776  44.951  &lt; 2e-16 ***\npoly(predictor, 2)1  159.368     12.681  12.568  &lt; 2e-16 ***\npoly(predictor, 2)2 -106.939     12.681  -8.433 4.92e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.68 on 48 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8195 \nF-statistic: 114.5 on 2 and 48 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod_poly10)\n\n\nCall:\nlm(formula = response ~ poly(predictor, 10), data = asymptotic)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1659  -8.6908  -0.0494   8.8003  16.4012 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             79.818      1.552  51.426  &lt; 2e-16 ***\npoly(predictor, 10)1   159.368     11.084  14.378  &lt; 2e-16 ***\npoly(predictor, 10)2  -106.939     11.084  -9.648 5.37e-12 ***\npoly(predictor, 10)3    48.570     11.084   4.382 8.28e-05 ***\npoly(predictor, 10)4   -19.411     11.084  -1.751   0.0876 .  \npoly(predictor, 10)5     1.193     11.084   0.108   0.9148    \npoly(predictor, 10)6    -2.769     11.084  -0.250   0.8040    \npoly(predictor, 10)7    -1.343     11.084  -0.121   0.9042    \npoly(predictor, 10)8    -4.009     11.084  -0.362   0.7195    \npoly(predictor, 10)9    -2.851     11.084  -0.257   0.7984    \npoly(predictor, 10)10    5.769     11.084   0.520   0.6056    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.08 on 40 degrees of freedom\nMultiple R-squared:  0.8897,    Adjusted R-squared:  0.8621 \nF-statistic: 32.26 on 10 and 40 DF,  p-value: 4.846e-16\n\n\nThe hypothesis of a polynomial model is similar to that of multiple linear regression. A significance test is done for each individual coefficient, but we only consider the significance of the highest order term. For example, if a 10-term polynomial model is used but \\(x^10\\) is not significant, then a simpler model should be considered.\n\\[H_0: \\beta_k = 0\\] \\[H_0: \\beta_k \\neq 0\\] There is also an overall model significance, but if the highest order term is significant, the model will be significant.\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\] \\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0\\] In the case that all coefficients are zero, the model is better represented by the mean of the data.\nAs for interpretation of the quadratic model, we can write the equation, but the coefficients do not have much meaning.\n\\[\\text{response} = 79.82 + 159.37 \\cdot \\text{predictor} + -106.94 \\cdot \\text{predictor}^2\\] Given it is a quadratic equation, we could calculate the peak of the curve (\\(x_{peak} = -\\frac{b}{2a}\\) and substitute to get \\(y_{peak} = c - \\frac{b^2}{4a}\\)). For a polynomial of a higher degree, it would not be meaningful.\nAlthough a little out of order, we can also check the assumptions. Recall that linearity is not an assumption for a non-linear model – we can disregard the shape of the line for the Residuals vs Fitted plot, and focus on the distribution of the point around the line (which is even). The residuals fall on the line for the Normal-QQ plot indicating residuals are normally distributed, and the Scale-Location plot shows the residuals evenly distributed (equal varince), and there are no extreme outliers in the Residuals vs Leverage plot.\n\npar(mfrow = c(2, 2))\nplot(mod_poly2)",
    "crumbs": [
      "**📘 Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#polynomial-regression",
    "href": "module03/043-nonlinear.html#polynomial-regression",
    "title": "Non-linear regression",
    "section": "",
    "text": "A polynomial equation is an extension to linear regression and can still be fitted using least squares. Typically, a polynomial equation has multiple terms of the same predictor (i.e. one \\(x\\)). The more terms in the polynomial, the more complex the model and the less likely it follows an actual biological relationship. A complex model may ‘fit’ the data well, but fail to represent the true relationship between the variables (overfitting).\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + ... + \\beta_k x_i^k + \\epsilon_i \\] where \\(k\\) is the degree of the polynomial.\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nCubic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nEach level increases the power of the predictor by 1.\n\n\n\nTo fit a polynomial, we still use the lm() function. To create the polynomial terms, we use the poly(x, n) function, where x is the predictor and n is the degree of the polynomial.\nBelow is an example with asymptotic data – it increases rapidly to a certain point and then levels off. The linear model (blue) does not capture the complexity of the relationship. The polynomial model with 10 terms (green) fits the data well, but it is too complex between \\(x\\) = 5-10. In this case, the polynomial model with 2 terms (red), aka the quadratic model, fits the data the best.\n\n\nCode\n# Generate some data with an asymptotic relationship\nset.seed(442) # set seed\n\nasymptotic &lt;- tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = 100 * (1 - exp(-0.5 * predictor)) + rnorm(length(predictor), mean = 0, sd = 10)\n)\n\n\n\nmod_lin &lt;- lm(response ~ predictor, asymptotic) # blue\nmod_poly2 &lt;- lm(response ~ poly(predictor, 2), asymptotic) # red\nmod_poly10 &lt;- lm(response ~ poly(predictor, 10), asymptotic) # green\n\n\n\nCode\nggplot(asymptotic, aes(x = predictor, y = response)) +\n  geom_point(size = 2) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_lin)), color = \"slateblue\", size = 1.5, linetype = 2) +\n  geom_line(aes(y = predict(mod_poly2)), color = \"brown\", size = 1, linetype = 1) +\n  geom_line(aes(y = predict(mod_poly10)), color = \"seagreen\", size = 1.5, linetype = 2) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nThis is also evident when comparing the summary() of the models. The linear model explains the least amount of variation in \\(y\\) (Multiple R-squared = 57.01%). The 2-term polynomial model explains 81.95% of variation (Adjusted R-squared), whereas the 10-term polynomial model explains 86.21% of variation (Adjusted R-squared). The first three terms of the polynomial model are significant, the fourth is almost significant, but the remainder are not. Comparing the 2 and 10-term polynomial models, are the extra 8 terms worth an extra 4.26% of variation explained? Considering the 10-term polynomial is very complex and overfitted – the principle of parsimony would say ‘no’.\n\nsummary(mod_lin)\n\n\nCall:\nlm(formula = response ~ predictor, data = asymptotic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.838 -11.028  -1.038  16.376  31.392 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.9163     5.4567   7.682 5.88e-10 ***\npredictor     7.5804     0.9404   8.061 1.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.77 on 49 degrees of freedom\nMultiple R-squared:  0.5701,    Adjusted R-squared:  0.5613 \nF-statistic: 64.97 on 1 and 49 DF,  p-value: 1.544e-10\n\nsummary(mod_poly2)\n\n\nCall:\nlm(formula = response ~ poly(predictor, 2), data = asymptotic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.056  -7.199  -0.193   8.959  22.482 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           79.818      1.776  44.951  &lt; 2e-16 ***\npoly(predictor, 2)1  159.368     12.681  12.568  &lt; 2e-16 ***\npoly(predictor, 2)2 -106.939     12.681  -8.433 4.92e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.68 on 48 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8195 \nF-statistic: 114.5 on 2 and 48 DF,  p-value: &lt; 2.2e-16\n\nsummary(mod_poly10)\n\n\nCall:\nlm(formula = response ~ poly(predictor, 10), data = asymptotic)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1659  -8.6908  -0.0494   8.8003  16.4012 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             79.818      1.552  51.426  &lt; 2e-16 ***\npoly(predictor, 10)1   159.368     11.084  14.378  &lt; 2e-16 ***\npoly(predictor, 10)2  -106.939     11.084  -9.648 5.37e-12 ***\npoly(predictor, 10)3    48.570     11.084   4.382 8.28e-05 ***\npoly(predictor, 10)4   -19.411     11.084  -1.751   0.0876 .  \npoly(predictor, 10)5     1.193     11.084   0.108   0.9148    \npoly(predictor, 10)6    -2.769     11.084  -0.250   0.8040    \npoly(predictor, 10)7    -1.343     11.084  -0.121   0.9042    \npoly(predictor, 10)8    -4.009     11.084  -0.362   0.7195    \npoly(predictor, 10)9    -2.851     11.084  -0.257   0.7984    \npoly(predictor, 10)10    5.769     11.084   0.520   0.6056    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.08 on 40 degrees of freedom\nMultiple R-squared:  0.8897,    Adjusted R-squared:  0.8621 \nF-statistic: 32.26 on 10 and 40 DF,  p-value: 4.846e-16\n\n\nThe hypothesis of a polynomial model is similar to that of multiple linear regression. A significance test is done for each individual coefficient, but we only consider the significance of the highest order term. For example, if a 10-term polynomial model is used but \\(x^10\\) is not significant, then a simpler model should be considered.\n\\[H_0: \\beta_k = 0\\] \\[H_0: \\beta_k \\neq 0\\] There is also an overall model significance, but if the highest order term is significant, the model will be significant.\n\\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k = 0\\] \\[H_0: \\beta_1 = \\beta_2 = ... = \\beta_k \\neq 0\\] In the case that all coefficients are zero, the model is better represented by the mean of the data.\nAs for interpretation of the quadratic model, we can write the equation, but the coefficients do not have much meaning.\n\\[\\text{response} = 79.82 + 159.37 \\cdot \\text{predictor} + -106.94 \\cdot \\text{predictor}^2\\] Given it is a quadratic equation, we could calculate the peak of the curve (\\(x_{peak} = -\\frac{b}{2a}\\) and substitute to get \\(y_{peak} = c - \\frac{b^2}{4a}\\)). For a polynomial of a higher degree, it would not be meaningful.\nAlthough a little out of order, we can also check the assumptions. Recall that linearity is not an assumption for a non-linear model – we can disregard the shape of the line for the Residuals vs Fitted plot, and focus on the distribution of the point around the line (which is even). The residuals fall on the line for the Normal-QQ plot indicating residuals are normally distributed, and the Scale-Location plot shows the residuals evenly distributed (equal varince), and there are no extreme outliers in the Residuals vs Leverage plot.\n\npar(mfrow = c(2, 2))\nplot(mod_poly2)",
    "crumbs": [
      "**📘 Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#exponential-regression",
    "href": "module03/043-nonlinear.html#exponential-regression",
    "title": "Non-linear regression",
    "section": "Exponential regression",
    "text": "Exponential regression\nAn exponential relationship either grows or decays at an increasing rate. The equation for an exponential relationship is:\n\\(y = y_0e^{kx}\\)\nWhere,\n\n\\(y\\) is the response and \\(x\\) is the predictor\n\\(y_0\\) is the value of \\(y\\) when \\(x = 0\\)\n\\(k\\) is the rate of change\n\n\\(k\\) can be estimated with the equation \\(slope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}\\), but usually a value of 1 is a good starting point. If it is a decay curve, \\(k\\) will be negative (i.e. -1).\nBecause of the equation for the exponential curve, \\(y_0\\) cannot be zero.\n\nIn practice\n\n\nCode\nset.seed(123)\ngrowth &lt;- tibble(\n  predictor = seq(0, 10, by = 0.2),\n  response = abs(exp(0.5 * predictor) + rnorm(length(predictor), mean = 1, sd = 5))\n)\n\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nBased on the plot, we can estimate the initial parameters for the model. The lower limit (\\(y_0\\)) is around 0, and the rate of change (\\(k\\)) is around 1. We can then fit the model.\n\nmod_exponential &lt;- nls(response ~ y0 * exp(k * predictor),\n  data = growth,\n  start = list(y0 = 0.1, k = 1)\n)\n\nAfter we fit out model, we assess the assumptions plots and check our residuals for INE.\n\nThe residuals vs fitted and standardised residuals vs fitted plots are interpreted similar to linear regression – there is an even spread of points around the ‘0’ horizontal line which indicates equal variances. Recall linearity is not an assumption.\nThe Normal Q-Q plot shows the points are on the line, i.e. residuals are normally distributed.\nThe Autocorrelation plot is new but interpreted similarly to residuals vs fitted – there should be random even scatter around the ‘0’ horizontal line. This is a test for independence, and if there is a pattern (e.g. quadratic, line), it suggests the residuals are not independent.\n\nNonlinear regression is often done with purpose (i.e. the data has a certain shape or there is a known relationship), so the assumptions are often met. If they are not met, the fit and statistics may not be as reliable – but the relationship may be too complex to model altogether.\n\n# Assumption plots\nlibrary(nlstools)\nresids &lt;- nlsResiduals(mod_exponential)\nplot(resids)\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nsummary(mod_exponential)\n\n\nFormula: response ~ y0 * exp(k * predictor)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \ny0   1.1694     0.1291   9.059 4.82e-12 ***\nk    0.4847     0.0121  40.057  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.409 on 49 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 1.204e-06\n\n\nThe model is significant given the p-value is less than 0.05 for all parameters. If this were real data (e.g. population growth), the parameters themselves could be useful, e.g. rate of change. The parameterised model equation is: $ y = 1.17 e^{-0.484x} $.\nThe R-squared value is not reported for nonlinear models as the sum of squares is not partitioned into explained and unexplained components. The residual standard error and plots can be used instead to compare between models.\nThe Number of iterations to convergence is the number of times the computer changed the parameters to try and get a better fit.\n\n\nCode\nggplot(data = growth, aes(x = predictor, y = response)) +\n  geom_point() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  geom_line(aes(y = predict(mod_exponential)), color = \"red\", size = 2)\n\n\n\n\n\n\n\n\n\n\n\nPoor estimation\nIf the starting values are too far (most likely the ‘rate of change’ term), the model will not run and there will be an error. However, there is some flexibility allowed. Below we use some inaccurate initial estimates – the model still reaches the same result (fit, parameters, errors), it just takes more tries (iterations).\n\nmod_exponential &lt;- nls(response ~ y0 * exp(k * predictor),\n  data = growth,\n  start = list(y0 = 10, k = 2)\n)\n\nsummary(mod_exponential)\n\n\nFormula: response ~ y0 * exp(k * predictor)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \ny0   1.1694     0.1291   9.059 4.82e-12 ***\nk    0.4847     0.0121  40.057  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.409 on 49 degrees of freedom\n\nNumber of iterations to convergence: 33 \nAchieved convergence tolerance: 5.692e-07\n\n\n\n\nSelf-starting function\nThe self-starting function for exponential curves can be found in the nlraa package. The function is SSexpf() and has the same formula as above – but has different names for parameters (\\(y_0\\) = \\(a\\), \\(k\\) = \\(c\\)). We can re-define the names of the parameters when we use the function. It reaches the same result with less effort and typically fewer iterations.\n\n# currently not evaluating this code as decay object not found\nlibrary(nlraa)\nmod_exponential &lt;- nls(response ~ SSexpf(predictor, y0, k), data = decay)\nsummary(mod_exponential)",
    "crumbs": [
      "**📘 Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#horizontal-asymptotic-regression",
    "href": "module03/043-nonlinear.html#horizontal-asymptotic-regression",
    "title": "Non-linear regression",
    "section": "Horizontal asymptotic regression",
    "text": "Horizontal asymptotic regression\nAn asymptotic relationship is a type of non-linear relationship where the response variable approaches a limit as the predictor variable increases. This is common in growth curves, where growth is rapid at first and then slows down as it approaches a maximum (e.g. age vs height, population growth, diminishing returns). There are multiple equations for an asymptotic relationship, but we will cover the equation that is covered by the self-starting function SSasymp().\n\\[ y = Asym + (R_0-Asym) \\cdot e^{-e^{lrc} \\cdot x} \\]\n\n\\(R_0\\) is value of \\(y\\) when \\(x = 0\\).\n\\(Asym\\) is the upper limit: the maximum value of \\(y\\).\n\\(lrc\\) is the rate of change: the rate at which \\(y\\) approaches the upper limit.\n\n\nIn practice\nWith the nls() function, we first need to estimate our initial parameters for \\(R_0\\), \\(Asym\\) and \\(lrc\\). We can do this by plotting the data and making an educated guess. For the asymptotic data, we can see that the lower limit (\\(R_0\\)) is around 0 and the upper limit (\\(Asym\\)) is around 100. The rate of change (\\(lrc\\)) is a little harder to estimate, so we will guess a value of 0.8.\nIf the model returns an error (singular gradient matrix at initial parameter estimates), we can try different values – the most likely culprit will be the rate of change (\\(lrc\\)).\n\n\nCode\nggplot(data = asymptotic, aes(x = predictor, y = response)) +\n  geom_point() +\n  geom_hline(yintercept = 100, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  ## plot the rate\n  geom_segment(aes(x = 0, y = 0, xend = 2.5, yend = 100),\n    arrow = arrow(length = unit(0.5, \"cm\")),\n    color = \"red\"\n  ) +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n# Fit the model\nfit_asymptotic &lt;- nls(response ~ Asym + (R0 - Asym) * (exp(-exp(lrc) * predictor)),\n  data = asymptotic,\n  start = list(R0 = 0, Asym = 100, lrc = 0.8)\n)\n\nThe alternative is to use SSasymp() and skip the estimation of the parameters. We simply list the predictor, then the names we want the three parameters to be called. The order is pre-defined, so we must label Asym, R0 and lrc in that order. We could use letters (a, b, c) or any other interpretable names.\n\nmod_asymptotic &lt;- nls(response ~ SSasymp(predictor, Asym, R0, lrc), data = asymptotic)\n\nAfter we fit out model, we assess the assumptions plots and check our residuals for INE – they look fine.\n\n# Assumption plots\nlibrary(nlstools)\nresids &lt;- nlsResiduals(mod_asymptotic)\nplot(resids)\n\n\n\n\n\n\n\n\nFinally, we evaluate the model. The model is significant since the p-value is less than 0.05 for all parameters. There is not much to interpret - except the Residual standard error. We can compare models directly with this error term. R2 is not calculated for non-linear models.\nThe resulting model equation would be \\(y = 98.5 + (-14.5-98.5) \\cdot e^{-e^{-0.463} \\cdot x}\\).\n\nsummary(mod_asymptotic)\n\n\nFormula: response ~ SSasymp(predictor, Asym, R0, lrc)\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nAsym  98.5204     2.2852  43.113  &lt; 2e-16 ***\nR0   -14.5176     6.6416  -2.186  0.03374 *  \nlrc   -0.4626     0.1134  -4.079  0.00017 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.21 on 48 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 3.197e-07\n\n\nWe can then plot the original data and the line of best fit.\n\n\nCode\nggplot(asymptotic, aes(predictor, response)) +\n  geom_point(color = \"black\") + # Original data points\n  geom_line(y = predict(mod_asymptotic), color = \"red\", size = 1) + # Fitted line\n  theme_minimal() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()",
    "crumbs": [
      "**📘 Module 3**",
      "Non-linear regression"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html#logistic-regression",
    "href": "module03/043-nonlinear.html#logistic-regression",
    "title": "Non-linear regression",
    "section": "Logistic regression",
    "text": "Logistic regression\nLogistic regression is a type of non-linear regression typically used when the response variable is binary (0 or 1). The logistic function is an S-shaped or sigmoid curve that models the probability of the response variable being 1. The equation that SSlogis() (base R) assumes \\(y\\) is positive and uses:\n\\[ y = \\frac{Asym}{1+e^{\\frac{xmid-x}{scal}}} \\] where\n\n\\(Asym\\) is the upper limit: the maximum value of \\(y\\).\n\\(xmid\\) is the value of \\(x\\) when \\(y\\) is halfway between the lower and upper limits (inflection point).\n\\(scal\\) is the rate of change: the rate at which \\(y\\) approaches the upper limit.\n\n\nIn practice\nSo if we were to estimate the initial parameters…\n\n\n\n\n\n\n\n\n\nAnd we would estimate the initial parameters for the model. The upper limit (\\(Asym\\)) is around 300, the inflection point (\\(xmid\\)) is around 5, and the rate of change (\\(scal\\)) is around 1.\n\nmod_logistic &lt;- nls(response ~ Asym / (1 + exp((xmid - predictor) / scal)),\n  data = logistic,\n  start = list(Asym = 300, xmid = 5, scal = 1)\n)\n\nBut we can also use the self-starting function SSlogis().\n\nmod_logistic &lt;- nls(response ~ SSlogis(predictor, Asym, xmid, scal), data = logistic)\n\nThe assumptions seem to be met.\n\nresids &lt;- nlsResiduals(mod_logistic)\nplot(resids)\n\n\n\n\n\n\n\n\nAnd then we interpret.\n\nsummary(mod_logistic)\n\n\nFormula: response ~ SSlogis(predictor, Asym, xmid, scal)\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nAsym 319.16801    4.58138   69.67   &lt;2e-16 ***\nxmid   4.97308    0.06899   72.08   &lt;2e-16 ***\nscal   1.37636    0.05176   26.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.763 on 48 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 1.88e-06\n\n\nThe model is significant since the p-value is less than 0.05 for all parameters. The parameterised model is \\(y = \\frac{310}{1+e^{\\frac{4.93-x}{1.35}}}\\). The Residual standard error is 4.41 (which is not bad considering \\(y\\) ranges from 0 ot 300).\nFinally, we visualise our model.\n\n\nCode\nggplot(logistic, aes(predictor, response)) +\n  geom_point(color = \"black\") + # Original data points\n  geom_line(y = predict(mod_logistic), color = \"red\", size = 1) + # Fitted line\n  theme_minimal() +\n  labs(x = \"Predictor\", y = \"Response\") +\n  theme_classic()",
    "crumbs": [
      "**📘 Module 3**",
      "Non-linear regression"
    ]
  }
]