[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX1002 Handbook",
    "section": "",
    "text": "Preface\nWelcome to the ENVX1002 handbook. This handbook is designed to be an optional companion to the lectures, labs and assessments for the course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-handbook",
    "href": "index.html#how-to-use-this-handbook",
    "title": "ENVX1002 Handbook",
    "section": "How to use this handbook",
    "text": "How to use this handbook\nRead this handbook before, or after each lecture to better understand certain concepts. You can also use this book as a reference when working on lab exercises and assessments.\nYou may download a PDF copy of this handbook on the sidebar of the HTML version of this book. You may also download the source code for this book from the GitHub repository.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "ENVX1002 Handbook",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to acknowledge the work of previous generations of teaching staff who created the bulk of the teaching material within this manual. In particular, the work of:\n\nAssoc. Prof. Mick O’Neill\nDr. Kathryn Aufflick\nAssoc. Prof. Peter Thomson\nProf. Thomas Bishop\n\nWhen you are ready, check the sidebar to get started.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "001-intro.html",
    "href": "001-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is ENVX?\nA practical definition of ENVX is “The use of statistical and computing methods to answer quantitative biological questions.”\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#what-is-envx",
    "href": "001-intro.html#what-is-envx",
    "title": "1  Introduction",
    "section": "",
    "text": "ENVX is at the interface of applied statistics, data science and life and environmental science",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#introduction-to-research",
    "href": "001-intro.html#introduction-to-research",
    "title": "1  Introduction",
    "section": "1.2 Introduction to Research",
    "text": "1.2 Introduction to Research\nYou will soon learn that the University of Sydney has a great tradition in research (with many of our efforts earning international recognition). Your teachers will include some experiences of or findings from their research in their classes, and you will have an opportunity to participate in research projects throughout your degree. ENVX comprises of three vertically integrated units designed to equip you to confidently approach the design and data analysis aspects of your research. The diagram below shows one view of the process of research (or experimentation).\n\n\n\nThe research process\n\n\n\n1.2.1 The Use of Statistics in Research\nStatistics provides us with an avenue for exploring and reporting the findings of the research. In terms of reporting, it allows us to give a measure (generally a probability) of the extent to which our conclusion could be wrong. Statistics is a means of informing the decision-making process.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#data",
    "href": "001-intro.html#data",
    "title": "1  Introduction",
    "section": "1.3 Data",
    "text": "1.3 Data\nData is the information we collect on the subjects, factors and variables we are interested in studying. Data may be textual (i.e. words) or numerical.\n\n1.3.1 Observations, samples and populations\nPopulations may be real and therefore able to be listed e.g. all veterinarian practices in the Sydney region. They may be hypothetical and unable to all be listed e.g. all dogs possible that could have a particular treatment applied to them.\nEach sampling unit (or observation) in the population (e.g. plot of land, animal, etc.) has a value y (e.g. nematode count, gestational length): \\(y_1,y_2,y_3,...y_N\\). Typically the population size (N) is very large - even infinite! The population can be described by population parameters e.g. population mean = \\(\\mu\\), population variance = \\(\\sigma^2\\). These are generally characters from the Greek alphabet. Since populations are large, we usually cannot determine these values exactly.\nOften we wish to make generalisations about populations that are too large or too difficult to survey completely. In these cases we sample the population and use characteristics of the sample to extrapolate to characteristics of the larger population. We can define a sample (of size n) as drawn from population “at random”. We call each piece of information recorded about a sampling unit or subject (e.g. plant, person, animal, 1x1m plot of land) an observation.\n\n\n\nn = 20 units (v) sampled at random from population of N = 1,000 (O)\n\n\nThe sample is taken to be representative of the population – but there are no guarantees! It is easier/quicker/cheaper to take a sample than study the entire population (since n &lt;&lt; N). The results obtained from the sample not important in themselves - the importance is in how it can be used to estimate population parameters i.e. \\(\\bar{y}\\) estimates \\(\\mu\\); \\(s^2\\) estimates \\(\\sigma^2\\).\nExample: For a sample of \\(n = 10\\) cows, gestation length in cattle was measured in days. The sample mean \\(\\bar{y} = 345\\) days estimates the population mean \\(\\mu = ???\\) days. The sample standard deviation \\(s = 10\\) days estimates population standard deviation \\(\\sigma = ???\\) days.\n\n\n1.3.2 Variables (types of data)\nThe table below shows fresh weights of cabbages that were included in a field trial that was investigating the effects of irrigation frequency and plant spacing on cabbage yields. This fresh weight measurement is on a continuous scale.\nGenerally in an experiment, several different characteristics of the subjects are measured/recorded. We may score the level of insect damage to leaves of the plant on a scale of 0 to 3 (0 = no damage, 1 = slight damage, 2 = moderate damage, 3 = heavy damage). This is a discrete categorical scale as only certain values on the scale are defined, but it is also an ordered scale. If we had difficulty deciding how to categorize plants using this scale, we might choose to classify plants as either damaged or not damaged, which is a binary measurement scale.\nAlready you can begin to see that from just one fairly simple field trial quite a lot of data can be generated, also data of differing properties/types.\n\n\n\nYields of cabbage (mean fresh weight per head in kg) for 24 plots (Source: Mead, Curnow & Hasted (2003)).\n\n\nData may also be classified as either quantitative (e.g. root length) or qualitative (e.g. plant species). Quantitative observations are based on some sort of measurement e.g. length, weight, temperature, pH. Qualitative observations are based on categories reflecting a quality or characteristic of the observed event e.g. male vs. female, diseased vs. healthy, mutant vs. wild type.\nThe most common types of variables are:\n\nContinuous (and interval) data can assume any value in some (possible unbounded) interval of real numbers. Examples are length, weight, temperature, volume, height.\nDiscrete variables assume only isolated values. E.g. trees per hectare, items per quadrat, number of diseased plants in a section of a glasshouse. They arise from counting – usually either the number of successes in n trials (binary data) OR the number of occurrences of the event in an interval of time or space (count data).\nCategorical variables\n\nBinary variables (listed above as discrete variables) may also be thought of as categorical variables since the subject falls into either of 2 mutually exclusive categories (yes/no, alive/dead, diseased/not diseased etc.).\nOrdinal variables are not measured but nevertheless have a natural ordering. E.g. candidates for political office can be ranked by individual voters. The rank values have no inherent meaning outside the “order” that they provide. That is, a candidate ranked 2 is not twice as preferable as the person ranked 1. (Compare this with measurement variables where a plant 2 feet tall is twice as tall as a plant1 foot tall. With measurement variables such ratios are meaningful, while with ordinal variables they are not.)\nNominal data is qualitative data. Some examples are species, gender, genotype, phenotype, healthy/diseased. Unlike ranked data, there is no “natural” ordering that can be assigned to these categories.\n\n\nNote that some applied statistics texts will define the types of data slightly differently to that shown above.\nIn this unit of study most emphasis will be on the analysis of continuous measurement variables. However a few basic analyses for discrete and categorical variables will be covered.\nRecognising the type of data we have measured is REALLY important as it helps to determine the choice of analysis (and even the descriptive statistics we undertake e.g. the mean of a score doesn’t make sense, but the median is a good alternative measure of central tendency).\n\n\n1.3.3 Relationships between variables\nSometimes you will want to explore the relationship between two (or more) variables that you have measured in your research. You will explore the strength of the relationship and the nature of it (e.g. linear, exponential etc.).\nIn the most simple case, we examine the amount of variability in one variable (Y, the dependent variable) that is explained by changes in another variable (X, the independent variable). Often the X variable is called the predictor and the Y variable the response.\n\n\n1.3.4 Biological and Environmental Variability\nVariation is the norm. It occurs in both observational studies as well as designed experiments. For example, river flow varies from sampling time to sampling time and from sampling location to sampling location. Also, levels of soil contamination on a site vary from site to site.\nThis type of variation distinguishes the biological and environmental sciences from the physical sciences, which shows relatively little variability:\n\nDropping a ball from a certain height: the time to reach the ground is (nearly) identical each time;\nAmount of a chemical product produced when reagents are mixed are (nearly) identical and predictable.\n\nBiological data is far more variable due to environmental and genetic effects. To interpret biological data, we need to control variation by an appropriate experimental design, and adjust for variation by means of statistical analyses.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#statistical-distributions",
    "href": "001-intro.html#statistical-distributions",
    "title": "1  Introduction",
    "section": "1.4 Statistical distributions",
    "text": "1.4 Statistical distributions\nWe use statistical distributions to determine the probability of occurrence of our particular sets of observations. A statistical distribution is a representation (using either mathematical formula or a table) of all possible outcomes of a given event. The most well-known distribution is the normal (or Gaussian) distribution for which the probability distribution function is the familiar bell-shaped curve.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#sampling",
    "href": "001-intro.html#sampling",
    "title": "1  Introduction",
    "section": "1.5 Sampling",
    "text": "1.5 Sampling\nThe aim of sampling is to gain a representative picture of the population. There are various methods and strategies for doing this. Experimental groups/samples must be constructed without bias and must be large enough to give the researcher an acceptable level of confidence in the results.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#hypotheses",
    "href": "001-intro.html#hypotheses",
    "title": "1  Introduction",
    "section": "1.6 Hypotheses",
    "text": "1.6 Hypotheses\nA hypothesis is a tentative explanation for the initial or ad hoc observations made. It suggests a cause and effect or associative relationship that is testable, e.g. yield response to nitrogen (N) fertilizer. The purpose and design of an experiment is to test the hypothesis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#statistical-tests",
    "href": "001-intro.html#statistical-tests",
    "title": "1  Introduction",
    "section": "1.7 Statistical tests",
    "text": "1.7 Statistical tests\nWe decide whether or not a research outcome if significant by conducting a statistical test. Firstly we set arbitrary critical thresholds of probability (P-values). The occurrence of an event whose estimated probability is less than a critical threshold is regarded as a statistically significant outcome. The usual significance level chosen is P&lt;0.05. You will also see P&lt;0.01 and P&lt;0.001 used in research literature. Which statistical test you use (there are many!) depends on the type of data you have collected and the question you wish to ask.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "001-intro.html#software",
    "href": "001-intro.html#software",
    "title": "1  Introduction",
    "section": "1.8 Software",
    "text": "1.8 Software\nIn this unit of study, we will focus on the use of one statistical package called R. It has been designed specifically for statistical analysis and is freely available. It is a powerful tool for data analysis and is widely used in the biological and environmental sciences.\nYou will also learn to use Microsoft Excel to organize and summarise your data. In the next section some of the basic features of Excel are presented. See computer lab 1.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "020-exploring_data.html",
    "href": "020-exploring_data.html",
    "title": "2  Exploratory Data Analysis EDA",
    "section": "",
    "text": "2.1 Numerical Summaries\nThe summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g. sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis EDA</span>"
    ]
  },
  {
    "objectID": "020-exploring_data.html#numerical-summaries",
    "href": "020-exploring_data.html#numerical-summaries",
    "title": "2  Exploratory Data Analysis EDA",
    "section": "",
    "text": "2.1.1 Measures of Central Tendency\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (“mu”) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (“sigma”) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read “y bar”) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)’s to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\n[1] 5\n\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the “middle” value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read “y tilde”) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\n[1] 5.5\n\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\n[1] 6\n\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g. height) or discrete (e.g. number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n2.1.2 Measures of Spread\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1   # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3   # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(dnorm(x_values, mean_value, std_dev_A), \n        dnorm(x_values, mean_value, std_dev_B), \n        dnorm(x_values, mean_value, std_dev_C)),\n  curve = factor(c(rep(\"A\", length(x_values)), \n                   rep(\"B\", length(x_values)), \n                   rep(\"C\", length(x_values)))\n  )\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(title = \"Normal Distributions with Different Spreads\",\n       x = \"Value\",\n       y = \"Density\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"),\n                     labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")) +\n  theme_minimal()\n\n\n\n\nFigure 1.1 A, B & C are distributions with the same mean (zero) but varying standard deviations.\n\n\n\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more “spread” out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or “spread” is the range – the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you’ve taken. [It’s unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y)-min(y)\n\n[1] 7\n\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the “spread”. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n 75% \n11.5 \n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25) \n\n75% \n 10 \n\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g. body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n“The degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e. the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.”\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis EDA</span>"
    ]
  },
  {
    "objectID": "020-exploring_data.html#tabular-summaries",
    "href": "020-exploring_data.html#tabular-summaries",
    "title": "2  Exploratory Data Analysis EDA",
    "section": "2.2 Tabular Summaries",
    "text": "2.2 Tabular Summaries\n\n2.2.1 Tables of means\nTables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5),  # Assuming a Poisson distribution for count data \n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nTreatment\nMeanSedgePlants\n\n\n\n\nControl\n5.060\n\n\nHigh\n20.050\n\n\nLow\n9.955\n\n\nMedium\n14.860\n\n\n\n\n\n\n\n2.2.2 Frequency Tables\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e. \\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e. \\(\\Sigma^r_{i=1}f_i/n \\times 100\\)",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis EDA</span>"
    ]
  },
  {
    "objectID": "020-exploring_data.html#graphical-summaries",
    "href": "020-exploring_data.html#graphical-summaries",
    "title": "2  Exploratory Data Analysis EDA",
    "section": "2.3 Graphical Summaries",
    "text": "2.3 Graphical Summaries\n\n2.3.1 Bar Graphs and Histograms\nFrequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"data/Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count)/sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\n\n\n\nFigure 1.2 Relative frequency histogram of sedge plant data.\n\n\n\n\nContinuous measurements are free to take any whole or fractional number within their range e.g. plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"data/Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\n\n\n\nFigure 1.3 Frequency histogram of creeping bentgrass data.\n\n\n\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ≈ 95 mm (the centre of the data) and standard deviation ≈ 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e. a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e. 30 or more data values).\n\n\n2.3.2 Boxplots\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) “plugs” were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\n\n\n\n\n\n\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g. Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.500   9.575  10.500  10.440  11.375  12.800 \n\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the “box”\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a “*” or \\(\\bullet\\).\n\nStep 3: Determining the “whisker” lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e. to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e. to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\n\n\n\n\n\n\n2.3.3 Scatterplots\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis EDA</span>"
    ]
  },
  {
    "objectID": "020-exploring_data.html#shapes-of-distributions",
    "href": "020-exploring_data.html#shapes-of-distributions",
    "title": "2  Exploratory Data Analysis EDA",
    "section": "2.4 Shapes of Distributions",
    "text": "2.4 Shapes of Distributions\n\n2.4.1 Symmetric\nThis is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we’ll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\n Root_length_mm  \n Min.   : 65.00  \n 1st Qu.: 86.75  \n Median : 93.00  \n Mean   : 93.86  \n 3rd Qu.:102.00  \n Max.   :135.00  \n\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the “x” at the right hand side of the graph. The “4” indicates that this is the 4th observation in the data set i.e. the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1+p2\n\n\n\n\nFigure 1.5 Boxplot (left) and histogram (right) of the creeping bentgrass data show symmetry.\n\n\n\n\n\n\n2.4.2 Positively Skewed\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"data/TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n    TcCB_ppb     \n Min.   : 0.010  \n 1st Qu.: 0.235  \n Median : 0.570  \n Mean   : 1.412  \n 3rd Qu.: 1.292  \n Max.   :26.600  \n\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1+p2\n\n\n\n\nFigure 1.6 Boxplot (left) and histogram (right) of the TcCB data showing positive skewness.\n\n\n\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n2.4.3 Negatively Skewed\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"data/Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n   AgeAtOnset    \n Min.   : 3.800  \n 1st Qu.: 7.225  \n Median :11.150  \n Mean   : 9.714  \n 3rd Qu.:12.000  \n Max.   :13.200  \n\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1+p2\n\n\n\n\nFigure 1.7 Boxplot (left) and histogram (right) of arthritis data showing negative skewness.\n\n\n\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n2.4.4 Statistics of Shape\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\n\nRoot_length_mm \n    0.08475045 \n\nskewness(tccb)\n\nTcCB_ppb \n5.998866 \n\nskewness(arthritis)\n\nAgeAtOnset \n-0.8257301 \n\n\nKURTOSIS\nKurtosis is a measure of how “peaked” (leptokurtic) a frequency distribution is or how “flattened” (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\n\nRoot_length_mm \n      3.354701 \n\nkurtosis(tccb)\n\nTcCB_ppb \n46.11769 \n\nkurtosis(arthritis)\n\nAgeAtOnset \n  2.030982 \n\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g. weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis EDA</span>"
    ]
  },
  {
    "objectID": "020-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "href": "020-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "title": "2  Exploratory Data Analysis EDA",
    "section": "2.5 Normality versus Non-Normality (in the Descriptive Statistics Context)",
    "text": "2.5 Normality versus Non-Normality (in the Descriptive Statistics Context)\n\n2.5.1 Normality\nMost natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is “flatness”).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n2.5.2 Non-Normality\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We’ll look at these later.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory Data Analysis EDA</span>"
    ]
  },
  {
    "objectID": "021-probability_distributions.html",
    "href": "021-probability_distributions.html",
    "title": "3  Probability distributions",
    "section": "",
    "text": "3.1 Probability\nSimple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\nComplementary Events\nProbability of an event not occurring = 1 – probability of event occurring.\n\\(P(E) =1− P(E)\\)\nExercises\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) – P(A \\text{ and } B)\\)\nExercise\nProduct Rule\nWhen we do more than one thing (e.g. toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "021-probability_distributions.html#probability",
    "href": "021-probability_distributions.html#probability",
    "title": "3  Probability distributions",
    "section": "",
    "text": "\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\n\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\n\n\n\n\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\n\n\n\n\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\n\n\n\n\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "021-probability_distributions.html#probability-density-functions",
    "href": "021-probability_distributions.html#probability-density-functions",
    "title": "3  Probability distributions",
    "section": "3.2 Probability Density Functions",
    "text": "3.2 Probability Density Functions\nProbability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "021-probability_distributions.html#types-of-distributions",
    "href": "021-probability_distributions.html#types-of-distributions",
    "title": "3  Probability distributions",
    "section": "3.3 Types of distributions",
    "text": "3.3 Types of distributions\nJust as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g. normal distribution, or discrete e.g. binomial distribution. In this unit of study we will only consider continuous distributions.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "021-probability_distributions.html#probability-for-continuous-distributions",
    "href": "021-probability_distributions.html#probability-for-continuous-distributions",
    "title": "3  Probability distributions",
    "section": "3.4 Probability for Continuous Distributions",
    "text": "3.4 Probability for Continuous Distributions\nFor discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g. the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don’t mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nFigure 3.1 The symmetric bell shaped distribution – just one distribution in the family of continuous distributions.\n\n\n\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ≤ Y ≤ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nFigure 3.2 The shaded area under the PDF curve for a normal distribution is interpreted as the probability of a value occurring within a defined range.\n\n\n\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student’s T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these… If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = 'Standard Normal')\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = 'Log-Normal')\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) + \n  geom_line() + \n  facet_wrap(~Distribution, scales = 'free_x') + \n  theme_minimal() + \n  labs(title = \"Log-Normal PDF vs. Normal PDF\", \n       x = \"Value\", \n       y = \"Density\")\n\n\n\n\nThe normal and log normal distributions",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "021-probability_distributions.html#the-normal-distribution",
    "href": "021-probability_distributions.html#the-normal-distribution",
    "title": "3  Probability distributions",
    "section": "3.5 The Normal Distribution",
    "text": "3.5 The Normal Distribution\nWe began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the “bell-shaped” distribution commonly observed in histograms of biological and environmental data e.g. height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at μ and σ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ’the variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e. \\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g. \\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n3.5.1 Calculating Probabilities for Standardised Normal Values\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e. probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e. 50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n[1] 0.5\n\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n[1] 0.9678432\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1-pnorm(1.85)\n\n[1] 0.03215677\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\\(P(–1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; –1) \\approx  0.1587\\) (from R)\n\\(P(–1 &lt; Z &lt; 2) \\approx  0.9772 – 0.1587 \\approx  0.8185\\)\n\npnorm(2)\n\n[1] 0.9772499\n\npnorm(-1)\n\n[1] 0.1586553\n\npnorm(2) - pnorm(-1)\n\n[1] 0.8185946\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.5.2 Calculating Probabilities for Non-Standardised Normal Values\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e. \\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.5.3 Calculating Probabilities for Non-Standardised Normal Values\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n[1] 0.02275013\n\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n[1] 1.071029\n\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n[1] 1.728971\n\n\n\n\n3.5.4 Exploring the Properties of the Normal Distribution - fun with Excel\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we’ve obtained these probabilities we’ll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‘z’; ‘PDF via formula’; ‘PDF via NORMDIST’.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we’re going to obtain the corresponding probabilities in 2 different ways – you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don’t need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution…\n\n\n3.5.5 Normality Tests\nNormality tests are the first introduction you’ll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe’ll expand these steps soon…\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e. whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"data/TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\n\n    Shapiro-Wilk normality test\n\ndata:  TcCB$TcCB\nW = 0.40034, p-value &lt; 2.2e-16\n\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability distributions</span>"
    ]
  },
  {
    "objectID": "022-sampling_distributions.html",
    "href": "022-sampling_distributions.html",
    "title": "4  Sampling distributions",
    "section": "",
    "text": "4.1 The Central Limit Theorem\nThe Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p. 23 Mead et al. (1993)]\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"data/Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\n\n[1] 301.8958\n\nsd(carrots$Weight_g)\n\n[1] 221.3131\n\nggplot(carrots, aes(x=Weight_g)) + \n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\n\n[1] 301.435\n\nsd(sample_means)\n\n[1] 104.5059\n\nggplot() + \n  geom_histogram(aes(x=sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\n\n[1] 300.7962\n\nsd(sample_means)\n\n[1] 43.70914\n\nggplot() + \n  geom_histogram(aes(x=sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "022-sampling_distributions.html#the-central-limit-theorem",
    "href": "022-sampling_distributions.html#the-central-limit-theorem",
    "title": "4  Sampling distributions",
    "section": "",
    "text": "Create a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\n\n\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?",
    "crumbs": [
      "Describing data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Sampling distributions</span>"
    ]
  },
  {
    "objectID": "030-ttest1.html",
    "href": "030-ttest1.html",
    "title": "5  One-sample \\(t\\)-test",
    "section": "",
    "text": "5.1 Confidence intervals for \\(\\mu\\)\nConfidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "030-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "href": "030-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "title": "5  One-sample \\(t\\)-test",
    "section": "5.2 Confidence interval for \\(\\mu\\) (when \\(\\sigma\\) is known and data is normally distributed)",
    "text": "5.2 Confidence interval for \\(\\mu\\) (when \\(\\sigma\\) is known and data is normally distributed)\n(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n[1] -1.959964\n\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\n[1] 0.5977601\n\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci\n\n[1] 0.3233181 0.4833485",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "030-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "href": "030-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "title": "5  One-sample \\(t\\)-test",
    "section": "5.3 Confidence interval for \\(\\mu\\) (when \\(\\sigma\\) is NOT known and data is normally distributed)",
    "text": "5.3 Confidence interval for \\(\\mu\\) (when \\(\\sigma\\) is NOT known and data is normally distributed)\nThere are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n5.3.1 Student’s T Distribution\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n5.3.2 T-Based Confidence Interval\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\n\n[1] -2.063899\n\nqt(0.975, 24)\n\n[1] 2.063899\n\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "030-ttest1.html#what-is-meant-by-confidence-interval",
    "href": "030-ttest1.html#what-is-meant-by-confidence-interval",
    "title": "5  One-sample \\(t\\)-test",
    "section": "5.4 What is meant by confidence interval?",
    "text": "5.4 What is meant by confidence interval?\nIf an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "030-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "href": "030-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "title": "5  One-sample \\(t\\)-test",
    "section": "5.5 Data Transformation and the Lognormal Distribution",
    "text": "5.5 Data Transformation and the Lognormal Distribution\nAs you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"data/BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\n\n[1] 0.529368\n\nkurtosis(bcc_df$BloodCellCount)\n\n[1] 3.292761\n\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$BloodCellCount\nW = 0.96042, p-value = 6.607e-09\n\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\n\n[1] -0.1373212\n\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n[1] 2.854581\n\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$sqrt_BloodCellCount\nW = 0.97117, p-value = 4.13e-07\n\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\n\n[1] -0.8655114\n\nkurtosis(bcc_df$log_BloodCellCount)\n\n[1] 3.684001\n\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$log_BloodCellCount\nW = 0.92515, p-value = 3.025e-13\n\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"data/TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\n\n[1] 2.691321\n\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n[1] 13.53575\n\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tccb_df$sqrt_TcCB_ppb\nW = 0.76119, p-value = 1.869e-11\n\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\n\n[1] -0.02521078\n\nkurtosis(tccb_df$log_TcCB_ppb)\n\n[1] 3.449717\n\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tccb_df$log_TcCB_ppb\nW = 0.99555, p-value = 0.9868\n\n\nTransformation successful - symmetric distribution\n\n5.5.1 The Natural Log (\\(\\Log_e\\)) Transformation\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log y = y has a normal N(,2) distribution, then the original variable has a lognormal distribution with parameters  and 2, y ~ LN(,2). Note that  and 2 are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal LN(,2). distribution are Mean\nVariance\nWe can illustrate these relationships by using the parameter estimates and from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>One-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html",
    "href": "031-ttest2.html",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "",
    "text": "6.1 Assumptions\nThe two-sample \\(t\\)-test assumes that the data are :\nIdeally these assumptions should be tested before you carry about the two-sample \\(t\\)-test.",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html#assumptions",
    "href": "031-ttest2.html#assumptions",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "",
    "text": "Continuous,\nat least approximately normally distributed, and\nthe variances of the two sets are homogeneous (i.e. the same).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html#what-if-assumptions-are-not-met",
    "href": "031-ttest2.html#what-if-assumptions-are-not-met",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "6.2 What if assumptions are not met?",
    "text": "6.2 What if assumptions are not met?\nIf the data do not meet either of these assumptions, then you may choose an appropriate transformation or look for another technique to use e.g. a non-parametric method (see non-parametric sections of this book).\nIf the variances cannot be assumed to be equal (but the data are normally distributed), there is a way of adjusting the two-sample \\(t\\)-test to compensate for this – it’s called the Satterthwaite’s approximation – more on this later!",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html#three-variants-of-the-two-sample-t-test",
    "href": "031-ttest2.html#three-variants-of-the-two-sample-t-test",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "6.3 Three Variants of the Two-Sample \\(t\\)-test",
    "text": "6.3 Three Variants of the Two-Sample \\(t\\)-test\nThere are 3 different ways in which a two-sample \\(t\\)-test can be used. They ALL assume that the data is approximately normally distributed.\n\nIndependent samples, equal variance - run a two-sample \\(t\\)-test assuming equal variances.\nIndependent samples, unequal variance - run a two-sample \\(t\\)-test assuming unequal variances.\nPaired samples - run a paired \\(t\\)-test.\n\nIn choosing which variant of the \\(t\\)-test is applicable in your situation, you first need to decide whether your two samples are paired or unpaired (independent).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html#paired-data",
    "href": "031-ttest2.html#paired-data",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "6.4 Paired Data",
    "text": "6.4 Paired Data\nPaired samples arise when we measure pairs of similar experimental units. In this pair of experimental units, one unit receives Treatment 1 and the other receives Treatment 2.\nIn some cases, treatments are applied to the same experimental unit e.g. one half of a piece of fruit receives Trt 1 and the other half Trt 2; two plants of different varieties are grown in the same pot (here variety is the treatment).\nIn recognising this pairing in our analysis we are taking into account the fact that biological variation between pairs is likely to be larger than within pairs. This way, we get a clearer picture of the difference that is due to the treatment factor.\n\n6.4.1 Examples of paired data\n\nObservations at two times on the same experimental unit\n\nBefore and after readings of particle matter in the air on 3 sites near a new power station (before the station was built, and after it became operational). (Dytham 2003, 80)\nMeasurements of water flow on two consecutive days at 6 sites along a river. (Dytham 2003, 83)\n\nObservations on 2 halves/parts of the same experimental unit\n\nOne half of each (uncut) grapefruit was exposed to sunlight, and the other half was shaded (McConway et al 1999, p. 198).\nA standard (recommended) variety of wheat is compared with a new variety via 2 similar plots on each of 8 farms (Clewer and Scarisbrick 2001, p. 46).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html#two-sample-t-test-for-independent-samples",
    "href": "031-ttest2.html#two-sample-t-test-for-independent-samples",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "6.5 Two-Sample t-Test for Independent Samples",
    "text": "6.5 Two-Sample t-Test for Independent Samples\nProcedure for the test:\n\nSet up the null and alternate hypotheses\nDecide on the level of significance, 5%, 1%, 0.01% etc.\nCheck the assumptions of normality and equal variance. We use an F-test to formally test for equality of variance.\nCalculate the test statistic \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{SED}\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means, and \\(SED\\) is the standard error of the difference between the means.\nCalculate the degrees of freedom (df).\nFind the P-value in printed statistical tables or via GenStat or Excel.\nMake a statistical conclusion by comparing this P-value to your chosen level of significance (if P &lt;α, then reject null hypothesis).\nCalculate the confidence interval.\nInterpret your results biologically.\n\n\n\n\n\n\n\nCalculating the test statistic\n\n\n\nWhen calculating the test statistic, use \\(y_1\\) as the larger mean. This will give a positive value for \\(t\\).",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "031-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "href": "031-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "title": "6  Two-sample \\(t\\)-test",
    "section": "6.6 Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)",
    "text": "6.6 Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)\nA two-sample t-test shows whether there is evidence of a difference in population means. The magnitude of this difference can be estimated with a confidence interval.\nA 95% confidence interval for the true difference \\(\\mu_1 - \\mu_2\\) is given by \\[\\bar{y}_1 - \\bar{y}_2 \\pm t^{\\alpha/2}_{df} \\times SED\\]\nwhere \\(\\bar{y}_1 - \\bar{y}_2\\) is the difference between the sample means, \\(t^{\\alpha/2}_{df}\\) is the critical value from the t-distribution for the chosen level of significance and degrees of freedom, and \\(SED\\) is the standard error of the difference between the means.\nThe df and SED need to take into account whether or not you are assuming equal variances.\n6.6 SED and df for Independent Samples with EQUAL Variances\n6.7 SED and df for Independent Samples with UNequal Variances\nWhat do you do if when you’re checking your assumptions for a two- sample t-test you find… Normality  Equality of variance \nYou can go ahead with a modified t-test or you can choose a different test.\nThis modified t-test used in the case of unequal variances is often called Satterthwaite’s approximate t-test.\n6.7.1 Satterthwaite’s Approximate t-Test\nThe null and alternate hypotheses remain the same, i.e. H0: μ1 = μ2 or H0: μ1 - μ2 = 0\nThe formula for the test statistic, t, in Satterthwaite’s approximate test is a little different to that for the t-test with equal variances. The s.e.d. changes because we can no longer use a pooled estimate of the variance (since the variances cannot be assumed equal).\nA correction for unequal variance is made to the degrees of freedom.\nThen proceed as usual through the rest of the test - i.e. find the P-value; draw a statistical conclusion about your hypothesis; interpret your results biologically.\n6.8 F-Test for Equality of Variances\nOne of the conditions for the independent sample t-test to be valid is that the population variances σ12 and σ22 are equal.\nTo test the null hypothesis that σ12 = σ22 divide the larger s2 value by the smaller s2 to obtain the variance ratio, v.r.:\nTo undertake this two-tailed test at the 5% level you need to carry out the one-tailed test at the 2.5% level. You can use the 2.5% F table of critical values, or you can use GenStat via the menus Data&gt;Probability Calculations… to find the P-value.\nIf you use the printed statistical table, you will need to compare the critical value you find there with the variance ratio you calculated. If the calculated variance ratio &gt; critical value, reject H0 and conclude that the variances are significantly different.\n6.9 Example: Two-Sample t-Test\nWeights of two breeds of cattle are to be compared: 15 cattle from Breed 1 and 12 cattle from Breed 2 were randomly sampled. Their recorded weights (kg) are shown.\nBreed 1 Breed 2 148.1 187.6 146.2 180.3 152.8 198.6 135.3 190.7 151.2 196.3 146.3 203.8 163.5 190.2 146.6 201.0 162.4 194.7 140.2 221.1 159.4 186.7 181.8 203.1 165.1\n165.0\n141.6\nThe following descriptive statistics are obtained from these data.\nBreed 1 Breed2\nSample mean (kg) 153.700 196.175 Sample s.d. (kg) 12.301 10.616\nIs there any systematic difference in their weights? The question, “Is there any systematic difference in their weights?”, is asking whether or not there is any significant difference between the 2 samples – and in effect if there is any significant difference between the 2 breeds because the factor that distinguishes the 2 samples is breed.\nWe could answer this question by testing whether or not the population mean weights for the 2 breeds can be assumed to be equal:\nH0: μbreed 1 = μBreed 2 versus H1: μBreed 1 ≠ μBreed 2\nTo test this particular hypothesis about the equality of the means (as an avenue for answering our broader question), we would use a two-sample t-test.\nHowever, it is REALLY important to remember that there are other tests and hypotheses that we could use to answer our broader question (about whether or not there is any statistical difference between the weights of the 2 breeds). We’ll consider some non-parametric alternatives in Biometry 2.\nTo proceed with a two-sample t-test, there are 2 assumptions that we need to check: • Normality of Breed 1 data and normality of Breed 2 data • Equality of variances of the 2 samples\nBoth of these checks are hypothesis tests in their own right and contain the usual elements of a hypothesis test i.e. null & alternate hypothesis; test statistic; df; P-value or critical value; conclusion.\nTesting the Assumption of Normality If you are doing the test by hand, you would need to assume that the data are normally distributed (and hope this is true). At least the data in this example is continuous… which is one small step towards normality.\nTesting the Assumption of Equality of Variance H0: σ2Breed 1 = σ2Breed 2 versus H1: σ2Breed 1 ≠ σ2Breed 2\nTest statistic:\nThere are 2 degrees of freedom to calculate for an F test. They are called the numerator df, 1 and the denominator df, 2. (Remember from school days, numerator is the top half of a fraction, denominator is the bottom half of a fraction.) The  that looks like a curly ‘v’ is the Greek letter ‘nu’.\nFor an F test used to test equality of variance, df are: 1 = n1 – 1, 2 = n2 – 1.\nHere, 1 = nBreed 1 -1 = 15 - 1 = 14 and 2 = nBreed 2 - 1 = 12 – 1 = 11.\nUsing the 2.5% F table, we can compare (at the upper tail) the Fcritical and Fobserved values. If Fobs &gt; Fcrit, we reject H0 and conclude that the variances of the 2 samples are NOT equal.\nFrom these tables, it is not possible to find exactly, so we will make do with the closest possible value .\nWe find Fcrit ≈ 3.33 and Fobs = 1.34. Since Fcrit &gt; Fobs ,we CAN assume that the variances are equal.\nSo… assuming normality and equal variances, we can complete a “pooled” two-sample t-test where\nTesting the null hypothesis that H0: μBreed 1 = μBreed 2, we find that there is a significant difference in the mean weights of the two breeds of cattle (T = 9.46, df = 25, P &lt; 0.001). The mean weight of Breed 2 is significantly higher and we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1.\nThis last piece of information, “we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1”, is obtained from the 95% confidence interval for the true difference (μ1 – μ2).\nNB. Recall that a 95% confidence interval for the true difference (μ1 – μ2) is .\n6.10 Paired t-Test\nWe can re-write the generic null hypothesis for a two-sample test of means, H0: μ1 = μ2 as… H0: μ1 – μ2 = 0, or H0: μd = 0.\nThe paired t-test is actually a one-sample t-test of the differences between pairs of observations (from 2 different samples).\nAssumption: • Data is approximately normally distributed.\nSince we are doing a one-sample t-test on the differences then the assumption of equal variances is not relevant. Firstly, create a single column of data to use in the t test. Each value in the column corresponds to the difference between the 2 values for a particular matched pair.\nFarm Yield Variety A (kg) Yield Variety B (kg) Difference 1 17.8 14.7 3.1 2 18.5 15.2 3.3 3 12.2 12.9 -0.7 4 19.7 18.3 1.4 5 10.8 10.1 0.7 6 11.9 12.2 -0.3 7 15.6 13.5 2.1 8 12.5 9.9 2.6\nThe difference for Farm 1 = 17.8 – 14.7 = 3.1.\nData Source: Clewer and Scarisbrick (2001, 46)\nSecondly, find some summary statistics of the differences so you can complete the t test:\nNo. of values, n = 8        Sum = 12.2          Mean = 1.525\nVariance = 2.299            Std Dev = 1.516\nand df = n -1, where d is usually 0.\n6.11 Confidence Interval for 1 – 2 (Paired Samples)\nA 95% confidence interval for the mean difference is",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-sample $t$-test</span>"
    ]
  },
  {
    "objectID": "032-nonparametric1.html",
    "href": "032-nonparametric1.html",
    "title": "7  Chi-squared tests",
    "section": "",
    "text": "The chi-squared distribution (where chi is pronounced ‘ky’) is a very widely used distribution in statistics. Its symbol is 2. It has MANY applications. Here we will consider only two of these applications – tests of agreement with expected outcomes, and contingency tables.\n7.1 Notes on the 2 Distribution\nThe density function for a 2 distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the 2 density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g. 1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the 2 distribution approaches the shape (and properties) of a normal distribution.\nFigure 7.1 The shape of the 2 density function for various degrees of freedom.\nThe mean and variance of a 2 variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as  (Greek n), then\nMean of   variable =   (i.e. mean = df)\nVariance of   variable = 2 (i.e. variance = twice the df)\nCritical values of a 2 distribution are given in the 2 probability table that appears as Appendix A.5.\n7.2 Testing Agreement of Frequency Data with Expectation Models\n7.2.1 Steps in Chi-Squared Tests of Agreement\nThe process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\nChoose an appropriate hypothesis test for the type of data you have, and the type of question you’re asking.\nChoose the level of significance for the test.\nWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\nCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\nCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic – however there are alternative tests designed to accommodate these situations.\nCalculate the test statistic and degrees of freedom.\nObtain the P-value.\nDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n7.2.2 Examples: Testing Whether Outcomes are Equally Probable\nEXAMPLE 1\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a “fair” coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a “fair” coin.\nEXAMPLE 2 (from Mead et al, 2003)\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\nCheese First preference A 5 B 7 C 18 D 10 Total 40\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n =  =9.80.\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the value of 9.80 is greater than the value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since P &lt; 0.05.)\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with\n =  =…      etc.\n7.2.3 Example: Testing Whether Outcomes are in Expected Proportions (from Mead et al, 2003)\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen).\nThe figures obtained for the primula plants follow. Normal eye Primrose Queen eye Total Flat leaves 328 122 450 Crimped leaves 77 33 110 Total 405 155 560\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, ¾ of the total 560 observation in the first class of the characteristic and the remaining ¼ in the second class. Further, this model predicts that ¾ of the flat-leaved plants should have normal eyes, resulting in ¾ × ¾ of all the plants or 9/16 with flat leaves and normal eyes; the remaining ¼ of the flat-leaved plants, which is ¼ × ¾ or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes.\nThe calculation of these expected or predicted proportions is shown below.\nNormal eye  Primrose Queen eye\nFlat leaves ¾ × ¾ = 9/16 ¼ × ¾ = 3/16 Crimped leaves ¾ × ¼ = 3/16 ¼ × ¼ = 1/16\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35.\nThe observed and expected frequencies are summarized in the table below.\nNormal eye  Primrose Queen eye\nFlat leaves 328 (315) 122 (105) Crimped leaves 77 (105) 33 (35)\n    =  \n    = 0.54 + 2.75 + 7.47 + 0.11 = 10.77. \nWe compare 10.77 with the 5% point of the distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable.\nSee pp. 332-333 of Mead et al, 2003 for what to do next… after rejecting the model.\n7.3 Contingency Tables\n7.3.1 Example: (2 x 2) Contingency Table\nConsider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\nRecovered   \nYes No  Total\nProcedure 1 14 6 20 Procedure 2 8 12 20 Total 22 18 40\nThis is one form of a 22 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\nSolution: We will perform a statistical hypothesis test:\nH0: There is no difference in the true recovery rates for animals on either procedure H1: The recovery rates do differ.\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\nH0: p1 = p2\nH1: p1 ≠ p2\nEstimates of individual recovery rates are = 14/20 = 0.7 and = 8/20 = 0.4. Is this difference due to chance?\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n  =  .\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be 20 22/40 = 20 0.55 = 11 animals. In general this can be written as:\nSo the expected frequencies for the cells in the table are:\n  Expected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\nRecovered   \nYes No  Total\nProcedure 1 14 (11) 6 (9) 20 Procedure 2 8 (11) 12 (9) 20 Total 22 18 40\nThe table shows observed (and expected) frequencies. The 2 test statistic is then calculated using:\nLarge values of 2 indicate discrepancies between observed and expected frequencies, i.e. large values indicate that H0 should be rejected in favour of H1.\nThe df of this 2 test is 1 for a 22 contingency table. In general,\nIf H0 is true, the observed 2 is just one observation from a 2 distribution with 1 df:\nSince , there is (just) not sufficient evidence to reject H0. Thus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n  7.3.2 Example: (4 x 3) Contingency Table\nThe second example is a 43 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n    Disease Status      \nVaccine No Mild Severe Total Control 100 (137.3) 71 (42.6) 29 (20.1) 200 A 146 (133.9) 32 (41.6) 17 (19.6) 195 B 149 (132.5) 28 (41.2) 16 (19.3) 193 C 146 (137.3) 37 (42.6) 17 (20.1) 200 Total 541 168 79 788\nThe table shows observed (and expected) frequencies.\nWe test H0 that there is no association between disease status and vaccination given, i.e. all vaccinations have equal effectiveness.\nAssuming H0 is true, the expected frequencies are calculated as follows:\ne.g. Expected frequency for an animal in the Control, No disease group:\n.\nAs before, the test statistic is and this will have (4-1)(3-1) = 6 df.\nThis is how to do the test in the Stats menu in GenStat:\nStats &gt; Statistical Tests &gt; Contingency Tables…\nGenStat requires the data to be set up as a 43 Table type of spreadsheet (as opposed to a Variate or Matrix). It then reports the X2 test statistic and P-Value by selecting the Pearson method.\nPearson chi-squared value is 45.22 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\nLikelihood chi-squared value is 43.34 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001   The ML 2 is calculated as follows:\nThe degrees of freedom are (r – 1)(c – 1) as before, where r and c are the numbers of rows and columns respectively.\nIn general, the 2 approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\nDisease Status  \nVaccine No Mild Severe Total Control 10 7 3 20 A 15 3 2 20 B 15 3 2 20 C 15 4 2 21 Total 55 17 9 81",
    "crumbs": [
      "Inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chi-squared tests</span>"
    ]
  },
  {
    "objectID": "040-describing_relationships.html",
    "href": "040-describing_relationships.html",
    "title": "8  Describing relationships",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Describing relationships</span>"
    ]
  },
  {
    "objectID": "041-linear_functions.html",
    "href": "041-linear_functions.html",
    "title": "9  Linear functions",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear functions</span>"
    ]
  },
  {
    "objectID": "042-linear_functions_multi.html",
    "href": "042-linear_functions_multi.html",
    "title": "10  Linear functions – multiple predictors",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear functions -- multiple predictors</span>"
    ]
  },
  {
    "objectID": "043-nonlinear.html",
    "href": "043-nonlinear.html",
    "title": "11  Non-linear functions",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Non-linear functions</span>"
    ]
  }
]