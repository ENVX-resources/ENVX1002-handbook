[
  {
    "objectID": "module01/01-exploring_data.html",
    "href": "module01/01-exploring_data.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g. sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).\n\n\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (“mu”) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (“sigma”) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read “y bar”) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)’s to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the “middle” value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read “y tilde”) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g. height) or discrete (e.g. number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1 # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3 # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(\n    dnorm(x_values, mean_value, std_dev_A),\n    dnorm(x_values, mean_value, std_dev_B),\n    dnorm(x_values, mean_value, std_dev_C)\n  ),\n  curve = factor(c(\n    rep(\"A\", length(x_values)),\n    rep(\"B\", length(x_values)),\n    rep(\"C\", length(x_values))\n  ))\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(\n    title = \"Normal Distributions with Different Spreads\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  scale_color_manual(\n    values = c(\"red\", \"green\", \"blue\"),\n    labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")\n  ) +\n  theme_minimal()\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more “spread” out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or “spread” is the range – the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you’ve taken. [It’s unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y) - min(y)\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the “spread”. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25)\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g. body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n“The degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e. the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.”\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]\n\n\n\n\n\n\nTables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5), # Assuming a Poisson distribution for count data\n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e. \\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e. \\(\\Sigma^r_{i=1}f_i/n \\times 100\\)\n\n\n\n\n\n\nFrequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"data/Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count) / sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\nContinuous measurements are free to take any whole or fractional number within their range e.g. plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"data/Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ≈ 95 mm (the centre of the data) and standard deviation ≈ 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e. a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e. 30 or more data values).\n\n\n\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) “plugs” were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g. Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the “box”\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a “*” or \\(\\bullet\\).\n\nStep 3: Determining the “whisker” lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e. to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e. to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.\n\n\n\n\n\n\nThis is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we’ll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the “x” at the right hand side of the graph. The “4” indicates that this is the 4th observation in the data set i.e. the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1 + p2\n\n\n\n\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"data/TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1 + p2\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"data/Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1 + p2\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\nskewness(tccb)\nskewness(arthritis)\n\nKURTOSIS\nKurtosis is a measure of how “peaked” (leptokurtic) a frequency distribution is or how “flattened” (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\nkurtosis(tccb)\nkurtosis(arthritis)\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g. weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.\n\n\n\n\n\n\nMost natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is “flatness”).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We’ll look at these later.",
    "crumbs": [
      "**📕 Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#numerical-summaries",
    "href": "module01/01-exploring_data.html#numerical-summaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g. sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).\n\n\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (“mu”) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (“sigma”) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read “y bar”) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)’s to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the “middle” value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read “y tilde”) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g. height) or discrete (e.g. number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1 # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3 # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(\n    dnorm(x_values, mean_value, std_dev_A),\n    dnorm(x_values, mean_value, std_dev_B),\n    dnorm(x_values, mean_value, std_dev_C)\n  ),\n  curve = factor(c(\n    rep(\"A\", length(x_values)),\n    rep(\"B\", length(x_values)),\n    rep(\"C\", length(x_values))\n  ))\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(\n    title = \"Normal Distributions with Different Spreads\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  scale_color_manual(\n    values = c(\"red\", \"green\", \"blue\"),\n    labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")\n  ) +\n  theme_minimal()\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more “spread” out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or “spread” is the range – the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you’ve taken. [It’s unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y) - min(y)\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the “spread”. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25)\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g. body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n“The degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e. the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.”\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]",
    "crumbs": [
      "**📕 Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#tabular-summaries",
    "href": "module01/01-exploring_data.html#tabular-summaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Tables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5), # Assuming a Poisson distribution for count data\n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e. \\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e. \\(\\Sigma^r_{i=1}f_i/n \\times 100\\)",
    "crumbs": [
      "**📕 Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#graphical-summaries",
    "href": "module01/01-exploring_data.html#graphical-summaries",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Frequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"data/Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count) / sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\nContinuous measurements are free to take any whole or fractional number within their range e.g. plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"data/Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ≈ 95 mm (the centre of the data) and standard deviation ≈ 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e. a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e. 30 or more data values).\n\n\n\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) “plugs” were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g. Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the “box”\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a “*” or \\(\\bullet\\).\n\nStep 3: Determining the “whisker” lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e. to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e. to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.",
    "crumbs": [
      "**📕 Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#shapes-of-distributions",
    "href": "module01/01-exploring_data.html#shapes-of-distributions",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we’ll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the “x” at the right hand side of the graph. The “4” indicates that this is the 4th observation in the data set i.e. the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1 + p2\n\n\n\n\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"data/TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1 + p2\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"data/Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1 + p2\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\nskewness(tccb)\nskewness(arthritis)\n\nKURTOSIS\nKurtosis is a measure of how “peaked” (leptokurtic) a frequency distribution is or how “flattened” (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\nkurtosis(tccb)\nkurtosis(arthritis)\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g. weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.",
    "crumbs": [
      "**📕 Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/01-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "href": "module01/01-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Most natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is “flatness”).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We’ll look at these later.",
    "crumbs": [
      "**📕 Module 1**",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "module01/03-sampling_distributions.html",
    "href": "module01/03-sampling_distributions.html",
    "title": "Sampling distributions",
    "section": "",
    "text": "First some definitions…\nA simple random sample is a sample of size n drawn from a population of size \\(N\\) in such a way that every possible sample of size \\(n\\) has the same probability of being selected. Variability among the simple random samples drawn from the same population is called sampling variability, and the probability distribution that characterizes some aspect of the sampling variability, usually the mean but not always, is called a sampling distribution.\nSample means are extremely important because in experiments we apply treatments to samples of material and use the mean results or yields as measures of the effects of the treatments.\n\\(\\bar y\\) is just one estimate of \\(\\mu\\), say \\(\\bar y_1\\). If another sample of size \\(n\\) were drawn from the population, we would have a slightly different estimate of \\(\\mu\\), say \\(\\bar y_2\\). For example, if we take a sample of 20 from the herd of 100 cows in the Table below we may find that the mean is 16.025. If we calculate the mean for a second sample (say the values in columns 5 & 7 of the Table), we have \\(\\bar y  = 17.055\\). If we take other samples we could get more values of the sample mean, in general all different, and by taking sufficient samples we could obtain the distribution of the values of the sample means for a given size of sample. That is, this process could be repeated, giving many different estimates of \\(\\mu\\), say \\(\\bar y_1, \\bar y_2, \\bar y_3, \\bar y_4,...\\) This forms a distribution of possible sample means, \\(\\bar y\\)’s.\nProperties we would expect the distribution of the sample mean to have:\n\nThe mean value of the distribution of the sample mean would be the same as the mean value of the distribution of the original observations (since there is no reason for expecting it to be either greater or smaller). (Population mean of the \\(\\bar y\\)’s is \\(\\mu\\).)\nThe mean of a number of observations should be a better estimate of the population mean so that we would expect the spread of the distribution of the sample means to be less than that of the distribution of the original observations.\n\nTable Average weekly milk yields (in gallons) of a herd of 100 cows [p. 10 Mead et al. (1993)]\n A result (without mathematical proof)…\nIf a variable y is distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the variable \\(\\bar y\\) which is the mean of a sample of \\(n\\) observations of \\(y\\), is distributed with mean \\(\\mu\\) and variance \\(sigma^2/n\\). The variance of the distribution of the sample mean is, as expected, less than the variance of an individual observation.\nThe square root of the variance of a sample mean is called the standard error of the mean rather than the standard deviation of the mean. This term is used to avoid confusion with the standard deviation of the distribution of individual observations.\nThe standard error of the mean is given by\n\\(\\sigma/\\sqrt n\\) or \\(\\sqrt{\\frac{\\sigma^2}{n}}\\).\nwhich is usually estimated by the standard deviation of the sample mean,\n\\(se(\\bar y)=s/\\sqrt n\\) = or \\(\\sqrt{\\frac{s^2}{n}}\\)\n\n$se(y) is a measure of accuracy for estimating \\(\\mu\\) such that:\n\n\\(se(\\bar y)\\) \\(\\uparrow\\) as \\(s\\) \\(\\uparrow\\)\n\\(se(\\bar y)\\) \\(\\downarrow\\) as \\(n\\) \\(\\uparrow\\).\n\nTo halve the size of a standard error, the sample size (n) needs to be increased \\(4\\times\\); a reduction to 1/3 requires a \\(9\\times\\) increase in \\(n\\), etc.\n\nExample: Root growth of rye grass\nSince the sample mean of \\(\\bar y = 305\\) mg and standard deviation of \\(s = 46\\) mg was based on \\(n = 10\\) pots, the standard error of \\(\\bar y\\) is\n\\(se(\\bar y) = \\frac{46}{\\sqrt{10}} = 14.5\\) mg.\n\ns &lt;- 46\nn &lt;- 10\nse &lt;- s / sqrt(n)\nse\n\nHow many observations are required to have a standard error of no more than 5 mg?\nSolution:\n\\(se(\\bar y) = 5 = \\frac{46}{\\sqrt n}\\) \\(\\sqrt n = 46/5 = 9.2\\) \\(n = 9.2^2 = 85\\)\nSo n = 85 observations would be required\nNotes - We now can also think of \\(\\bar y\\) being an entire distribution of values, but in practice we usually only observe one value of\\(\\bar y\\) - If the original data is normally distributed, \\(y \\sim N(\\mu,\\sigma^2)\\), the distribution of the sample mean is also normal, \\(y \\sim N(\\mu,\\sigma^2/n)\\)\nExample:\n\nRye grass root growth (mg dry weight), \\(y \\sim N(300, 50^2)\\)\nBased on samples of size n = 10, \\(\\bar y \\sim N(300, 50^2 / 10)\\), i.e. \\(\\bar y \\sim N(300, 15.82)\\).\n\n Two other mathematical results concerning the distribution of the sample mean emphasize the importance of the normal distribution:\n\nIf the distribution of the original observation, \\(y\\), is normal, then the distribution of the sample mean, \\(\\bar y\\), is also normal.\nFor almost all forms of distribution of the original observation, \\(y\\), the distribution of the sample mean, \\(\\bar y\\), for a sample of \\(n\\) observations, tends to the normal distribution as \\(n\\) increases.\n\nContinuing from the 2nd result…\nIn fact the tendency for distributions of sample means to become normal is so strong that, for many forms of original distribution, if n is more than 5 the distribution of the sample mean is almost indistinguishable from the normal distribution. The result, which is known as the Central Limit Theorem, is extremely useful. It can be used to explain why so many biological quantities have an approximately normal distribution.\nExample: Distribution of the Sample Mean\nRye grass root growth (mg dry weight), \\(y \\sim N(300, 502)\\) A single root measurement is made. How likely is it that the dry weight exceeds 320 mg? (i.e. \\(P(Y &gt; 320)\\))\nSolution:\n\\(P(Y &gt; 320) = 1 - P(Y \\leq 320)\\)\n\nmu &lt;- 300\nsigma &lt;- 50\n1 - pnorm(320, mu, sigma)\n\nBased on samples of size n = 10, \\(\\bar y ~ N(300, 502 / 10)\\), i.e. \\(\\bar y \\sim N(300, 15.82)\\). How likely is it that a sample mean based on 10 observations exceeds 320 mg? (i.e. P(\\(\\bar y &gt; 320\\)))\n\nmu &lt;- 300\nsigma &lt;- 50\nn &lt;- 10\n1 - pnorm(320, mu, sigma / sqrt(n))\n\nNote that it is less likely for a mean of 10 observations to be this “far” from the population mean, compared with a single value.\n\n\nThe Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p. 23 Mead et al. (1993)]\n\nCreate a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"data/Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\nsd(carrots$Weight_g)\n\nggplot(carrots, aes(x = Weight_g)) +\n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "**📕 Module 1**",
      "Sampling distributions"
    ]
  },
  {
    "objectID": "module01/03-sampling_distributions.html#the-central-limit-theorem",
    "href": "module01/03-sampling_distributions.html#the-central-limit-theorem",
    "title": "Sampling distributions",
    "section": "",
    "text": "The Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p. 23 Mead et al. (1993)]\n\nCreate a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"data/Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\nsd(carrots$Weight_g)\n\nggplot(carrots, aes(x = Weight_g)) +\n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\nsd(sample_means)\n\nggplot() +\n  geom_histogram(aes(x = sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") +\n  theme_minimal()\n\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "**📕 Module 1**",
      "Sampling distributions"
    ]
  },
  {
    "objectID": "module03/040-describing_relationships.html",
    "href": "module03/040-describing_relationships.html",
    "title": "Describing relationships",
    "section": "",
    "text": "Describing relationships\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**📘 Module 3**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "module03/041-linear_functions.html",
    "href": "module03/041-linear_functions.html",
    "title": "Linear functions",
    "section": "",
    "text": "Linear functions\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**📘 Module 3**",
      "Linear functions"
    ]
  },
  {
    "objectID": "labs/Lab01.html",
    "href": "labs/Lab01.html",
    "title": "Lab 01 – Introduction",
    "section": "",
    "text": "Tip\n\n\n\nLearning Outcomes\nAt the end of this practical students should be able to:\n\nuse Microsoft Word for writing equations\nuse Excel and RStudio to calculate simple summary statistics\nUnderstand the link between R and RStudio\nProduce your own rendered Quarto document\n\n\n\n\n\nMake sure you have access to:\n\nMicrosoft Word and Excel\nR and RStudio\nThe data set and code file for this computer lab:\n\n\n\n\n\nAt the beginning of the next few weeks we will be doing some short activities before getting into the stats to help you foster a sense of belonging, learn more about your peers, and help better prepare you for your studies. This week we will start with a simple introduction, but before we do this, we would like to acknowledge those who were here before us:\n\nWe would like to acknowledge and pay respect to the traditional owners of the land on which we meet; the Gadigal people of the Eora Nation. It is upon their ancestral lands that the University of Sydney is built. As we share our own knowledge, teaching, learning and research practices within this university may we also pay respect to the knowledge embedded forever within the Aboriginal Custodianship of Country.\n\nTo learn more about why we do Acknowledgement of Country, and the difference to Welcome to Country, see the following page: Welcome and Acknowledgement.\n\n\n\nWe are all from diverse backgrounds and have followed different paths to get to where we are today. To help you get to know your peers, your demonstrator will lead a class discussion, posting a number of questions on AnswerGarden, where you can then anonymously post your answer to the questions. Links will be provided once your demonstrator has set up the question.\nAfter about 20 minutes of discussion, we can get started on the Stats! Welcome to ENVX1002!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#before-you-begin",
    "href": "labs/Lab01.html#before-you-begin",
    "title": "Lab 01 – Introduction",
    "section": "",
    "text": "Make sure you have access to:\n\nMicrosoft Word and Excel\nR and RStudio\nThe data set and code file for this computer lab:",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#settling-in",
    "href": "labs/Lab01.html#settling-in",
    "title": "Lab 01 – Introduction",
    "section": "",
    "text": "At the beginning of the next few weeks we will be doing some short activities before getting into the stats to help you foster a sense of belonging, learn more about your peers, and help better prepare you for your studies. This week we will start with a simple introduction, but before we do this, we would like to acknowledge those who were here before us:\n\nWe would like to acknowledge and pay respect to the traditional owners of the land on which we meet; the Gadigal people of the Eora Nation. It is upon their ancestral lands that the University of Sydney is built. As we share our own knowledge, teaching, learning and research practices within this university may we also pay respect to the knowledge embedded forever within the Aboriginal Custodianship of Country.\n\nTo learn more about why we do Acknowledgement of Country, and the difference to Welcome to Country, see the following page: Welcome and Acknowledgement.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#answergardens",
    "href": "labs/Lab01.html#answergardens",
    "title": "Lab 01 – Introduction",
    "section": "",
    "text": "We are all from diverse backgrounds and have followed different paths to get to where we are today. To help you get to know your peers, your demonstrator will lead a class discussion, posting a number of questions on AnswerGarden, where you can then anonymously post your answer to the questions. Links will be provided once your demonstrator has set up the question.\nAfter about 20 minutes of discussion, we can get started on the Stats! Welcome to ENVX1002!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#excel-worksheets-and-cells",
    "href": "labs/Lab01.html#excel-worksheets-and-cells",
    "title": "Lab 01 – Introduction",
    "section": "Excel worksheets and cells",
    "text": "Excel worksheets and cells\nExcel files come in series of worksheets where data is stored in cells. The columns are given letters and the rows are given numbers, enabling a particular cell to be referenced by a combination a letter and number. In the screenshot below the number 2 in the orange cell could be referenced by B3. In a blank worksheet type 2 in the B3 cell.\nIn cell C3 type =B3\nThe equals sign tells Excel you are calculating something or referring to a cell. You should now have 2 in cell C3.\n\n\n\nScreenshot of Excel Typing number 2 in cell B3\n\n\nAt the bottom of the Excel page you will see references to each of the worksheets in the file, for example ‘Sheet1’, ‘Sheet2’, ‘Sheet3’. This enables you to store multiple data sets in the one file. In this unit the data sets for each exercise will be stored in separate worksheets but in the same file.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#basic-arithmetic-in-excel",
    "href": "labs/Lab01.html#basic-arithmetic-in-excel",
    "title": "Lab 01 – Introduction",
    "section": "Basic arithmetic in Excel",
    "text": "Basic arithmetic in Excel\nWhen typing equations, make sure you start by typing = . This tells Excel you want to solve the input equation.\nThe basic arithmetic operators return numeric values:\n\n\n\nKey\nOperation\n\n\n\n\n+\nAddition\n\n\n-\nsubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponentiation\n\n\n\nThese can be used in combination with numbers or cell references.\nFor example, to get a value of 4 in cell D3 you can type either\n=2*2 (type numbers)\nor\n=B3*C3 (reference cells)\nIt is better to reference cells so that if you change the values the same equations can be applied.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#basic-functions-in-excel",
    "href": "labs/Lab01.html#basic-functions-in-excel",
    "title": "Lab 01 – Introduction",
    "section": "Basic functions in Excel",
    "text": "Basic functions in Excel\nSome basic functions are:\n\n\n\nFunction\nOperation\n\n\n\n\nSUM\nSums a range of cells\n\n\nCOUNT\nCounts a range of cells\n\n\nLN\nNatural Log\n\n\nEXP\nExponent\n\n\n\nThese can be used in combination with numbers or cell references.\nFor example in cell E3 you can type either\n=EXP(4) or =EXP(D3),\nAnother example is in cell F3 type\n=COUNT(B3:D3)\nNote that Excel has an auto-complete function that allows you to select from a list of functions after typing the first letter i.e. =C. Selecting the function gives a brief description of what the function does.\n\n\n\nScreenshot of Excel Typing =C to show function description\n\n\nOnce the function has been selected, you can proceed to type the opening bracket and enter in the cell reference, cell range or numeric value. Excel aids you in showing what the required input is as you type the opening bracket (see image below). The square bracket indicates an optional value, in this case if only one cell is selected =COUNT(E3), then the function will return the value 1.\n\n\n\nScreenshot of Excel showing required input arguments for COUNT function",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#simple-summary-statistics-in-excel",
    "href": "labs/Lab01.html#simple-summary-statistics-in-excel",
    "title": "Lab 01 – Introduction",
    "section": "Simple summary statistics in Excel",
    "text": "Simple summary statistics in Excel\nThere are functions for calculating summary statistics in Excel. Click on a cell where you want the answer to be entered and then use the menu by Formulas &gt;&gt; Insert Function. A screenshot for calculating the sum is shown below.\n\n\n\nScreenshot of Excel Insert Function\n\n\nOn the next screen you can then select the cells where the observations are located from which then median will be calculated.\nAfter a while you should get to know the name of the functions in Excel and be able to write the arguments in manually. In the screenshot below the function is MEDIAN and it refers to cells between (and including) A2 and A9. A row or column of cells can be represented by the starting cell, then colon, then final cell (A2:A9).\n\n\n\nScreenshot of Excel calculating the Median of cells A2 to A9\n\n\nSome of examples of the functions that can be accessed in Excel are shown below. Note that the .S and .P extensions for variance and standard deviation are from later excel versions.\n\n\n\nStatistic\nFunction\n\n\n\n\nMinimum\n=MIN\n\n\nMaximum\n=MAX\n\n\nArithmetic mean\n=AVERAGE( )\n\n\nMedian\n=MEDIAN( )\n\n\npopulation variance\n=VAR.P()\n\n\nSample variance\n=VAR( ) or =VAR.S()\n\n\nPopulation standard deviation\n=STDEV.P()",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#calculating-simple-summary-statistics-in-excel",
    "href": "labs/Lab01.html#calculating-simple-summary-statistics-in-excel",
    "title": "Lab 01 – Introduction",
    "section": "Calculating simple summary statistics in Excel",
    "text": "Calculating simple summary statistics in Excel\nIn this exercise you will use the Lead_content.csv See start of lesson for link to download the file.\nThis data was collected from a recreational parkland in Sydney and is a measurement of the lead concentration (mg/kg) detected in the soil, measured through chemical analysis (ICP-OES). There are a total of 60 samples collected from around the park. The park was originally a municipal landfill but remediated in 1990, so we expected to find low levels of lead. The guide value set by the Australian Government is 300 mg/kg and this is where further investigation is needed (potential to cause harm).\nIn excel, calculate the following:\n\nminimum value\nMaximum value\nmean\nmedian\nrange\nsample variance\nsample standard deviation\n\nFrom these statistics,\n\nWere there any samples higher than the guide value?\nWere there any samples where no lead was detected?\nWhat is the mean value?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#getting-a-copy-of-r-rsudio",
    "href": "labs/Lab01.html#getting-a-copy-of-r-rsudio",
    "title": "Lab 01 – Introduction",
    "section": "Getting a copy of R & RSudio",
    "text": "Getting a copy of R & RSudio\nRemember that:\nR = Engine and RStudio = Interface\nBoth are free & opensource and downloadable from https://posit.co/download/rstudio-desktop/.\nMake sure you have installed both - it is best to have the latest version of R and RStudio. If you have not done this yet, please do so now.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#r-basics",
    "href": "labs/Lab01.html#r-basics",
    "title": "Lab 01 – Introduction",
    "section": "R basics",
    "text": "R basics\n\nTo begin with we are going to open R (“The Engine”) from the program files menu on your computer.\nWhen you open R, a window containing the R console will open. It will look slightly different depending on the operating system you use. The screenshot is from a Mac.\n\n\n\n\nThe R Console\n\n\n\nAt the bottom of the screen is the command prompt &gt;. Commands are typed at the command prompt and followed by the ENTER key.\nFrom now type all the command you see into R. If you type in an expression, when you hit the ENTER key, the expression will be evaluated and the result returned, for example, lets add 5 and 5 together.\n\n\n5+5\n\n\nR is an object-oriented programming language and the basic unit in R is called an object. Objects can store single numbers, columns of data, modelling output, functions and other kinds of information.\n\nThe class of the object determines the way in which commands are executed on an object and the way in which data can be stored by the object. For example, vectors store a single column of data, a matrix can store multiple columns of data and a data frame can store multiple columns of data where the columns may be of different data types (e.g. numbers and text).\nWe can also save our result above into a named object using the assignment operator &lt;- or =. I like the arrow because it points in the direction of the object being created. For example, the following command saves the value 5+5 as an object called myData.\n\n\nmyData &lt;- 5+5\n\n\nYou can view the contents of an object by typing the object name:\n\n\nmyData\n\n\nWhen you hit the RETURN Key, you will see the following output:\n\n\nObject names can be made up of letters, numbers and , and _ symbols. A name must start with . or a letter. If it starts with . the second character must not be a number.\nR is case sensitive, so calling mydata is not the same as myData and will generate an error:\n\n\nmydata\n\nError: object 'mydata' not found\n\n\n\nTo see a list of all the named objects you’ve created in R, use the objects function:\n\n\nobjects()\n\n\nTo delete an object, use the remove function:\n\n\nremove(myData)\n\n\nIf you type and enter an incomplete command, a continuation prompt will appear on the next line: +. You can continue typing the command followed by the ENTER Key.\n\n\nmyData3 &lt;-\n+ 8\n\n\nTo cancel a command at the continuation prompt (or during execution of a command), press the ESC key.\nThe up and down arrow keys can be used to scroll through previous commands.\nComments can be indicated by a hash mark (#) - everything on the line following the hash mark will be ignored by R. This can be after R code on a line or on a separate line as shown below.\n\n\n#I am adding 6+6 and saving it to an object called my.Data\nmy.Data &lt;- 6+6",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#basic-arithmetic-in-r",
    "href": "labs/Lab01.html#basic-arithmetic-in-r",
    "title": "Lab 01 – Introduction",
    "section": "Basic arithmetic in R",
    "text": "Basic arithmetic in R\n\nThe symbols for basic arithmetic operators are shown in the table below.\n\n\nParentheses ( ) can be used to specify order of operations.\nYou can perform basic calculations by typing expressions into the command line.\n\n\n(5*10) ^2\n\n\nBetter still you can assign results to a named object to be used at a later date.\n\n\nmyresult &lt;- 20/10 + 6 - 1\n\n\nFor example, We can then halve the value of myresult.\n\n\nmyresult &lt;- myresult/2",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#text-editors-in-r",
    "href": "labs/Lab01.html#text-editors-in-r",
    "title": "Lab 01 – Introduction",
    "section": "Text editors in R",
    "text": "Text editors in R\n\nUntil now you have copied and pasted commands which you may wish to use again. This is particularly important as you begin to write series of commands to perform a certain task. One option is to save these commands to text files and copy the relevant commands into R as needed. You can save logical groupings of commands into different text files.\nA better option is to use text editors – one example is RStudio and unlike notepad it allows syntax highlighting of R commands. When an R session is open, RStudio includes an additional menu and toolbar and it allows the user to interact with R by submitting code in whole or in part.\nFrom now on you should start to use RStudio by copying the commands into a R file and then submitting them to R. By doing this you will have a record of the commands you have used. From now you will be using R through RStudio and not the console directly. Over time you will develop a library of code to perform analyses and create graphics.\nThe screenshot below shows RStudio, the top left window shows your code and the bottom left window shows the input and output in R. The top right hand window side shows the objects you have created, for example myData. The bottom left hand corner shows graphics, in this case a histogram, but can also show other useful features such as the help menu.\n\n\n\n\nR Studio",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#simple-data-analysis-in-rstudio",
    "href": "labs/Lab01.html#simple-data-analysis-in-rstudio",
    "title": "Lab 01 – Introduction",
    "section": "Simple data analysis in RStudio",
    "text": "Simple data analysis in RStudio\n\nDownload and open the file Code_Topic1.r see link at start of lesson. Once open using the File &gt; Open menu items in RStudio, you will see most of the commands you have been typing in the code editor. Rather than typing commands directly into into R we will now start to use RStudio code editor.\n\nYou send code from your open R code file which the top left hand pane of to the R console which is found in the bottom left hand pane. The output, e.g. the mean, will also appear in the bottom left hand window.\n\nTo send a line of code to R from your R file (the Code_Topic1.R file), click anywhere in the line and click on the Run icon or use the short cut CTRL+ENTER (Windows) or COMMAND+ENTER (Mac) or click on Run to run all code at once. The output is shown in the bottom left hand window.\n\nYou can also use the # symbol to write comments which R will ignore. It is really important to comment throughout your code to help you and others who may use it to understand what the code does. It is recommended you copy the output into your R file and comment it out using # so you have a complete record of your work.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#getting-data-into-rstudio",
    "href": "labs/Lab01.html#getting-data-into-rstudio",
    "title": "Lab 01 – Introduction",
    "section": "Getting data into RStudio",
    "text": "Getting data into RStudio\n\nRStudio can accept data from many different sources; for example directly from scientific instruments or even scraping the internet for data. In this topic we are only considering small data sets so we will enter the data manually via the keyboard.\nA vector (or list) of numbers can be manually entered using the assignment operator and the c function which essentially means combine, an example is below.\n\n\nmyDataset &lt;- c(5,12,52,32,14,6.1)\n\n\nNow it is your turn. Similar to above, use the c function to enter a soil carbon data set (48, 56, 90, 78, 86, 71, 42) as an object called Carbon. We will then calculate some basic statistics on this data set.\n\n\nCarbon&lt;-c(48, 56, 90, 78, 86, 71, 42)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#summary-statistics-in-rstudio",
    "href": "labs/Lab01.html#summary-statistics-in-rstudio",
    "title": "Lab 01 – Introduction",
    "section": "Summary statistics in RStudio",
    "text": "Summary statistics in RStudio\n\nNow we have entered the data in R we want to do something with it, such as calculate summary statistics.\nR functions behave differently depending on the data type.\nSome functions will work only on specific data types, other functions will use different methods on different data types.\n\nTo find the mean of data set we use the mean function.\n\nmean(Carbon)\n\nOther commands related to summary statistics include:\n\nmedian - median\nvar - sample variance\nsd - sample standard deviation\nmin - minimum value\nmax - maximum value\nlength - number of observations (length of the vector)\n\nCalculate all of the statistics above using R.\n\nmedian(Carbon)\nvar(Carbon)\nsd(Carbon)\nmin(Carbon)\nmax(Carbon)\nlength(Carbon)\n\n\nRather than using all of these indiviudally you can use the summary function which gives the minimum, maximum, mean and median values. We will consider the 1st Qu. and 2nd Qu. in the next practical.\n\n\nsummary(Carbon)\n\n\nNote, that it does not calculate the standard deviation, variance or number of observations.\nThat is all we will do in R in this practical. Remember to save your code so you can refer and reuse this for your online quiz and other assessments in the future.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#projects",
    "href": "labs/Lab01.html#projects",
    "title": "Lab 01 – Introduction",
    "section": "Projects",
    "text": "Projects\nHaving all your files organised into a logical folder structure will be super beneficial in this course and also your day-to-day life. Also Super important is that you back up your work in the cloud. You can do this using One Drive which you can access as through your university account, you can also use your own cloud storage (Dropbox, iCloud,…) and or you can use your GitHub account.\nYour tutors can assist you to get your files organised. The following is my recommendation:\n\nSet up a course folder called ENVX1002 on your desktop/network drive/USB/cloud storage.\nSet up a project folder called Lab_1 in your ENVX1002 folder.\n\n\nCreate a folder called ENVX1002 on your laptop or the class desktop (prefereably in a folder backed up to the cloud. If you are using a class computer you will need to save the file to upload to your cloud storage or USB/External Hard Drive or zip the folder and email to yourself at the end of the class.\nOpen R Studio go to the file drop down menu and select New Project\n\n\n\n\nNew project\n\n\n\nSelect New directory and navigate to your class folder.\n\n\n\n\nNew project\n\n\n\n\nEnter on the directory name, for example Lab_1, and click on Create Project.\nWell done! you have now set up a project.\n\n 6. You should now see a folder called Lab_1 in your ENVX1002 folder. If you open that folder you will see a file called Lab_1.Rproj. This is your project file and it will open RStudio with the working directory set to the Lab_1 folder.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#coding-in-quarto",
    "href": "labs/Lab01.html#coding-in-quarto",
    "title": "Lab 01 – Introduction",
    "section": "Coding in Quarto",
    "text": "Coding in Quarto\nYour tutor will assist you to open a Quarto (.qmd) file. You will do this for each Lab and your also for your reports:\n\nOpen a qmd file, and save it as, for example, Prac1.qmd.\nRun the file using render.\nView filename.html in a browser.\nNow experiment with editing the file (both text and code chunks) and then re-run using render.\nUse this file to store your summary of today’s lab work.\n\nThe screenshots will help you do this.\n\nNavigate to New File &gt; Quarto Document\nEnter in an appropriate name for your file such as ENVX1002_lab_1 and enter in your name. Select HTML format.\n\n\n\n\nNew Quarto\n\n\n\nWell done! you have created a new Quarto file. First save your file by navigating to the File menu i.e. File &gt; Save as. Name your file ENVX1002_lab_1.qmd it should automatically save in your Project folder.\nFinally you can Render the file by selecting the render button - see if you can spot it - it’s a blue block arrow pointing to the right above the text editor.\nGo to your project folder and open the HTML file by double clicking on it, it will open in your default browser.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab01.html#summing-up",
    "href": "labs/Lab01.html#summing-up",
    "title": "Lab 01 – Introduction",
    "section": "Summing up",
    "text": "Summing up\nWell done!\nYou now know how to:\n\nwrite equations in a word document\ndo basic operations in Excel\ndo basic operations in R and RStudio and\nconsider how to manage files and folders and\nset up a project as well as generate an Quarto Document\n\nTo do by next week:\n\ncomplete anything you have missed from today’s lab\ncomplete the practice O-quiz - this is to help you get familiar with Canvas quizzes.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 01 -- Introduction"
    ]
  },
  {
    "objectID": "labs/Lab03.html",
    "href": "labs/Lab03.html",
    "title": "Lab 03 – Probability distributions",
    "section": "",
    "text": "Tip\n\n\n\nLearning Outcomes\nAt the end of this computer practical, students should be able to:\n\nCalculate Binomial probabilities\n\nUsing your calculator;\nR commands;\nR simulations.\n\nCalculate Poisson probabilities\n\nUsing your calculator;\nR commands;\nR simulations.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#notation",
    "href": "labs/Lab03.html#notation",
    "title": "Lab 03 – Probability distributions",
    "section": "Notation",
    "text": "Notation\nFactorials: \\(n! = n(n - 1)(n - 2)...(3)(2)(1)\\) for \\(n \\ge 1\\) and \\(0! = 1\\).\nBinomial coefficients: \\(\\left(\\begin{matrix}n\\\\x\\end{matrix}\\right)=\\frac{n!}{x!(n-x)!}\\) for \\(x=1,2,3,...,n\\)\nThe Binomial distribution models a context in which we have a fixed number n of independent Binary trials and a fixed likelihood of a success at each trial \\(p = P(success)\\).\nX = the number of successes in n trials \\(\\sim Bin(n,p)\\)\nProbability distribution function: \\(P(X = x) = \\left(\\begin{matrix}n\\\\x\\end{matrix}\\right)p^x(1-p)^{n-x}\\) for \\(x=1,2,3,...,n\\)\nCumulative distribution function (CDF): \\(F(x) = P(X \\le x)\\).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-1---walk-through",
    "href": "labs/Lab03.html#exercise-1---walk-through",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 1 - Walk through",
    "text": "Exercise 1 - Walk through\nWith your neighbour, discuss how coin tossing is related to the Binomial distribution?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-2---walk-through",
    "href": "labs/Lab03.html#exercise-2---walk-through",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 2 - Walk through",
    "text": "Exercise 2 - Walk through\nPracticing using the Binomial distribution formula\n\nCalculate 4!, 3!, 2! and 1!.\n\n\\[4!=4 \\times 3 \\times 2 \\times 1\\]\nNow you calculate the rest\n\nShow that\n\n\\(\\left(\\begin{matrix}4\\\\0\\end{matrix}\\right)=1\\), \\(\\left(\\begin{matrix}4\\\\1\\end{matrix}\\right)=4\\), \\(\\left(\\begin{matrix}4\\\\2\\end{matrix}\\right)=6\\), \\(\\left(\\begin{matrix}4\\\\3\\end{matrix}\\right)=4\\), \\(\\left(\\begin{matrix}4\\\\4\\end{matrix}\\right)=1\\)\n\\(\\left(\\begin{matrix}4\\\\0\\end{matrix}\\right)=\\frac{4!}{0!(4-0)!}=\\frac{4!}{4!}=1\\)\n\\(\\left(\\begin{matrix}4\\\\1\\end{matrix}\\right)=\\frac{4!}{1!(4-1)!}=\\frac{4!}{1!\\times3!}=\\frac{4\\times3\\times2\\times1}{1\\times3\\times2\\times1}=\\frac{4}{1}=4\\)\n\nNow you calculate the rest!\nNote you can also calculate this using the nCr option on your calculator - watch this YouTube video which gives a nice demonstration with some elevator music in the background to relax you.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-3---walk-through",
    "href": "labs/Lab03.html#exercise-3---walk-through",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 3 - Walk through",
    "text": "Exercise 3 - Walk through\nCollaboration - Simulate the Binomial Distribution by Coin Tossing\n\nChoose a partner and together toss a coin 4 times counting the number of heads (use Flip a coin if you don’t have a coin). Record the number of heads in an excel spreadsheet called (Simulation 1) and set out a table in excel similar to below. Now repeat this process 19 more times.\n\n\n\n\n\nSimulation\nToss 1\nToss 2\nToss 3\nToss 4\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n\n\n3\n\n\n\n\n\n\n…\n\n\n\n\n\n\n…\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\nTally up the frequency of each number of heads and fill in a table similar to the following in excel.\n\n\n\n\nNumber heads \\(x\\)\n0\n1\n2\n3\n4\nTotal\n\n\n\n\nFrequency\n\n\n\n\n\n20\n\n\n\n\nNow fill in the probability distribution in excel = the proportion of number of heads in all simulations i.e. number of simulations ith 0 heads/20, number of simulations with 1 head/20, number of simulations with 2 heads/20,….\n\n\n\n\nNumber heads \\(x\\)\n0\n1\n2\n3\n4\nTotal\n\n\n\n\nProbability \\(P(X=x)\\)\n\n\n\n\n\n1\n\n\n\n\nnow see if you can make a barplot in Excel of the above table Excel instructinos\n\n\n\n\nBarchart 1\n\n\n\n\n\nBarchart 2\n\n\nQuestion: With your stats partner, discuss the shape of the distribution, is it what you would expect to see and compare this to what the barplot you made in excel, are they similar?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-4---walk-through",
    "href": "labs/Lab03.html#exercise-4---walk-through",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 4 - Walk through",
    "text": "Exercise 4 - Walk through\nNow we will use R to simulate 4 coin tosses representing 1 as heads and 0 as tails. Note that I have withheld the output to avoid “spoiling” the suprise.\n\ncoin &lt;- c(0, 1) # vector representing coin tosses c(tails, heads)\nset.seed(1) # makes sure we all get the same answer i.e. we use the same randomly generated numbers\ntosses &lt;- sample(coin, size = 4 * 20, replace = T, prob = c(0.5, 0.5)) # this randomly picks a 0 or 1 from coin and there is an equal probability of sampling both. It creates a vector of results i.e. bo tosses in total\ndim(tosses) &lt;- c(20, 4) # This creates a data frame with 4 columns and 20 rows that represents the 20 trials of 4 coin tosses\ntosses ## this prints the result\nNumHeads &lt;- rowSums(tosses) ## this sums the number of heads in each trial\nNumHeads # this prints the results\n\n\nThe table function tallies each of the numbers of heads for each simulation. In each trial (4 tosses) How many times did you toss no heads, how many times did you toss 1 head, how many times did you toss 2 heads, how many times did you toss 3 heads and how many times did you toss 4 heads?\n\n\nFreq_table &lt;- table(NumHeads)\nFreq_table\n\nThe prob.table function divides each frequency by 20 i.e.\\(P(X=0)=2/20\\)\n\nprop.table(Freq_table)\n\n\nNow we can plot this. Notice how we customise the x-axis to ensure that 0 is displayed as even though there may be no instances where we got zero heads, it is still a possibility even if the probability is low. for this we use the argument , xaxt = “n” and then draw customised axis labels using the axis function.\n\n\nplot(prop.table(Freq_table), ylab = \"P(X=X)\", xlab = \"x\", xlim = c(0, 4), xaxt = \"n\")\naxis(1, at = 0:4)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-5",
    "href": "labs/Lab03.html#exercise-5",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 5",
    "text": "Exercise 5\nRepeat exercise 4, but this time change the ‘prob = c(0.5, 0.5)’ to ‘prob = c(0.3, 0.7)’.\nQuestion: What happens to the shape of the distribution? What type of coin does this change in probability represent?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-6",
    "href": "labs/Lab03.html#exercise-6",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 6",
    "text": "Exercise 6\nRepeat exercise 4, but this time change the set.seed(1) to set.seed(123).\nQuestion: What happens to the frequency table ‘Freq_table’ why do you think it might change?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-7",
    "href": "labs/Lab03.html#exercise-7",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 7",
    "text": "Exercise 7\nSix calves were born after artificial insemination (AI) with regular semen. Assuming that the probability for either being male or female is 0.5,\nQuestion: What is the probability that all 6 are male?\n\n\\(X=\\) number of male calves \\(\\sim Bin(n=6,p=0.5)\\) \\(P(X=6)=\\left(\\begin{matrix}6\\\\6\\end{matrix}\\right)0.5^6(1-0.5)^{6-6} = 0.015625\\)\nor in R we can calculate using the dbinom function which calculates the exact probability of having 6 male calfs born\n\ndbinom(6, 6, 0.5)\n\nQuestion: What is the probability that more than 4 calves are female?\nNOTE P(more than 4 calves are female) = P(less than 2 calcves are male) = P(X=0)+P(X=1)\n\\(=\\left(\\begin{matrix}6\\\\0\\end{matrix}\\right)0.5^0(1-0.5)^{6-0}+\\left(\\begin{matrix}6\\\\1\\end{matrix}\\right)0.5^1(1-0.5)^{6-1} = 0.109375\\)\nor in R\n\n1 - pbinom(4, 6, 0.5)\n\nQuestion: What are your assumptions? What is a more accurate estimate of the P(male calf)? Beef Article\nAssumptions: Each of the births is independent and P(male calf = 0.5). (In reality, it is 1.06 males per every female born in large populations of cattle, which gives p = 0.5145631. See article link.)\nUsing the updated probability p = 0.5145631 and using R recalculate\n\nQuestion: What is the probability that all 6 are male?\n\nQuestion: What is the probability that more than 4 calves are female?\n\nQuestion: Finally, calculate what is the probability that exactly 4 females are born?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-1---walk-through-1",
    "href": "labs/Lab03.html#exercise-1---walk-through-1",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 1 - Walk through",
    "text": "Exercise 1 - Walk through\nLets take a look at an example. Let us assume that on average 5 shoppers enter a store each hour. Say the shop is open for 10 hours per day, what would a typical day look like. Assuming that the number of visits follows a Poisson distribution i.e. If \\(X \\sim Po(\\lambda)\\), and because we know \\(\\lambda=5\\) and \\(n=10\\), we can simulate this in R using the following:\n\nshoppers &lt;- rpois(10, 5)\nshoppers\n\nQuestion: Take a look at the minimum and maximum number of shoppers generated by the model and discuss with your neighbour how you, as a shop owner, might use this information?\n\nQuestion: Now run the simulation for a full 5 day week, is there any change?\n\nQuestion: Now suppose as a shop owner you are concerned that your shop assistant might get overwhelmed if more than 10 shoppers come in, in any one hour period, what is the probability of this?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#exercise-2",
    "href": "labs/Lab03.html#exercise-2",
    "title": "Lab 03 – Probability distributions",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe also often say that the Poisson Distribution is good for modeling rare events. For example, recent work in Drosophila suggests the spontaneous rate of deleterious mutations is 1.2 per diploid genome.\nOpen the following article and read the abstract and then search the word poisson and read the final two sentences of this paragraph.\nNature article\nAssume that X = the number of deleterious mutations \\(X \\sim Po(1.2)\\).\nQuestion: What is the probability that an individual has 0 mutations?\n\\(P(X=0)=\\frac{1.2^0e^{-1.2}}{0!}=0.3011942\\)\nOr in R\n\ndpois(0, 1.2)\n\nQuestion: What is the probability that an individual has less than or equal to 2 mutations?\n\\(P(X\\le2)=P(X=0)+P(X=1)+P(X=2)\\) \\(=\\frac{1.2^0e^{-1.2}}{0!}+\\frac{1.2^1e^{-1.2}}{1!}+\\frac{1.2^2e^{-1.2}}{2!}\\) \\(=0.8794871\\)\nOr in R\n\nppois(2, 1.2)\n\nMore recent research found that the average spontaneous rate of deleterious mutations was actually 1.4 per diploid genome. Using R recalculate the following.\n\nQuestion: What is the new probability that an individual has 0 mutations. How does this compare to the former probability, what does this suggest?\nQuestion: What is the new probability that an individual has less than or equal to 2 mutations How does this compare to the former probability, what does this suggest?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab03.html#advanced-exercise",
    "href": "labs/Lab03.html#advanced-exercise",
    "title": "Lab 03 – Probability distributions",
    "section": "Advanced exercise",
    "text": "Advanced exercise\nFrom the tutorial we looked at Tomato germination. Use simulations to find approximate Binomial probabilities obtain an estimate of the probability of getting 7 or more germinating seeds, by doing simulations from a \\(Bin(8, 0.7)\\) in R.\nRemember that the exact probability is\n\n1 - pbinom(6, 8, 0.7)\n\n\nFirst try 10 simulations and fill out a similar table to below in excel. What is your estimate of the probability of 7 germinating seeds?\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 10, 8, 0.7) # randomly generates 10 values from Bin(8,0.7)\nseeds ## prints seeds\ntable(seeds) / 10 # turns counts into probabilities\nbarplot(table(seeds) / 10, xlab = \"Number Germinated x\", ylab = \"Probability\", col = \"green\")\n\n\nNext try 100 simulations.\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 100, 8, 0.7) # randomly generates 100 values from Bin(8,0.7)\nseeds ## prints seeds\ntable(seeds) / 100 # turns counts into probabilities\nbarplot(table(seeds) / 100, xlab = \"Number germinated (x)\", ylab = \"Probability\", col = \"green\")\n\n\nNow we will try 10000 simulations. What do you notice about the histogram?\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 10000, 8, 0.7) # randomly generates 100 values from Bin(8,0.7)\ntable(seeds) / 10000 # turns counts into probabilities\nbarplot(table(seeds) / 10000, xlab = \"Number germinated (x)\", ylab = \"Probability\", col = \"green\")\n\n\nNow we can compare the exact results and the simulated results by adding up the probabilities for 7 and 8 seeds germinating in each of the scenarios.\n\nNote that the numbers will may differ slightly each time as they a randomly generated unless you set the same seed.\n\n\n\nNumber germinated\n\\(P(X\\ge{7}\\))\n\n\n\n\nExact method using R\n\n\n\nSimulation (n=10)\n\n\n\nSimulation (n=100)\n\n\n\nSimulation (n=10000)\n\n\n\n\nQuestion: Which of the simulations gives the closest result to the exact method, why do you think this is? - try ask chatGPT or use the # q: What is ? prompt if you have enabled copilot.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 03 -- Probability distributions"
    ]
  },
  {
    "objectID": "labs/Lab05.html",
    "href": "labs/Lab05.html",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 1-sample t-test\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#welcome",
    "href": "labs/Lab05.html#welcome",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 1-sample t-test\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#create-a-new-project",
    "href": "labs/Lab05.html#create-a-new-project",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "Create a new project",
    "text": "Create a new project\nReminder (skip to step 2 if you are going to use the directory you created in your tutorial)\nStep 1: Create a new project file for the practical put in your ENVX1002 Folder. File &gt; New Project &gt; New Directory &gt; New Project.\nStep 2: Download the data files from canvas or using above link and copy into your project directory.\nI recommend that you make a data folder in your project directory to keep things tidy! If you make a data folder in your project directory you will need to indicate this path before the file name.\nStep 3: Open a new Quarto file.\ni.e. File &gt; New File &gt; Quarto Document and save it immediately i.e. File &gt; Save.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#problems-with-your-personal-computer-and-r",
    "href": "labs/Lab05.html#problems-with-your-personal-computer-and-r",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "Problems with your personal computer and R",
    "text": "Problems with your personal computer and R\nNOTE: If you are having problems with R on your personal computer that cannot easily be solved by a demonstrator, please use the Lab PCs.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#installing-packages",
    "href": "labs/Lab05.html#installing-packages",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "Installing packages",
    "text": "Installing packages\nRemember All of the functions and data sets in R are organised into packages. There are the standard (or base) packages which are part of the source code - the functions and data sets that make up these packages are automatically available when R is opened. There are also many contributed packages. These have been written by many different authors, often to implement methods that are not available in the base packages. If you are unable to find a method in the base packages, you might be able to find it in a contributed package. The Comprehensive R Archive Network (CRAN) site (http://cran.r-project.org/) is where many contributed packages can be downloaded. Click on packages on the left hand side. We will download two packages in this class using the install.packages command and we then load the package into R using the library command.\nAlternatively, in RStudio click on the Packages tab &gt; Install &gt; type in package name &gt; click install.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#normally-you-choose-0.05-as-a-level-of-significance",
    "href": "labs/Lab05.html#normally-you-choose-0.05-as-a-level-of-significance",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "1. Normally you choose 0.05 as a level of significance:",
    "text": "1. Normally you choose 0.05 as a level of significance:\nThis value is generally accepted in the scientific community and is also linked to type 2 errors where choosing a lower significance increases the likelihood of a type 2 error occurring.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#write-null-and-alternative-hypotheses",
    "href": "labs/Lab05.html#write-null-and-alternative-hypotheses",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "2. Write null and alternative hypotheses:",
    "text": "2. Write null and alternative hypotheses:\n\n\nQuestion: Write down the null hypothesis and alternative hypotheses:\nH0: &lt; Type your answer here &gt;\nH1: &lt; Type your answer here &gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#check-assumptions-normality",
    "href": "labs/Lab05.html#check-assumptions-normality",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "3. Check assumptions (normality):",
    "text": "3. Check assumptions (normality):\n\na. load data:\nMake sure you set your working directory first\n\n# Type your R code here\n\nIt is always good practice to look at the data first to make sure you have the correct data, it loaded in correctly and know what the names of the columns are. This can be done by typing the name of the data Milk or for large datasets, use str() to show the first 6 lines:\n\n# Type your R code here\n\n\n\nb. Tests for normality:\nqqplots:\n\n# Type your R code here\n\nHistogram and boxplots:\n\n# Type your R code here\n\n\n\nQuestion: Do the plots indicate the data are normally distributed?\nAnswer: &lt; Type your answer here &gt;\n\n\nShapiro-Wilk test of normality:\n\n# Type your R code here\n\n\n\nQuestion: Does the Shapiro-Wilk test indicate the data are normally distributed? Explain your answer.\nAnswer: &lt; Type your answer here &gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#calculate-the-test-statistic",
    "href": "labs/Lab05.html#calculate-the-test-statistic",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "4. Calculate the test statistic",
    "text": "4. Calculate the test statistic\nIn R we achieve this via the command t.test(milk$Yield, mu = …) The R output first gives us the calculated t value, the degrees of freedom, and then the p-value, it then provides the 95% CI and the mean of the sample. Were mu = … is written enter in the hypothesised mean.\n\n# write your R code here\n\n\n5. Obtain P-value or critical value\n\n\nQuestion: Does the hypothesised economic threshold lie within the confidence intervals?\nAnswer: &lt; Type your answer here &gt;\n\n\n\n\n6. Make statistical conclusion\n\n\nQuestion:: Based on the P-value, do we accept or reject the null hypothesis?\nAnswer: &lt; Type your answer here &gt;\n\n\n\n\n7. Write a scientific (biological) conclusion\n\n\nQuestion:: Now write a scientific (biological) conclusion based on the outcome in 6.\nAnswer: &lt; Type your answer here &gt;",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#thanks",
    "href": "labs/Lab05.html#thanks",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "Thanks!",
    "text": "Thanks!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#exercise-1-carrots",
    "href": "labs/Lab05.html#exercise-1-carrots",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "Exercise 1: Carrots",
    "text": "Exercise 1: Carrots\nA farmer is growing carrots for a restaurant. The restaraunt wants their carrots to be 10 cm long, so the farmer wants to check if the carrots in their field differ significantly from the needed length.\n\n#Read in data\n\ncarrots &lt;- c(7, 7, 13, 5, 13, 10, 11, 12, 10,  9)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab05.html#exercise-2-penguins",
    "href": "labs/Lab05.html#exercise-2-penguins",
    "title": "Lab 05 – Hypothesis Testing",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nRey has just landed on earth and notived that penguins look really similar to porgs. Using weight as the point of comparison, she wants to know if two different penguin species weigh the same as her pet Porg Stevie, who weighs 4000g.\nWe will be using the Palmer penguin dataset to test if chinstrap and gentoo penguins weigh the same as Stevie.\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n\n2.1 Chinstrap\n\nchinstrap &lt;-  penguins%&gt;%\n  filter(species == \"Chinstrap\")%&gt;%\n  na.omit()\n\n\n\n2.2 Gentoo\n\ngentoo &lt;-penguins%&gt;%\n  filter(species == \"Gentoo\")%&gt;%\n  na.omit() \n\n\n\nAttribution\nThis lab was developed using resources that are available under a Creative Commons Attribution 4.0 International license, made available on the SOLES Open Educational Resources repository.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 05 -- Hypothesis Testing"
    ]
  },
  {
    "objectID": "labs/Lab04.html",
    "href": "labs/Lab04.html",
    "title": "Lab 04 – Sampling distributions",
    "section": "",
    "text": "Project 1 is due in week 5 and for many of you this may be your first university assignment; some may be nervous, while others may be more relaxed. Your demonstrators will use the start of this practical to discuss how you may be feeling for this first assessment and share ways you might want to approach and prepare for the assessment.\nThis is the last reflective activity for now, thank you all for contributing so far and we hope you have found some benefit in the activities. Now for some probability!\n\n\n\n\n\n\nLearning outcomes\n\n\n\nAt the end of this computer practical, students should be able to:\n\ncalculate tail, interval and inverse probabilities associated with the Normal distribution\ncalculate probabilities associated sampling distribution of the sample mean by using simulation in R and using R commands.\n\n\n\nLink to data is below:\n\nENVX1002_Data4.xlsx\nAlternatively download from Canvas",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "href": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "title": "Lab 04 – Sampling distributions",
    "section": "Exercise 1 - Class activity - How tall is ENVX1002??",
    "text": "Exercise 1 - Class activity - How tall is ENVX1002??\n\nSet up a new PROJECT for Lab 4 and create a quarto document called Lab_4.qmd and save it in your project directory.\nRecord the height of all male and female students in the class up on the board (we will try to provide a tape if you are unsure about your height)\nEnter the data into r, for example:\n\n\nfemale_height &lt;- c(176, 180, 187, 168)\nmale_height &lt;- c(175, 183, 163, 190)\n\n\nCalculate the mean and standard deviation using R and graph the distribution for both genders, for example\n\n\nf_mean &lt;- mean(female_height)\nf_sd &lt;- sd(female_height)\nhist(female_height)\n\n\nDiscuss with your neighbour or post on the zoom chat, which model (distribution function) do you think would fit the data\nHow does your class compare to the Australian statistics. For this we will look at the mean and standard deviation of measured heights for men and women aged 18 - 24 from the ABS for 1995 see page 13 of\n\nhttps://www.ausstats.abs.gov.au/Ausstats/subscriber.nsf/Lookup/CA25687100069892CA256889001F4A36/$File/43590_1995.pdf\nNote that both reported and measured heights are provided and not surprisingly reported heights are bigger that the measured :o)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-2---milkfat-example",
    "href": "labs/Lab04.html#exercise-2---milkfat-example",
    "title": "Lab 04 – Sampling distributions",
    "section": "Exercise 2 - Milkfat example",
    "text": "Exercise 2 - Milkfat example\n\nPart 1\nThe milkfat content in milk (in %) for 120 cows are presented in the worksheet called ENVX1002_Data4.xlsx. Copy the file into your project directory and:\n\nImport the data into R.\n\n\nlibrary(readxl)\nmilkfat &lt;- read_excel(\"data/ENVX1002_Data4.xlsx\", sheet = \"Milkfat\")\n\n\nCalculate the summary statistics of Milkfat (mean, median and sd)\n\nNote that we use $ColumnName to select a column from the data\n\nmean(milkfat$Milkfat)\n\n\nWhat type of cows could they be? Compare your data to the table in the following link:\n\nhttps://lactalis.com.au/info-center/different-breeds-of-cows/\n\nWhat state could they be from? Check some of the recent Milk Production reports from Dairy Australia. The data can be found in the Average Milkfat & Protein (%) section of the PDF report: The reports can be found at the following link:\n\nhttps://www.dairyaustralia.com.au/resource-repository/2020/09/25/milk-production-report\n\nCould the data be normally distributed?\n\n\nCreate a histogram and boxplot of the milk fat data. Is the data Normally distributed?\n\n\nrequire(ggplot2)\nggplot(milkfat, aes(x = Milkfat)) +\n  geom_histogram(binwidth = 0.1, fill = \"lightblue\", color = \"black\") +\n  xlab(\"Milkfat (%)\")\n\n\nIn the UK, breakfast milk' (orChannel Island milk’) has 5.5% fat content. What percentage of the cows in this data set is yielding breakfast milk with \\(\\ge\\) 5.5%?\n\n\ns &lt;- sort(milkfat$Milkfat) # Sorts the data\ns # Look at the sorted data\nlength(s[s &gt;= 5.5]) # Counts how many are &gt;= 5.5\n\n\nIn Australia, full cream milk has greater than 3.2% milk fat content. What percentage of these cows is yielding full cream milk?\n\n\n## Your turn\n\n\n\nPart 2\nLet \\(X\\) represent the milk fat content for the population of this breed of cows.\n\nAssuming the population is normal, use the sample mean and standard deviation from the previous question as estimates of the population parameters. So \\(X\\sim (\\mu =..., \\sigma^2 = ...)\\).\nDraw a picture of the curve representing \\(X\\). The below example uses ggplot2 to draw the curve for \\(N(4.16,0.30^2)\\).\n\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(4.16 - 4 * 0.3, 4.16 + 4 * 0.3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.16, sd = 0.30)) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16, 0.30^2) ~ pdf))\n\n\nWhat is the probability that 1 cow has a fat content less than 4%? We will adapt the ggplot command above a picture of this probability and then use R to find the probability.\n\nHint: You may need to use the stat_function command to draw the curve and then use the pnorm command to find the probability.\n\nggplot(data.frame(x = c(4.16 - 4 * 0.3, 4.16 + 4 * 0.3)), aes(x = x)) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = 0.30),\n    geom = \"area\", fill = \"white\"\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 4.16, sd = 0.30),\n    xlim = c(4.16 - 4 * 0.3, 4), geom = \"area\", fill = \"red\"\n  ) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16, 0.30^2) ~ pdf))\n\n\npnorm(4, 4.16, 0.30)\n\n\nWhat is the probability that 1 cow (randomly sampled) has a fat content greater than 4.5%? Try and adapt the ggplots above to draw a picture of this probability and then use R to find the probability.\nFor a sample of 10 cows (randomly sampled), what is the probability that the sample mean milk fat content is greater than 4.2%?\n\nHint: First find the distribution of the sample mean \\(\\overline{X}\\). Then find \\(P(\\overline{X}&gt;4.2)\\)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-3---skin-cancer",
    "href": "labs/Lab04.html#exercise-3---skin-cancer",
    "title": "Lab 04 – Sampling distributions",
    "section": "Exercise 3 - Skin cancer",
    "text": "Exercise 3 - Skin cancer\nA dermatologist investigating a certain type of skin cancer induced the cancer in nine rats and then treated them with a new experimental drug. For each rat she recorded the number of hours until remission of the cancer. The rats had a mean remission time of 400 hours and a standard deviation of 30 hours. From this data, calculate the standard error of the mean.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-4---soil-carbon",
    "href": "labs/Lab04.html#exercise-4---soil-carbon",
    "title": "Lab 04 – Sampling distributions",
    "section": "Exercise 4 - Soil carbon",
    "text": "Exercise 4 - Soil carbon\nAn initial soil carbon survey of a farm based on 12 observations found that the sample mean \\(\\overline{X}\\) was 1.2% and the standard deviation s was 0.4%. How many observations would be needed to estimate the mean carbon value with a standard error of 0.1%?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "href": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "title": "Lab 04 – Sampling distributions",
    "section": "Exercise 5 - What’s in the media - looming state election",
    "text": "Exercise 5 - What’s in the media - looming state election\nAn article was published in the Sydney Morning Herald on Saturday 20.3.2010 about statistics related to opinion polls. Read it and find the sentences related to (i) populations versus samples (ii) standard error formula (iv) the effect of sample size on standard errors.\nhttp://www.smh.com.au/national/demystifying-the-dark-art-of-polling-20100319-qmai.html",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab04.html#exercise-6---extra-practice",
    "href": "labs/Lab04.html#exercise-6---extra-practice",
    "title": "Lab 04 – Sampling distributions",
    "section": "Exercise 6 - Extra practice",
    "text": "Exercise 6 - Extra practice\nThe average Australian woman has height (in cms) of 161.8 with a standard deviation of 6.\n\nThe Australian Institute of Sport ran a netball training camp for the best Australian young players. How tall were the goal position players? http://www.abc.net.au/news/2015-06-14/tall-athletes-get-support-at-ais-to-stand-as-proud-netballers/6544642\nWhat is the probability of finding an Australian woman of this height or taller?\n\nHints:\nStep 1: Using ggplot, draw a sketch of the Normal curve with the probability identified. You may need to draw a section of the right tail as the probability is small! We have provided the solution for the plotting to assist you.\nStep 2: Calculate the probability in R.\n\n1 - pnorm(189, 161.8, 6)\n\nggplot() +\n  stat_function(\n    fun = dnorm, args = list(mean = 161.8, sd = 6),\n    geom = \"area\", fill = \"white\", xlim = c(180, 161.8 + 4 * 6)\n  ) +\n  stat_function(\n    fun = dnorm, args = list(mean = 161.8, sd = 6),\n    geom = \"area\", fill = \"red\", xlim = c(161.8 + 4 * 6, 189)\n  ) +\n  xlab(\"x\") +\n  ylab(expression(N(161.8, 6^2) ~ pdf)) +\n  scale_x_continuous(breaks = 189)\n\n\nDharshani Sivalingam is the tallest netball player in the world. How tall is Dharshani? https://en.wikipedia.org/wiki/Tharjini_Sivalingam What is the probability of finding an Australian woman of Dharshani’s height?\nMadison Brown is one of the the shortest Australian International players. How tall is Madision? https://en.wikipedia.org/wiki/Madison_Browne What percentage of Australian women are between Madison and Dharshani’s heights?\nIf 80% of Australian women are above a certain height, what is that height?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 04 -- Sampling distributions"
    ]
  },
  {
    "objectID": "labs/Lab12.html",
    "href": "labs/Lab12.html",
    "title": "Lab 12 - Non-linear models",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nCalculate “by hand” the initial estimates of the parameters of a non-linear model\nInterpret tables of regression coefficients for polynomials to perform hypothesis testing\nFit polynomials and non-linear models to data using least-squares fitting using the SOLVER add-in in Excel\nFit polynomials and non-linear models to data in R, and interpret the outputs",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#before-we-begin",
    "href": "labs/Lab12.html#before-we-begin",
    "title": "Lab 12 - Non-linear models",
    "section": "Before we begin",
    "text": "Before we begin\nCreate your Quarto document and save it as Lab-12.qmd or similar.\nThe following data files are required:\n\neast_creek.csv\n\nOver the past few weeks you have explored linear models and how to interpret model summary output. Again we have stepped up the complexity, now venturing into the world of non-linear models.\nThis practical focuses on fitting non-linear models to data with an emphasis on 3 important classes of functions that all budding biologists and environmental scientists should know\n\npolynomials,\nexponential models, and\nlogistic models.\n\nA question before we begin:\nWhat are some advantages and disadvantages of non-linear models as compared to polynomials?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#polynomials",
    "href": "labs/Lab12.html#polynomials",
    "title": "Lab 12 - Non-linear models",
    "section": "Polynomials",
    "text": "Polynomials\n\nQuadratic\n\\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nwhere the parameters are the y-intercept (b0), the linear component (b1) and the quadratic component (b2).\nIf b2 is negative then the shape of the function is convex upwards, i.e. y increases with x until reaches a peak and then y decreases.\nIt is easy to understand so it has been commonly used for modelling the response of yield to inputs such as fertiliser, seeding rates. This is despite much criticism for being unrealistic.\nLimitations:\n\nrate of increase to peak is same as rate of decrease past peak\ndoes not level off as x becomes small or very large, y just keeps increasing or decreasing.\n\nCubic\n\\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)\nCompared to the quadratic model which has 1 turning point, a cubic model has 2 turning points.\n\n\n\nExercise 1: Interpreting polynomials\nA study was performed to examine the soil properties that control the within-field variation in crop yield. The focus of this question is on soil pH which among other things controls the availability of nutrients to plants.\nThis exercise does not require you to read in any data, but rather focus on interpreting the model output and comparing the models.\nThe figure below shows the raw observations of yield plotted against pH with three models fitted; a linear model, quadratic polynomial and a cubic polynomial.\n\nwhich line corresponds to which model?\n\n\n\nbased on the output from the 3 models below, which model fits the data best? Note: no hypothesis testing yet, just how well the model fits the data (r2).\n\n\n\nLinear model:\n\n\nQuadratic model:\n\n\nCubic model:\n\n\n\nUse the R output to perform hypothesis testing to find the best model. Write out the hypotheses you are testing.\n\n\n\n\nExercise 2: Fitting polynomials in R\nThis exercise will use real data from a yield-fertiliser trial in Bedfordshire, United Kingdom.\nFirst thing we can do is fit a linear model to the fertiliser data:\n\n# create fertiliser and yield objects\nfert&lt;-c(0,100,170,225)\nyield&lt;-c(3.32,5.23,5.41,5.02)\n\n# Fits a linear model and saves it to an object called lin.mod \nlin.mod&lt;-lm(yield~fert)\n\n# Summarises key features of model\nsummary(lin.mod)\n\n\nCall:\nlm(formula = yield ~ fert)\n\nResiduals:\n      1       2       3       4 \n-0.4469  0.6727  0.2995 -0.5252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 3.766929   0.634765   5.934   0.0272 *\nfert        0.007904   0.004243   1.863   0.2035  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7134 on 2 degrees of freedom\nMultiple R-squared:  0.6344,    Adjusted R-squared:  0.4515 \nF-statistic:  3.47 on 1 and 2 DF,  p-value: 0.2035\n\n\n\nWhat is the model fit like in this model?\n\nFit and plot a quadratic polynomial in R. In R a quadratic polynomial can be fitted using the following lines of code:\n\n# create a new variable which is the square of the fertilizer rates\nfert2&lt;-fert^2\n\n# fit the quadratic model incorporating fert2\nquad.mod&lt;-lm(yield~fert+fert2)\n\nsummary(quad.mod)\n\n\nCall:\nlm(formula = yield ~ fert + fert2)\n\nResiduals:\n        1         2         3         4 \n-0.005611  0.024528 -0.032791  0.013874 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  3.326e+00  4.324e-02   76.92  0.00828 **\nfert         2.786e-02  9.014e-04   30.91  0.02059 * \nfert2       -9.064e-05  3.921e-06  -23.12  0.02752 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0436 on 1 degrees of freedom\nMultiple R-squared:  0.9993,    Adjusted R-squared:  0.998 \nF-statistic: 731.7 on 2 and 1 DF,  p-value: 0.02613\n\n\n\nWhat is the fit like for our quadratic model? is it better than our linear model?\n\nIn Excel it is easy to fit a line, by creating a scatterplot, then add Trendline… and selecting Polynomial (2nd order).\nTo fit our polynomial line in R, we need to obtain model predictions first.\nTo plot model predictions you first need to predict at fine intervals of the predictor to make a continuous plot that is not jagged or stepped. To create a new prediction dataset you can use the following code:\n\n# creates a sequence of numbers from 0 to 225 going up in increments of 1\nnew.fert&lt;-seq(0,225,1) \n\nWe can use our model to predict at the values in the new prediction dataset, in this case new.fert.\n\nnew.pred&lt;-predict(quad.mod,list(fert=new.fert,fert2=new.fert^2))\n\nThe general form of the predict function is predict(model object, list object).\nThe list object tells R what object contains the data we will use to predict. For example in our case the model was built on fert and fert2 so we have to tell the predict function what object contains the new values for each of these, in our case new.fert.\nNow we plot the raw observation as points and add an overlay of the model fit as lines:\n\nplot(fert,yield,xlab='Fertilizer', ylab='Yield')\nlines(new.fert,new.pred) #Adds lines to original plot",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#exponential-function",
    "href": "labs/Lab12.html#exponential-function",
    "title": "Lab 12 - Non-linear models",
    "section": "Exponential function",
    "text": "Exponential function\n\n\\(y=y_0e^{kx}\\)\nwhere the parameters are y0 which is the multiplier which expresses the starting or final value, and k which is negative for exponential decay and positive for the exponential growth.\nThe half life (for decay) or doubling time (for growth) can be calculated as\n\\(\\frac{log_e 2}{k}\\)\nLimitations:\n\nharder to fit than polynomials\nexponential growth has no horizontal asymptote; keeps going up.\n\n\n\n\nExercise 3: Initial estimates for exponential growth function\nIn this exercise we will find initial estimates of the parameters of an exponential growth model by visual assessment of plots of the data and/or rough calculations. The initial estimates of the parameters are needed as starting points for the iterative fitting methods we will use in the practicals, e.g. SOLVER in Excel and the nls() function in R.\nThe plot and table below presents the population of the world from 1650-1965.\nWe wish to model the data with an exponential growth function of the form;\n\\(y=y_0e^{kx}\\)\nwhere\n\ny is the population in the year x,\ny0 is the population in 1650 and\nk is the rate constant.\n\n\n\nProvide an initial estimate of y0.\n\nThe parameter k can be estimated from a linear model fitted to loge population against year.\nRather than formally fitting a linear model you could estimate the slope approximately by using the smallest and largest value to estimate the slope and therefore k.\n\nUse this approach to estimate k.\n\nHint: \\[\nslope = k = \\frac{log_e y_{max} - log_e y_{min}}{x_{max} - x_{min}}\n\\]\n\nFor an exponential growth model the doubling time of a population can be estimated by loge2 /k.\n\nExamine the graph and/or table to estimate the doubling time and use this to estimate k. You will have to make k the subject in the equation for estimating the doubling time.\n\nHow similar were the estimates of k?\n\n\n\n\nExercise 4 : Exponential growth models\nThis data is from Jenkins & Adams (2010) who studied soil respiration rates against temperature for different vegetation communities in the Snowy Mountains. They fitted an exponential growth model to the data.\nThe purpose of this exercise is to illustrate the dangers of using Excel’s in-built functions for statistics more complex than calculating means and fitting simple models.\nPlot the data in Excel and using the Add Trendline… option. Make sure tick the option for displaying the equation in the graph.\n\nThe researchers performed the experiment up to a temperature of 40 degrees C, would you expect exponential growth in the respiration rate to continue if high temperatures were considered? Is there a better model?\n\nNow fit the same model in R using the nls function. Code to get you started is:\n\ntemp&lt;-c(5,10,20,25,30,35,40)\nrespiration&lt;-c(1,2,4,6,8,11,18)\n\n##Initial parameters\nexp.mod&lt;-c(Bp=1.0,Cp=0.1)\n\n##Fits exponential model\nres.exp&lt;-nls(respiration~Bp * exp(temp*Cp),start=exp.mod,trace=T)\n\n##Summarise model\nsummary(res.exp)\n\n\nNow you can fit a line to the plot. Does this look similar to your trendline in Excel?\n\n\n#Plots raw data\nplot(temp,respiration,xlab='Temperature',ylab='Respiration')\n\n#Creates new dataset for predictions ( 5 to 40 at an interval of 1)\ntemp.new&lt;-seq(5,40,1)\n\n#Makes predictions onto temp.new\npred.exp&lt;-predict(res.exp,list(temp=temp.new))\n\n#Adds model fit to existing plots\nlines (temp.new,pred.exp)\n\nCompare the parameters values between Excel and R. You can extract the RSS value from an nls object by using the code below:\n\ndeviance(res.exp)\n\n\nCalculate the RSS value for the Excel exponential model. Based on this, which is the better model?\n\nWhen faced with the need to fit an exponential function, one approach that was used before computing power became readily accessible was to log the y values which linearises the relationship with x, enabling the modeller to use a simple linear model.\nIf we linearise, model would be \\(log_e(y) = b_0 + b_1x\\), where \\(e^{b_0}\\) is the y0 parameter in an exponential model, and b1 is the k parameter in the exponential model. This is similar to what was demonstrated in the lecture this week.\nIn Excel, log the soil respiration data and fit a linear model. You will see that the fitted model gives the same values as the exponential model fitted to the untransformed data.\nIf you compare the r2 values for both you will see they are the same. This means that Excel reports the r2 of the linear model fitted to logged respiration as the r2 of the exponential model fitted to the raw data. This is naughty of Excel.\n\nFor the dataset used here the exponential model fits it so well the Excel approach is only slightly different to the correct approach used in R.\nIn cases where the model does not fit the data so well the differences would be larger. Logarithm makes smaller values larger and larger values smaller.\nWHY IS THIS SUB-OPTIMAL?\n\nRegression modelling assumes that the residuals are normally distributed so logging normally distributed data will change the distribution to a non-normal one – it is best to analyse the data without transformation.\nModelling data on the logged scale reduces the impact that larger values have on minimising the RSS but when you plot the fitted model with the original data you may observe large discrepancies for larger values. In other words, using a linear model on the log(data) can result in a higher discrepancy for larger values when plotting the fitted model on the original data. Therefore, the model fitted to the logged data is not necessarily the best on the original data.\nThe reporting of the R-squared on the logged data on a model purported to be fitted to untransformed data is just wrong as the R-squared on the log scale will be better as the variation in the data has been reduced but we really want to know how well an exponential model fits the raw data.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab12.html#logistic-function",
    "href": "labs/Lab12.html#logistic-function",
    "title": "Lab 12 - Non-linear models",
    "section": "Logistic function",
    "text": "Logistic function\n\n\\(y = A + \\frac{C}{1+e^{-B(t-M)}}\\)\nwhere the parameters are A which is the starting point or initial condition, C which is the value of y where the functions flattens out (the horizontal asymptote), M which is the value of x where the change in y is largest (always occurs at y = A + 0.5C), and 0.5B is growth rate of individual when t = M.\nCommonly used to model growth that has a sigmoid shape, i.e. where growth is initially slow, then picks up to a maximum, then slows down again as the system reaches a maximum.\nLimitations:\nHarder to fit than polynomials.\n\n\n\nExercise 5: Logistic models\nIn this exercise you will model the yield of pasture over time (since sowing) using a logistic function. Since the yield at sowing = 0, the A parameter will be set to 0 and the form of the logistic function that you will use is:\n\\(y = A + \\frac{C}{1+e^{-B(t-M)}}\\)\n\nFit the model in Excel using SOLVER and in R using the nls function. Refer to previous exercises for nls function structure and implement the logistic function. You will need to specify credible starting values for the parameters (see Box below).\n\n\nPlot the fitted model with the observations.\n\n\nCompare the parameters estimated by R and Excel.\n\n\nSTARTING VALUES\nWhen fitting non-linear functions (i.e. logistic or exponential) using iterative procedures such as nls or SOLVER the starting estimates of the parameters have to be approximately correct to find a solution.\nThe best way to ensure that you have suitable starting values is to plot the data with the predictions overlaid for your starting parameters. You can then see how close your initial model is to the data.\nFor the logistic model the parameters have clear meanings so suitable starting values are:\n\nC is the maximum value of y so just use the maximum value of y in the raw data as the starting value.\nM is the value of t when y = 0.5C, this can be read approximately off the graph.\nB is harder to estimate but good starting point is 0.1.\n\n\n\nThat’s it for Module 3! Great work exploring non-linear models!\nThank you all (students and demonstrators!) for your hard work and enthusiasm throughout this Module. Good luck with Project 3 and the final exam!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 12 - Non-linear models"
    ]
  },
  {
    "objectID": "labs/Lab09.html#before-you-begin",
    "href": "labs/Lab09.html#before-you-begin",
    "title": "Lab 9 - Describing relationships",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-09.Rmd or similar. The following data files are required:\n\nENVX1002_practical_data_Regression.xlsx",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-1-linear-modelling-in-excel",
    "href": "labs/Lab09.html#exercise-1-linear-modelling-in-excel",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 1: Linear Modelling in Excel",
    "text": "Exercise 1: Linear Modelling in Excel\nThis exercise focusses on fitting the model parameters and demonstrating two ways a model can be fitted - numerical or analytical;\n\nAnalytical: equation(s) are used directly to find solution, e.g. estimate parameters that minimise residual sum of squares\nNumerical: computer uses “random guesses” to find set of parameters to that minimises objective function, in this case residual sum of squares\n\nWe mostly use R for modelling, but R does everything automatically. It is important to know what is going on ‘behind the scenes’, which is why we are starting in Excel. Similar to the tutorial, you will be calculating each component of the model parameter step by step in the exercises that follow.\n\n\n1.1 Horses\nThis is our example of Analytical fitting method.\nThe number of horses on Canadian farms appeared to decrease after the war:\n\nTo see whether this is likely to be true, fit a model to the above data ‘by hand’ in Excel. To aid the calculation it is recommended to fill out the Excel table provided ENVX1002_practical_data_Regression.xlsx, you can find it in the spreadsheet labelled Horses.\n\nThe table we have provided in Excel has broken the regression parameter equations (b0, b1) into smaller components so you can understand the underlying mechanisms and where these values come from.\n\nPlot the two variables in Excel and fit a line. You can fit a number of models in Excel simply by right clicking on the scatter of points clicking Add Trendline …. Within the add Tendline window (see screenshot below), a number of options are given, here we want Linear and we want to tick display the Equation and Display r-squared on the chart.\n\n\n\n\n\nScreenshot: Format Trendline\n\n\n\nThe R-squared value is a measure of how well the model fits the data where 1.0 is a perfect fit; we will discuss this more in Week 10. The values which appear in the model equation should be the same as those obtained in your earlier calculations.\n\nAlthough it is important for the model equation, do you think the intercept provides a realistic value in this particular case? What does it mean?\n\n\nCalculate the correlation coefficient using the =CORREL function in Excel. Type =CORREL( and highlight the Year column, and then after a comma highlight the Horses column and close the brackets.\n\n\nIf the relationship was non-linear would this would be a good statistic to use to describe the relationship between horse and years? Explain your answer.\n\n\n\n\n1.2 Fertiliser data\nThis is our example of numerical fitting of a model.\nFigure 1 shows a plot of yield against fertiliser where a linear model is fitted through the scatterplot of raw observations. Intuitively you would draw this as a line that comes as close to possible to all observations which you may have come across as a ‘line of best fit’. In this exercise we will explore how models can be fitted automatically based on least-squares estimation.\n\n\n\n\nFigure 1: Plot of Yield-response to fertiliser\n\n\n\nIn Figure 1 you will notice that the line does not fit the data perfectly which is typical of biological and environmental data. A measure of how far the model is from the data is the residual.\n\\[\\begin{equation}\nresidual = y_i - \\hat{y}_1\n\\label{1}\n\\end{equation}\\]\nWhere \\(y_i\\) is the observed value for the ith observation and \\(\\hat{y}_1\\) is the predicted value for the ith observation. In this case the predicted value is based on the linear model.\nIf we add up the square of the residuals for the n observations we get something called the Residual Sum of Squares (\\(SS_{res}\\)):\n\\[\\begin{equation}\nSS_{res} =\\sum_{i = 1}^{n} (y - \\hat{y})^2 \\label{2}\n\\end{equation}\\]\nThe best fitting model will have the smallest RSS. The general method is called least-squared estimation. We will now use Excel to find the optimal model.\nEnter values of 2 for the y-intercept (\\(b_0\\)) and 3 for the slope (\\(b_1\\)) in cells H2:H3. These are the initial guess values.\n\nNow use these parameter values to create predictions for each value of fertiliser in the Predicted column.\n\nMake sure that rather than writing in the value ‘2’ and ‘3’ for your predicted column, you refer to cells H2 and H3 (write as $H$2 and $H$3, see screenshot below). Once you have completed the equation, you can apply the equation to the other rows by clicking on the small box at the bottom right corner of the cell and drag it down the rows. Writing dollar signs into your references to H2 and H3 prevents your equation from moving down the row column.\n\n\n\n\nScreenshot of Predicted column input\n\n\n\n\nUse this information to calculate (i) residuals (ii) residuals2 (iii) RSS.\nCreate a plot similar to Figure 1 where the observations are plotted as symbols and the model predictions are a line. You should have your spreadsheet set up so that if you change the values of the parameters the plotted line changes as well. Try to fit the line manually. This can be difficult, especially for non-linear models.\nFollow instructions provided in the Tutorial, or in the file How to install Solver to ensure you have Solver ready to use in Excel.\n\nOnce you have added Solver, click on the tab Data &gt;&gt; Solver, and you will see the following (see screenshot below). For Set Objective, you need to select the cell where your RSS value has been calculated. We wish to minimize this so we click on Min, and we do this by Changing Cells where the parameters of the model are found, in this case the y-intercept and slope. Before clicking Solve, make sure you can see your calculated values so you can see how your how it all changes.\n\n\n\n\nScreenshot of solver with input values\n\n\n\nWhen ready, click on Solve and it should find a solution for the minimum RSS. Solver uses an iterative procedure to find the minimum RSS which means it successively guesses values until it finds the optimal value. This is a numerical solution to the problem of model fitting.\nYour ‘SOLVED’ parameters should be the same as what appears in your trendline equation.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-2-fitting-a-model-in-r",
    "href": "labs/Lab09.html#exercise-2-fitting-a-model-in-r",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 2: Fitting a model in R",
    "text": "Exercise 2: Fitting a model in R\nNow we have a deeper understanding of what is going on behind the scenes, we can fit linear models in R.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon’t forget to save as you go!\n\n\n2.1 Have a go - Fertiliser data\nYou will use the fertiliser data to fit a linear model in R. As we covered fitting linear models in the Tutorial, it is now your turn to have a go at fitting the models (with some hints along the way).\n\nRead the following code into R:\n\n\n# add the data to R Studio\nfert &lt;- c(0, 1, 2, 3, 4, 5)\nyield &lt;- c(2, 13, 19, 18, 25, 33)\n\n\nTo visually identify any trends or relationships, create a scatterplot of fertiliser vs yield. From the scatterplot you see, are there any relationships or trends evident?\n\n\n# Create a scatterplot\nplot(fert, yield)\n\n\nTo numerically determine whether there is a relationship, calculate the correlation coefficient. (assume data is normally distributed). Does the correlation coefficient indicate a relationship between fertiliser and yield?\n\n\n# To calculate the correlation coefficient:\ncor(fert, yield)\n\n\nYou can now fit the model in R using the lm() function. Remember to tell R the name of the object you want to store it as (in this case, model.lm &lt;-), then state the name of the function. The arguments within the function (i.e. between the brackets) will be yield ~ fert, with yield being the response variable and fert being the predictor.\n\n\n# Run your model\n## yield = response variable (x)\n## fert = predictor variable (y)\nmodel.lm &lt;- lm(yield ~ fert)\n\n# Obtain model summary - In here you can obtain the model parameters\n# Look for Intercept Estimate and fert Estimate\nsummary(model.lm)\n\n\nIn the model output obtained from summary(model.lm) the model parameters will be listed under ‘Estimate’ for the intercept and ‘fert’. Compare these values to what you have calculated in Excel.\n\n\nBased on this output, what would the model equation be? Does it match your findings in Excel?\n\n\nYou can now fit your model to the scatterplot you created previously using the abline() function. Make sure you run the plot function and the abline function in one go. If the lines are run separately, an error may appear saying “plot.new hasn’t been called yet”; this is because the abline function requires a current plot on which it can overlay the line.\n\nAlso remember, when presenting plots (e.g. in a report), they should be able to stand alone and be self-explanatory. We therefore need to make sure there are clear axis labels. This can be done using ‘xlab’ and ‘ylab’ arguments.\n\n# Add the linear model to your scatterplot\nplot(fert, yield, xlab = \"fertiliser applied\", ylab = \"Yield\")\nabline(model.lm, col = \"red\")\n\n\n\n\n2.2 ABARES data\nIn this final example we will be using a dataset obtained from the Australian Bureau of Agricultural and Resource Economics and Sciences (ABARES). The dataset provides a measure of productivity growth (TFP; Total Factor Productivity) in the Australian dairy industry from the years 1978 to 2018.\nMore information about the ABARES dataset and productivity can be found here.\n\nRead in the data from the Excel file for today’s practical.\n\nBecause we have such a large dataset this time, it is better to read the data straight from Excel than read in each individual value. Reading straight from the source file in Excel saves time and reduces chance of input error.\n\nlibrary(readxl)\n\nABARES &lt;- read_excel(\"data/ENVX1002_practical_data_Regression.xlsx\", sheet = \"ABARES\")\n\n\nCreate a scatterplot of Year against TFP. Dont forget the format will be different now - instead of only mentioning the object name, e.g. plot(yield, fert), you will need to refer to the specific columns within the ABARES dataset. (i.e. ABARES$Year).\n\n\nCan you see a trend between TFP and Year? Or are the points evenly scattered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary. Year will be our predictor and TFP will be our response variable. What are the model parameters (i.e. \\(b_0\\) and \\(b_1\\))?\n\n\nWhat would the equation for this model be?\n\n\nOverlay your model onto the scatterplot you produced earlier. When plotting make sure you refer to the column names as you did for the model (e.g. ABARES$Year).\n\n\nThat’s it! Great work today. Next week: interpreting linear models!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-1-cars-stopping-distance",
    "href": "labs/Lab09.html#exercise-1-cars-stopping-distance",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 1: Cars stopping distance",
    "text": "Exercise 1: Cars stopping distance\nFor this exercise we willuse the inbuilt dataset cars to see if there is a relationship between a cars speed (mph) and stopping distance (fft).\n\nhead(cars)\n\n\nCreate a scatterplot of speed vs distance\n\n\nIs there are trend? Or are the point evenly scatered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary.\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-2-penguins",
    "href": "labs/Lab09.html#exercise-2-penguins",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nFor this exercise, we will be using the palmer penguin data set to see if there is a relationship between bill and flipper length.\n\n#Load libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n#Clean data\npenguins &lt;- penguins%&gt;%\n  na.omit()#remove missing data\n\nhead(penguins)\n\n\nCreate a scatterplot of bill length vs flipper legth\n\n\nIs there are trend? Or are the point evenly scatered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary. What are the parameters?\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "labs/Lab09.html#exercise-3-old-faithful-geyser-data",
    "href": "labs/Lab09.html#exercise-3-old-faithful-geyser-data",
    "title": "Lab 9 - Describing relationships",
    "section": "Exercise 3: Old Faithful Geyser Data",
    "text": "Exercise 3: Old Faithful Geyser Data\nFor this exercise, we will be looking at the relationship between geyser eruption time and time between erupttions, using the inbuilt data set faithful\n\nhead(faithful)\n\n\nCreate a scatterplot of eruptions vs waiting time\n\n\nIs there are trend? Or are the point evenly scatered?\n\n\nCalculate the correlation coefficient between these two variables. Is there a strong relationship?\n\n\nFit a model to your data and obtain the model summary.\n\n\nWhat would the equation of the line be?\n\n\nOverlay your model onto the scatterplot you produced earlier",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 9 - Describing relationships"
    ]
  },
  {
    "objectID": "001-intro.html",
    "href": "001-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "A practical definition of ENVX is “The use of statistical and computing methods to answer quantitative biological questions.”\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.\n\n\n\nENVX is at the interface of applied statistics, data science and life and environmental science\n\n\n\n\n\nYou will soon learn that the University of Sydney has a great tradition in research (with many of our efforts earning international recognition). Your teachers will include some experiences of or findings from their research in their classes, and you will have an opportunity to participate in research projects throughout your degree. ENVX comprises of three vertically integrated units designed to equip you to confidently approach the design and data analysis aspects of your research. The diagram below shows one view of the process of research (or experimentation).\n\n\n\nThe research process\n\n\n\n\nStatistics provides us with an avenue for exploring and reporting the findings of the research. In terms of reporting, it allows us to give a measure (generally a probability) of the extent to which our conclusion could be wrong. Statistics is a means of informing the decision-making process.\n\n\n\n\nData is the information we collect on the subjects, factors and variables we are interested in studying. Data may be textual (i.e. words) or numerical.\n\n\nPopulations may be real and therefore able to be listed e.g. all veterinarian practices in the Sydney region. They may be hypothetical and unable to all be listed e.g. all dogs possible that could have a particular treatment applied to them.\nEach sampling unit (or observation) in the population (e.g. plot of land, animal, etc.) has a value y (e.g. nematode count, gestational length): \\(y_1,y_2,y_3,...y_N\\). Typically the population size (N) is very large - even infinite! The population can be described by population parameters e.g. population mean = \\(\\mu\\), population variance = \\(\\sigma^2\\). These are generally characters from the Greek alphabet. Since populations are large, we usually cannot determine these values exactly.\nOften we wish to make generalisations about populations that are too large or too difficult to survey completely. In these cases we sample the population and use characteristics of the sample to extrapolate to characteristics of the larger population. We can define a sample (of size n) as drawn from population “at random”. We call each piece of information recorded about a sampling unit or subject (e.g. plant, person, animal, 1x1m plot of land) an observation.\n\n\n\nn = 20 units (v) sampled at random from population of N = 1,000 (O)\n\n\nThe sample is taken to be representative of the population – but there are no guarantees! It is easier/quicker/cheaper to take a sample than study the entire population (since n &lt;&lt; N). The results obtained from the sample not important in themselves - the importance is in how it can be used to estimate population parameters i.e. \\(\\bar{y}\\) estimates \\(\\mu\\); \\(s^2\\) estimates \\(\\sigma^2\\).\nExample: For a sample of \\(n = 10\\) cows, gestation length in cattle was measured in days. The sample mean \\(\\bar{y} = 345\\) days estimates the population mean \\(\\mu = ???\\) days. The sample standard deviation \\(s = 10\\) days estimates population standard deviation \\(\\sigma = ???\\) days.\n\n\n\nThe table below shows fresh weights of cabbages that were included in a field trial that was investigating the effects of irrigation frequency and plant spacing on cabbage yields. This fresh weight measurement is on a continuous scale.\nGenerally in an experiment, several different characteristics of the subjects are measured/recorded. We may score the level of insect damage to leaves of the plant on a scale of 0 to 3 (0 = no damage, 1 = slight damage, 2 = moderate damage, 3 = heavy damage). This is a discrete categorical scale as only certain values on the scale are defined, but it is also an ordered scale. If we had difficulty deciding how to categorize plants using this scale, we might choose to classify plants as either damaged or not damaged, which is a binary measurement scale.\nAlready you can begin to see that from just one fairly simple field trial quite a lot of data can be generated, also data of differing properties/types.\n\n\n\nYields of cabbage (mean fresh weight per head in kg) for 24 plots (Source: Mead, Curnow & Hasted (2003)).\n\n\nData may also be classified as either quantitative (e.g. root length) or qualitative (e.g. plant species). Quantitative observations are based on some sort of measurement e.g. length, weight, temperature, pH. Qualitative observations are based on categories reflecting a quality or characteristic of the observed event e.g. male vs. female, diseased vs. healthy, mutant vs. wild type.\nThe most common types of variables are:\n\nContinuous (and interval) data can assume any value in some (possible unbounded) interval of real numbers. Examples are length, weight, temperature, volume, height.\nDiscrete variables assume only isolated values. E.g. trees per hectare, items per quadrat, number of diseased plants in a section of a glasshouse. They arise from counting – usually either the number of successes in n trials (binary data) OR the number of occurrences of the event in an interval of time or space (count data).\nCategorical variables\n\nBinary variables (listed above as discrete variables) may also be thought of as categorical variables since the subject falls into either of 2 mutually exclusive categories (yes/no, alive/dead, diseased/not diseased etc.).\nOrdinal variables are not measured but nevertheless have a natural ordering. E.g. candidates for political office can be ranked by individual voters. The rank values have no inherent meaning outside the “order” that they provide. That is, a candidate ranked 2 is not twice as preferable as the person ranked 1. (Compare this with measurement variables where a plant 2 feet tall is twice as tall as a plant1 foot tall. With measurement variables such ratios are meaningful, while with ordinal variables they are not.)\nNominal data is qualitative data. Some examples are species, gender, genotype, phenotype, healthy/diseased. Unlike ranked data, there is no “natural” ordering that can be assigned to these categories.\n\n\nNote that some applied statistics texts will define the types of data slightly differently to that shown above.\nIn this unit of study most emphasis will be on the analysis of continuous measurement variables. However a few basic analyses for discrete and categorical variables will be covered.\nRecognising the type of data we have measured is REALLY important as it helps to determine the choice of analysis (and even the descriptive statistics we undertake e.g. the mean of a score doesn’t make sense, but the median is a good alternative measure of central tendency).\n\n\n\nSometimes you will want to explore the relationship between two (or more) variables that you have measured in your research. You will explore the strength of the relationship and the nature of it (e.g. linear, exponential etc.).\nIn the most simple case, we examine the amount of variability in one variable (Y, the dependent variable) that is explained by changes in another variable (X, the independent variable). Often the X variable is called the predictor and the Y variable the response.\n\n\n\nVariation is the norm. It occurs in both observational studies as well as designed experiments. For example, river flow varies from sampling time to sampling time and from sampling location to sampling location. Also, levels of soil contamination on a site vary from site to site.\nThis type of variation distinguishes the biological and environmental sciences from the physical sciences, which shows relatively little variability:\n\nDropping a ball from a certain height: the time to reach the ground is (nearly) identical each time;\nAmount of a chemical product produced when reagents are mixed are (nearly) identical and predictable.\n\nBiological data is far more variable due to environmental and genetic effects. To interpret biological data, we need to control variation by an appropriate experimental design, and adjust for variation by means of statistical analyses.\n\n\n\n\nWe use statistical distributions to determine the probability of occurrence of our particular sets of observations. A statistical distribution is a representation (using either mathematical formula or a table) of all possible outcomes of a given event. The most well-known distribution is the normal (or Gaussian) distribution for which the probability distribution function is the familiar bell-shaped curve.\n\n\n\nThe aim of sampling is to gain a representative picture of the population. There are various methods and strategies for doing this. Experimental groups/samples must be constructed without bias and must be large enough to give the researcher an acceptable level of confidence in the results.\n\n\n\nA hypothesis is a tentative explanation for the initial or ad hoc observations made. It suggests a cause and effect or associative relationship that is testable, e.g. yield response to nitrogen (N) fertilizer. The purpose and design of an experiment is to test the hypothesis.\n\n\n\nWe decide whether or not a research outcome if significant by conducting a statistical test. Firstly we set arbitrary critical thresholds of probability (P-values). The occurrence of an event whose estimated probability is less than a critical threshold is regarded as a statistically significant outcome. The usual significance level chosen is P&lt;0.05. You will also see P&lt;0.01 and P&lt;0.001 used in research literature. Which statistical test you use (there are many!) depends on the type of data you have collected and the question you wish to ask.\n\n\n\nIn this unit of study, we will focus on the use of one statistical package called R. It has been designed specifically for statistical analysis and is freely available. It is a powerful tool for data analysis and is widely used in the biological and environmental sciences.\nYou will also learn to use Microsoft Excel to organize and summarise your data. In the next section some of the basic features of Excel are presented. See computer lab 1.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#what-is-envx",
    "href": "001-intro.html#what-is-envx",
    "title": "Introduction",
    "section": "",
    "text": "A practical definition of ENVX is “The use of statistical and computing methods to answer quantitative biological questions.”\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.\n\n\n\nENVX is at the interface of applied statistics, data science and life and environmental science",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#introduction-to-research",
    "href": "001-intro.html#introduction-to-research",
    "title": "Introduction",
    "section": "",
    "text": "You will soon learn that the University of Sydney has a great tradition in research (with many of our efforts earning international recognition). Your teachers will include some experiences of or findings from their research in their classes, and you will have an opportunity to participate in research projects throughout your degree. ENVX comprises of three vertically integrated units designed to equip you to confidently approach the design and data analysis aspects of your research. The diagram below shows one view of the process of research (or experimentation).\n\n\n\nThe research process\n\n\n\n\nStatistics provides us with an avenue for exploring and reporting the findings of the research. In terms of reporting, it allows us to give a measure (generally a probability) of the extent to which our conclusion could be wrong. Statistics is a means of informing the decision-making process.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#data",
    "href": "001-intro.html#data",
    "title": "Introduction",
    "section": "",
    "text": "Data is the information we collect on the subjects, factors and variables we are interested in studying. Data may be textual (i.e. words) or numerical.\n\n\nPopulations may be real and therefore able to be listed e.g. all veterinarian practices in the Sydney region. They may be hypothetical and unable to all be listed e.g. all dogs possible that could have a particular treatment applied to them.\nEach sampling unit (or observation) in the population (e.g. plot of land, animal, etc.) has a value y (e.g. nematode count, gestational length): \\(y_1,y_2,y_3,...y_N\\). Typically the population size (N) is very large - even infinite! The population can be described by population parameters e.g. population mean = \\(\\mu\\), population variance = \\(\\sigma^2\\). These are generally characters from the Greek alphabet. Since populations are large, we usually cannot determine these values exactly.\nOften we wish to make generalisations about populations that are too large or too difficult to survey completely. In these cases we sample the population and use characteristics of the sample to extrapolate to characteristics of the larger population. We can define a sample (of size n) as drawn from population “at random”. We call each piece of information recorded about a sampling unit or subject (e.g. plant, person, animal, 1x1m plot of land) an observation.\n\n\n\nn = 20 units (v) sampled at random from population of N = 1,000 (O)\n\n\nThe sample is taken to be representative of the population – but there are no guarantees! It is easier/quicker/cheaper to take a sample than study the entire population (since n &lt;&lt; N). The results obtained from the sample not important in themselves - the importance is in how it can be used to estimate population parameters i.e. \\(\\bar{y}\\) estimates \\(\\mu\\); \\(s^2\\) estimates \\(\\sigma^2\\).\nExample: For a sample of \\(n = 10\\) cows, gestation length in cattle was measured in days. The sample mean \\(\\bar{y} = 345\\) days estimates the population mean \\(\\mu = ???\\) days. The sample standard deviation \\(s = 10\\) days estimates population standard deviation \\(\\sigma = ???\\) days.\n\n\n\nThe table below shows fresh weights of cabbages that were included in a field trial that was investigating the effects of irrigation frequency and plant spacing on cabbage yields. This fresh weight measurement is on a continuous scale.\nGenerally in an experiment, several different characteristics of the subjects are measured/recorded. We may score the level of insect damage to leaves of the plant on a scale of 0 to 3 (0 = no damage, 1 = slight damage, 2 = moderate damage, 3 = heavy damage). This is a discrete categorical scale as only certain values on the scale are defined, but it is also an ordered scale. If we had difficulty deciding how to categorize plants using this scale, we might choose to classify plants as either damaged or not damaged, which is a binary measurement scale.\nAlready you can begin to see that from just one fairly simple field trial quite a lot of data can be generated, also data of differing properties/types.\n\n\n\nYields of cabbage (mean fresh weight per head in kg) for 24 plots (Source: Mead, Curnow & Hasted (2003)).\n\n\nData may also be classified as either quantitative (e.g. root length) or qualitative (e.g. plant species). Quantitative observations are based on some sort of measurement e.g. length, weight, temperature, pH. Qualitative observations are based on categories reflecting a quality or characteristic of the observed event e.g. male vs. female, diseased vs. healthy, mutant vs. wild type.\nThe most common types of variables are:\n\nContinuous (and interval) data can assume any value in some (possible unbounded) interval of real numbers. Examples are length, weight, temperature, volume, height.\nDiscrete variables assume only isolated values. E.g. trees per hectare, items per quadrat, number of diseased plants in a section of a glasshouse. They arise from counting – usually either the number of successes in n trials (binary data) OR the number of occurrences of the event in an interval of time or space (count data).\nCategorical variables\n\nBinary variables (listed above as discrete variables) may also be thought of as categorical variables since the subject falls into either of 2 mutually exclusive categories (yes/no, alive/dead, diseased/not diseased etc.).\nOrdinal variables are not measured but nevertheless have a natural ordering. E.g. candidates for political office can be ranked by individual voters. The rank values have no inherent meaning outside the “order” that they provide. That is, a candidate ranked 2 is not twice as preferable as the person ranked 1. (Compare this with measurement variables where a plant 2 feet tall is twice as tall as a plant1 foot tall. With measurement variables such ratios are meaningful, while with ordinal variables they are not.)\nNominal data is qualitative data. Some examples are species, gender, genotype, phenotype, healthy/diseased. Unlike ranked data, there is no “natural” ordering that can be assigned to these categories.\n\n\nNote that some applied statistics texts will define the types of data slightly differently to that shown above.\nIn this unit of study most emphasis will be on the analysis of continuous measurement variables. However a few basic analyses for discrete and categorical variables will be covered.\nRecognising the type of data we have measured is REALLY important as it helps to determine the choice of analysis (and even the descriptive statistics we undertake e.g. the mean of a score doesn’t make sense, but the median is a good alternative measure of central tendency).\n\n\n\nSometimes you will want to explore the relationship between two (or more) variables that you have measured in your research. You will explore the strength of the relationship and the nature of it (e.g. linear, exponential etc.).\nIn the most simple case, we examine the amount of variability in one variable (Y, the dependent variable) that is explained by changes in another variable (X, the independent variable). Often the X variable is called the predictor and the Y variable the response.\n\n\n\nVariation is the norm. It occurs in both observational studies as well as designed experiments. For example, river flow varies from sampling time to sampling time and from sampling location to sampling location. Also, levels of soil contamination on a site vary from site to site.\nThis type of variation distinguishes the biological and environmental sciences from the physical sciences, which shows relatively little variability:\n\nDropping a ball from a certain height: the time to reach the ground is (nearly) identical each time;\nAmount of a chemical product produced when reagents are mixed are (nearly) identical and predictable.\n\nBiological data is far more variable due to environmental and genetic effects. To interpret biological data, we need to control variation by an appropriate experimental design, and adjust for variation by means of statistical analyses.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#statistical-distributions",
    "href": "001-intro.html#statistical-distributions",
    "title": "Introduction",
    "section": "",
    "text": "We use statistical distributions to determine the probability of occurrence of our particular sets of observations. A statistical distribution is a representation (using either mathematical formula or a table) of all possible outcomes of a given event. The most well-known distribution is the normal (or Gaussian) distribution for which the probability distribution function is the familiar bell-shaped curve.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#sampling",
    "href": "001-intro.html#sampling",
    "title": "Introduction",
    "section": "",
    "text": "The aim of sampling is to gain a representative picture of the population. There are various methods and strategies for doing this. Experimental groups/samples must be constructed without bias and must be large enough to give the researcher an acceptable level of confidence in the results.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#hypotheses",
    "href": "001-intro.html#hypotheses",
    "title": "Introduction",
    "section": "",
    "text": "A hypothesis is a tentative explanation for the initial or ad hoc observations made. It suggests a cause and effect or associative relationship that is testable, e.g. yield response to nitrogen (N) fertilizer. The purpose and design of an experiment is to test the hypothesis.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#statistical-tests",
    "href": "001-intro.html#statistical-tests",
    "title": "Introduction",
    "section": "",
    "text": "We decide whether or not a research outcome if significant by conducting a statistical test. Firstly we set arbitrary critical thresholds of probability (P-values). The occurrence of an event whose estimated probability is less than a critical threshold is regarded as a statistically significant outcome. The usual significance level chosen is P&lt;0.05. You will also see P&lt;0.01 and P&lt;0.001 used in research literature. Which statistical test you use (there are many!) depends on the type of data you have collected and the question you wish to ask.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "001-intro.html#software",
    "href": "001-intro.html#software",
    "title": "Introduction",
    "section": "",
    "text": "In this unit of study, we will focus on the use of one statistical package called R. It has been designed specifically for statistical analysis and is freely available. It is a powerful tool for data analysis and is widely used in the biological and environmental sciences.\nYou will also learn to use Microsoft Excel to organize and summarise your data. In the next section some of the basic features of Excel are presented. See computer lab 1.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "R Guide/01-getting-started.html",
    "href": "R Guide/01-getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Welcome to the R Guide!",
    "crumbs": [
      "**🛟 R Guide**",
      "Getting started"
    ]
  },
  {
    "objectID": "module02/03-nonparametric1.html",
    "href": "module02/03-nonparametric1.html",
    "title": "Chi-squared tests",
    "section": "",
    "text": "Chi-squared tests\nThe chi-squared distribution (where chi is pronounced ‘ky’) is a very widely used distribution in statistics. Its symbol is 2. It has MANY applications. Here we will consider only two of these applications – tests of agreement with expected outcomes, and contingency tables.\n7.1 Notes on the 2 Distribution\nThe density function for a 2 distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the 2 density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g. 1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the 2 distribution approaches the shape (and properties) of a normal distribution.\nFigure 7.1 The shape of the 2 density function for various degrees of freedom.\nThe mean and variance of a 2 variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as  (Greek n), then\nMean of   variable =   (i.e. mean = df)\nVariance of   variable = 2 (i.e. variance = twice the df)\nCritical values of a 2 distribution are given in the 2 probability table that appears as Appendix A.5.\n7.2 Testing Agreement of Frequency Data with Expectation Models\n7.2.1 Steps in Chi-Squared Tests of Agreement\nThe process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\nChoose an appropriate hypothesis test for the type of data you have, and the type of question you’re asking.\nChoose the level of significance for the test.\nWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\nCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\nCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic – however there are alternative tests designed to accommodate these situations.\nCalculate the test statistic and degrees of freedom.\nObtain the P-value.\nDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n7.2.2 Examples: Testing Whether Outcomes are Equally Probable\nEXAMPLE 1\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a “fair” coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a “fair” coin.\nEXAMPLE 2 (from Mead et al, 2003)\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\nCheese First preference A 5 B 7 C 18 D 10 Total 40\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n =  =9.80.\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the value of 9.80 is greater than the value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since P &lt; 0.05.)\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with\n =  =…      etc.\n7.2.3 Example: Testing Whether Outcomes are in Expected Proportions (from Mead et al, 2003)\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen).\nThe figures obtained for the primula plants follow. Normal eye Primrose Queen eye Total Flat leaves 328 122 450 Crimped leaves 77 33 110 Total 405 155 560\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, ¾ of the total 560 observation in the first class of the characteristic and the remaining ¼ in the second class. Further, this model predicts that ¾ of the flat-leaved plants should have normal eyes, resulting in ¾ × ¾ of all the plants or 9/16 with flat leaves and normal eyes; the remaining ¼ of the flat-leaved plants, which is ¼ × ¾ or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes.\nThe calculation of these expected or predicted proportions is shown below.\nNormal eye  Primrose Queen eye\nFlat leaves ¾ × ¾ = 9/16 ¼ × ¾ = 3/16 Crimped leaves ¾ × ¼ = 3/16 ¼ × ¼ = 1/16\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35.\nThe observed and expected frequencies are summarized in the table below.\nNormal eye  Primrose Queen eye\nFlat leaves 328 (315) 122 (105) Crimped leaves 77 (105) 33 (35)\n    =  \n    = 0.54 + 2.75 + 7.47 + 0.11 = 10.77. \nWe compare 10.77 with the 5% point of the distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable.\nSee pp. 332-333 of Mead et al, 2003 for what to do next… after rejecting the model.\n7.3 Contingency Tables\n7.3.1 Example: (2 x 2) Contingency Table\nConsider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\nRecovered   \nYes No  Total\nProcedure 1 14 6 20 Procedure 2 8 12 20 Total 22 18 40\nThis is one form of a 22 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\nSolution: We will perform a statistical hypothesis test:\nH0: There is no difference in the true recovery rates for animals on either procedure H1: The recovery rates do differ.\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\nH0: p1 = p2\nH1: p1 ≠ p2\nEstimates of individual recovery rates are = 14/20 = 0.7 and = 8/20 = 0.4. Is this difference due to chance?\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n  =  .\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be 20 22/40 = 20 0.55 = 11 animals. In general this can be written as:\nSo the expected frequencies for the cells in the table are:\n  Expected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\nRecovered   \nYes No  Total\nProcedure 1 14 (11) 6 (9) 20 Procedure 2 8 (11) 12 (9) 20 Total 22 18 40\nThe table shows observed (and expected) frequencies. The 2 test statistic is then calculated using:\nLarge values of 2 indicate discrepancies between observed and expected frequencies, i.e. large values indicate that H0 should be rejected in favour of H1.\nThe df of this 2 test is 1 for a 22 contingency table. In general,\nIf H0 is true, the observed 2 is just one observation from a 2 distribution with 1 df:\nSince , there is (just) not sufficient evidence to reject H0. Thus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n  7.3.2 Example: (4 x 3) Contingency Table\nThe second example is a 43 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n    Disease Status      \nVaccine No Mild Severe Total Control 100 (137.3) 71 (42.6) 29 (20.1) 200 A 146 (133.9) 32 (41.6) 17 (19.6) 195 B 149 (132.5) 28 (41.2) 16 (19.3) 193 C 146 (137.3) 37 (42.6) 17 (20.1) 200 Total 541 168 79 788\nThe table shows observed (and expected) frequencies.\nWe test H0 that there is no association between disease status and vaccination given, i.e. all vaccinations have equal effectiveness.\nAssuming H0 is true, the expected frequencies are calculated as follows:\ne.g. Expected frequency for an animal in the Control, No disease group:\n.\nAs before, the test statistic is and this will have (4-1)(3-1) = 6 df.\nThis is how to do the test in the Stats menu in GenStat:\nStats &gt; Statistical Tests &gt; Contingency Tables…\nGenStat requires the data to be set up as a 43 Table type of spreadsheet (as opposed to a Variate or Matrix). It then reports the X2 test statistic and P-Value by selecting the Pearson method.\nPearson chi-squared value is 45.22 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\nLikelihood chi-squared value is 43.34 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001   The ML 2 is calculated as follows:\nThe degrees of freedom are (r – 1)(c – 1) as before, where r and c are the numbers of rows and columns respectively.\nIn general, the 2 approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\nDisease Status  \nVaccine No Mild Severe Total Control 10 7 3 20 A 15 3 2 20 B 15 3 2 20 C 15 4 2 21 Total 55 17 9 81",
    "crumbs": [
      "**📗 Module 2**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "module02/01-ttest1.html",
    "href": "module02/01-ttest1.html",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.\n\n\n\n(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci\n\n\n\n\nThere are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\nqt(0.975, 24)\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).\n\n\n\n\nIf an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.\n\n\n\nAs you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\nkurtosis(bcc_df$BloodCellCount)\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\nkurtosis(bcc_df$log_BloodCellCount)\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\nkurtosis(tccb_df$log_TcCB_ppb)\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.\n\n\n\n\nSometimes research questions are framed not as “What is a plausible range of values for such and such a parameter?” but rather “Are the data consistent with this particular value for the parameter?”. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the “health” of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don’t - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause , the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g. in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou’ll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).\n\n\n\n\n\n\nThe liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e. (population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e. more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let’s say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).\n\n\n\n\nOften researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case…\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!\n\n\n\n\n\nFor the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e. we assume we don’t know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs. \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the “old school” t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R’s t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has “heavier” tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).\n\n\n\n\nHypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI’s.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance – the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI’s as they are more informative than the p-value alone. For example, the width of the CI’s says something about the precision of the estimate.\n\n\n\nExample\nThere is evidence that total nitrogen levels in the river – like many other environmental quality data – are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical “smiley” shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n\nshapiro.test(tn$TN)\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\nWe can use the mean of this transformed data i.e. the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nshapiro.test(tn$log10_TN)\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet’s test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-intervals-for-mu",
    "href": "module02/01-ttest1.html#confidence-intervals-for-mu",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "href": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "href": "module02/01-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "There are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\nqt(0.975, 24)\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#what-is-meant-by-confidence-interval",
    "href": "module02/01-ttest1.html#what-is-meant-by-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "If an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "href": "module02/01-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "As you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\nkurtosis(bcc_df$BloodCellCount)\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\nkurtosis(bcc_df$log_BloodCellCount)\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\nkurtosis(tccb_df$log_TcCB_ppb)\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#hypothesis-testing",
    "href": "module02/01-ttest1.html#hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Sometimes research questions are framed not as “What is a plausible range of values for such and such a parameter?” but rather “Are the data consistent with this particular value for the parameter?”. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the “health” of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don’t - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause , the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g. in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou’ll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-z-tests",
    "href": "module02/01-ttest1.html#one-sample-z-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "The liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e. (population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e. more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let’s say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#general-notes-on-hypothesis-testing",
    "href": "module02/01-ttest1.html#general-notes-on-hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Often researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case…\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-t-tests",
    "href": "module02/01-ttest1.html#one-sample-t-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "For the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e. we assume we don’t know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs. \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the “old school” t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R’s t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has “heavier” tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "href": "module02/01-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Hypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI’s.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance – the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI’s as they are more informative than the p-value alone. For example, the width of the CI’s says something about the precision of the estimate.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/01-ttest1.html#one-sample-t-test-with-data-transformation",
    "href": "module02/01-ttest1.html#one-sample-t-test-with-data-transformation",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Example\nThere is evidence that total nitrogen levels in the river – like many other environmental quality data – are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical “smiley” shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n\nshapiro.test(tn$TN)\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\nWe can use the mean of this transformed data i.e. the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\nshapiro.test(tn$log10_TN)\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet’s test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**📗 Module 2**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html",
    "href": "module02/02-ttest2.html",
    "title": "Two-sample \\(t\\)-test",
    "section": "",
    "text": "In the one-sample tests just considered, we compared a population mean, \\(\\mu\\) (where our experimental sample represents the population) with a fixed numeric value of interest. In practice, this is fairly rare. More frequently we have samples drawn from two (or more) populations and we compare the two means to see if the treatments produce similar results.\nThe null hypothesis is that the two samples come from a population with the same true mean, \\(\\mu\\). This is commonly expressed as\n\\[H_0: \\mu_1 = \\mu_2\\]\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are the means of the two populations. The alternative hypothesis is that the two means are different, i.e.\n\\[H_1: \\mu_1 \\neq \\mu_2\\]",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#assumptions",
    "href": "module02/02-ttest2.html#assumptions",
    "title": "Two-sample \\(t\\)-test",
    "section": "Assumptions",
    "text": "Assumptions\nThe two-sample \\(t\\)-test assumes that the data are :\n\nContinuous,\nat least approximately normally distributed, and\nthe variances of the two sets are homogeneous (i.e. the same).\n\nIdeally these assumptions should be tested before you carry about the two-sample \\(t\\)-test.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#what-if-assumptions-are-not-met",
    "href": "module02/02-ttest2.html#what-if-assumptions-are-not-met",
    "title": "Two-sample \\(t\\)-test",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\nIf the data do not meet either of these assumptions, then you may choose an appropriate transformation or look for another technique to use e.g. a non-parametric method (see non-parametric sections of this book).\nIf the variances cannot be assumed to be equal (but the data are normally distributed), there is a way of adjusting the two-sample \\(t\\)-test to compensate for this – it’s called the Satterthwaite’s approximation – more on this later!",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#three-variants-of-the-two-sample-t-test",
    "href": "module02/02-ttest2.html#three-variants-of-the-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Three Variants of the Two-Sample \\(t\\)-test",
    "text": "Three Variants of the Two-Sample \\(t\\)-test\nThere are 3 different ways in which a two-sample \\(t\\)-test can be used. They ALL assume that the data is approximately normally distributed.\n\nIndependent samples, equal variance - run a two-sample \\(t\\)-test assuming equal variances.\nIndependent samples, unequal variance - run a two-sample \\(t\\)-test assuming unequal variances.\nPaired samples - run a paired \\(t\\)-test.\n\nIn choosing which variant of the \\(t\\)-test is applicable in your situation, you first need to decide whether your two samples are paired or unpaired (independent).",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#paired-data",
    "href": "module02/02-ttest2.html#paired-data",
    "title": "Two-sample \\(t\\)-test",
    "section": "Paired Data",
    "text": "Paired Data\nPaired samples arise when we measure pairs of similar experimental units. In this pair of experimental units, one unit receives Treatment 1 and the other receives Treatment 2.\nIn some cases, treatments are applied to the same experimental unit e.g. one half of a piece of fruit receives Trt 1 and the other half Trt 2; two plants of different varieties are grown in the same pot (here variety is the treatment).\nIn recognising this pairing in our analysis we are taking into account the fact that biological variation between pairs is likely to be larger than within pairs. This way, we get a clearer picture of the difference that is due to the treatment factor.\n\nExamples of paired data\n\nObservations at two times on the same experimental unit\n\nBefore and after readings of particle matter in the air on 3 sites near a new power station (before the station was built, and after it became operational). (Dytham 2003, 80)\nMeasurements of water flow on two consecutive days at 6 sites along a river. (Dytham 2003, 83)\n\nObservations on 2 halves/parts of the same experimental unit\n\nOne half of each (uncut) grapefruit was exposed to sunlight, and the other half was shaded (McConway et al 1999, p. 198).\nA standard (recommended) variety of wheat is compared with a new variety via 2 similar plots on each of 8 farms (Clewer and Scarisbrick 2001, p. 46).",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#two-sample-t-test-for-independent-samples",
    "href": "module02/02-ttest2.html#two-sample-t-test-for-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Two-Sample t-Test for Independent Samples",
    "text": "Two-Sample t-Test for Independent Samples\nProcedure for the test:\n\nSet up the null and alternate hypotheses\nDecide on the level of significance, 5%, 1%, 0.01% etc.\nCheck the assumptions of normality and equal variance. We use an F-test to formally test for equality of variance.\nCalculate the test statistic \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{SED}\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means, and \\(SED\\) is the standard error of the difference between the means.\nCalculate the degrees of freedom (df).\nFind the P-value in printed statistical tables or via GenStat or Excel.\nMake a statistical conclusion by comparing this P-value to your chosen level of significance (if P &lt;α, then reject null hypothesis).\nCalculate the confidence interval.\nInterpret your results biologically.\n\n\n\n\n\n\n\nCalculating the test statistic\n\n\n\nWhen calculating the test statistic, use \\(y_1\\) as the larger mean. This will give a positive value for \\(t\\).",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "href": "module02/02-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)",
    "text": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)\nA two-sample t-test shows whether there is evidence of a difference in population means. The magnitude of this difference can be estimated with a confidence interval.\nA 95% confidence interval for the true difference \\(\\mu_1 - \\mu_2\\) is given by \\[\\bar{y}_1 - \\bar{y}_2 \\pm t^{\\alpha/2}_{df} \\times SED\\]\nwhere \\(\\bar{y}_1 - \\bar{y}_2\\) is the difference between the sample means, \\(t^{\\alpha/2}_{df}\\) is the critical value from the t-distribution for the chosen level of significance and degrees of freedom, and \\(SED\\) is the standard error of the difference between the means.\nThe df and SED need to take into account whether or not you are assuming equal variances.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "href": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with EQUAL Variances",
    "text": "SED and df for Independent Samples with EQUAL Variances",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "href": "module02/02-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with UNequal Variances",
    "text": "SED and df for Independent Samples with UNequal Variances\nWhat do you do if when you’re checking your assumptions for a two- sample t-test you find that the variances are not equal? You can go ahead with a modified t-test or you can choose a different test.\nThis modified t-test used in the case of unequal variances is often called Satterthwaite’s approximate t-test.\n\nSatterthwaite’s Approximate t-Test\nThe null and alternate hypotheses remain the same i.e. \\[H_0: \\mu1 = \\mu2\\] or \\[H_0: \\mu1 - \\mu2 = 0\\]\nThe formula for the test statistic, \\(t\\), in Satterthwaite’s approximate test is a little different to that for the t-test with equal variances. The s.e.d. changes because we can no longer use a pooled estimate of the variance (since the variances cannot be assumed equal).\nA correction for unequal variance is made to the degrees of freedom.\nThen proceed as usual through the rest of the test - i.e. find the P-value; draw a statistical conclusion about your hypothesis; interpret your results biologically.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#f-test-for-equality-of-variances",
    "href": "module02/02-ttest2.html#f-test-for-equality-of-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "F-Test for Equality of Variances",
    "text": "F-Test for Equality of Variances\nOne of the conditions for the independent sample t-test to be valid is that the population variances σ12 and σ22 are equal.\nTo test the null hypothesis that σ12 = σ22 divide the larger s2 value by the smaller s2 to obtain the variance ratio, v.r.:\nTo undertake this two-tailed test at the 5% level you need to carry out the one-tailed test at the 2.5% level. You can use the 2.5% F table of critical values, or you can use GenStat via the menus Data&gt;Probability Calculations… to find the P-value.\nIf you use the printed statistical table, you will need to compare the critical value you find there with the variance ratio you calculated. If the calculated variance ratio &gt; critical value, reject H0 and conclude that the variances are significantly different.",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module02/02-ttest2.html#example-two-sample-t-test",
    "href": "module02/02-ttest2.html#example-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Example: Two-Sample t-Test",
    "text": "Example: Two-Sample t-Test\nWeights of two breeds of cattle are to be compared: 15 cattle from Breed 1 and 12 cattle from Breed 2 were randomly sampled. Their recorded weights (kg) are shown.\nBreed 1 Breed 2 148.1 187.6 146.2 180.3 152.8 198.6 135.3 190.7 151.2 196.3 146.3 203.8 163.5 190.2 146.6 201.0 162.4 194.7 140.2 221.1 159.4 186.7 181.8 203.1 165.1\n165.0\n141.6\nThe following descriptive statistics are obtained from these data.\nBreed 1 Breed2\nSample mean (kg) 153.700 196.175 Sample s.d. (kg) 12.301 10.616\nIs there any systematic difference in their weights? The question, “Is there any systematic difference in their weights?”, is asking whether or not there is any significant difference between the 2 samples – and in effect if there is any significant difference between the 2 breeds because the factor that distinguishes the 2 samples is breed.\nWe could answer this question by testing whether or not the population mean weights for the 2 breeds can be assumed to be equal:\nH0: μbreed 1 = μBreed 2 versus H1: μBreed 1 ≠ μBreed 2\nTo test this particular hypothesis about the equality of the means (as an avenue for answering our broader question), we would use a two-sample t-test.\nHowever, it is REALLY important to remember that there are other tests and hypotheses that we could use to answer our broader question (about whether or not there is any statistical difference between the weights of the 2 breeds). We’ll consider some non-parametric alternatives in Biometry 2.\nTo proceed with a two-sample t-test, there are 2 assumptions that we need to check: • Normality of Breed 1 data and normality of Breed 2 data • Equality of variances of the 2 samples\nBoth of these checks are hypothesis tests in their own right and contain the usual elements of a hypothesis test i.e. null & alternate hypothesis; test statistic; df; P-value or critical value; conclusion.\nTesting the Assumption of Normality If you are doing the test by hand, you would need to assume that the data are normally distributed (and hope this is true). At least the data in this example is continuous… which is one small step towards normality.\nTesting the Assumption of Equality of Variance H0: σ2Breed 1 = σ2Breed 2 versus H1: σ2Breed 1 ≠ σ2Breed 2\nTest statistic:\nThere are 2 degrees of freedom to calculate for an F test. They are called the numerator df, 1 and the denominator df, 2. (Remember from school days, numerator is the top half of a fraction, denominator is the bottom half of a fraction.) The  that looks like a curly ‘v’ is the Greek letter ‘nu’.\nFor an F test used to test equality of variance, df are: 1 = n1 – 1, 2 = n2 – 1.\nHere, 1 = nBreed 1 -1 = 15 - 1 = 14 and 2 = nBreed 2 - 1 = 12 – 1 = 11.\nUsing the 2.5% F table, we can compare (at the upper tail) the Fcritical and Fobserved values. If Fobs &gt; Fcrit, we reject H0 and conclude that the variances of the 2 samples are NOT equal.\nFrom these tables, it is not possible to find exactly, so we will make do with the closest possible value .\nWe find Fcrit ≈ 3.33 and Fobs = 1.34. Since Fcrit &gt; Fobs ,we CAN assume that the variances are equal.\nSo… assuming normality and equal variances, we can complete a “pooled” two-sample t-test where\nTesting the null hypothesis that H0: μBreed 1 = μBreed 2, we find that there is a significant difference in the mean weights of the two breeds of cattle (T = 9.46, df = 25, P &lt; 0.001). The mean weight of Breed 2 is significantly higher and we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1.\nThis last piece of information, “we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1”, is obtained from the 95% confidence interval for the true difference (μ1 – μ2).\nNB. Recall that a 95% confidence interval for the true difference (μ1 – μ2) is .\n6.10 Paired t-Test\nWe can re-write the generic null hypothesis for a two-sample test of means, H0: μ1 = μ2 as… H0: μ1 – μ2 = 0, or H0: μd = 0.\nThe paired t-test is actually a one-sample t-test of the differences between pairs of observations (from 2 different samples).\nAssumption: • Data is approximately normally distributed.\nSince we are doing a one-sample t-test on the differences then the assumption of equal variances is not relevant. Firstly, create a single column of data to use in the t test. Each value in the column corresponds to the difference between the 2 values for a particular matched pair.\nFarm Yield Variety A (kg) Yield Variety B (kg) Difference 1 17.8 14.7 3.1 2 18.5 15.2 3.3 3 12.2 12.9 -0.7 4 19.7 18.3 1.4 5 10.8 10.1 0.7 6 11.9 12.2 -0.3 7 15.6 13.5 2.1 8 12.5 9.9 2.6\nThe difference for Farm 1 = 17.8 – 14.7 = 3.1.\nData Source: Clewer and Scarisbrick (2001, 46)\nSecondly, find some summary statistics of the differences so you can complete the t test:\nNo. of values, n = 8        Sum = 12.2          Mean = 1.525\nVariance = 2.299            Std Dev = 1.516\nand df = n -1, where d is usually 0.\n6.11 Confidence Interval for 1 – 2 (Paired Samples)\nA 95% confidence interval for the mean difference is",
    "crumbs": [
      "**📗 Module 2**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "R Guide/10-copilot.html",
    "href": "R Guide/10-copilot.html",
    "title": "GitHub Copilot and RStudio",
    "section": "",
    "text": "This is the GitHub Copilot guide!",
    "crumbs": [
      "**🛟 R Guide**",
      "GitHub Copilot and RStudio"
    ]
  },
  {
    "objectID": "CONTRIBUTING/quick-start.html",
    "href": "CONTRIBUTING/quick-start.html",
    "title": "Contributing",
    "section": "",
    "text": "To contribute to this project you will need to first ensure that you have been properly configured. This includes setting up your workflow, installing the necessary dependencies, and understanding the project’s structure.\n\n\nThese are basic instructions but will get you up and running quickly. For more detailed instructions, see the relevant documentation (when suggested). 1. Clone the repository to your local machine. 2. Create a new branch with a descriptive name (e.g., lab-11). 3. Make your changes, and save/commit your changes frequently. 5. Render the entire project to ensure that your changes have not broken anything. 6. Push your branch to the GitHub repository. 7. Create a pull request in GitHub or GitKraken for review.\n\n\n\nHere are some best practices for effective branching and pull requests:\n\nAlways create a new branch for each piece of work.\nUse descriptive branch names that reflect the work being done.\nKeep branches focused on a single task or feature.\nMake small, frequent commits with clear messages.\nUpdate your branch regularly with changes from main.\nTest your changes thoroughly before creating a pull request.\nWrite clear pull request descriptions explaining your changes.\nDelete branches after they are merged.\n\nFinally, as long as the main branch is always deployable (i.e. it renders without errors), you can be confident that your changes will not break the project."
  },
  {
    "objectID": "CONTRIBUTING/quick-start.html#quick-start",
    "href": "CONTRIBUTING/quick-start.html#quick-start",
    "title": "Contributing",
    "section": "",
    "text": "These are basic instructions but will get you up and running quickly. For more detailed instructions, see the relevant documentation (when suggested). 1. Clone the repository to your local machine. 2. Create a new branch with a descriptive name (e.g., lab-11). 3. Make your changes, and save/commit your changes frequently. 5. Render the entire project to ensure that your changes have not broken anything. 6. Push your branch to the GitHub repository. 7. Create a pull request in GitHub or GitKraken for review."
  },
  {
    "objectID": "CONTRIBUTING/quick-start.html#tips",
    "href": "CONTRIBUTING/quick-start.html#tips",
    "title": "Contributing",
    "section": "",
    "text": "Here are some best practices for effective branching and pull requests:\n\nAlways create a new branch for each piece of work.\nUse descriptive branch names that reflect the work being done.\nKeep branches focused on a single task or feature.\nMake small, frequent commits with clear messages.\nUpdate your branch regularly with changes from main.\nTest your changes thoroughly before creating a pull request.\nWrite clear pull request descriptions explaining your changes.\nDelete branches after they are merged.\n\nFinally, as long as the main branch is always deployable (i.e. it renders without errors), you can be confident that your changes will not break the project."
  },
  {
    "objectID": "labs/Lab10.html",
    "href": "labs/Lab10.html",
    "title": "Lab 10 - Linear Functions",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nFit simple linear models and obtain associated model summaries in R\nOverlay fitted models onto scatterplots in R\nUndertake hypothesis testing to determine if slope \\(\\neq\\) 0\nCheck assumptions are met prior to assessing model output\nAssess model summary in terms of fit and P-values",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#before-you-begin",
    "href": "labs/Lab10.html#before-you-begin",
    "title": "Lab 10 - Linear Functions",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-10.Rmd or similar. The following data files are required:\n\nENVX1002_wk10_practical_data_Regression.xlsx\n\nLast week you fitted models in R, now it is time to understand what the output means.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon’t forget to save as you go!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-1-walkthrough---fertiliser",
    "href": "labs/Lab10.html#exercise-1-walkthrough---fertiliser",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 1: Walkthrough - Fertiliser",
    "text": "Exercise 1: Walkthrough - Fertiliser\nLike last week, we will start off our R modelling journey by fitting a model to the fertiliser data.\n\nRead the following code into R:\n\n\n# add the data to R Studio\nfert &lt;- c(0, 1, 2, 3, 4, 5)\nyield &lt;- c(2, 13, 19, 18, 25, 33)\n\n\n1.1 Scatterplot and correlation\nTo visually identify any trends or relationships, last week we created a scatterplot of the data. This is helps us visually understand our points so we know what we might expect from the model, and possibly identify if the relationship is looking non-linear.\n\n# Create a scatterplot\nplot(fert, yield)\n\nRemembering back to last week, we then calculated the correlation coefficient to numerically determine whether there was a relationship between fertiliser and yield.\nUsing the code below, we found there was quite a strong relationship between fertiliser and yield (0.964):\n\n# Correlation coefficient\ncor(fert, yield)\n\n\n\n1.2 State hypotheses\nRemembering back to the lecture and tutorial, the general equations for our hypotheses are:\n\\[\nH_0 : \\beta_1 = 0\n\\]\n\\[\nH_1 : \\beta_1 \\neq 0\n\\]\nIn the context of our data, the hypotheses would be:\n\\(H_0\\): Slope = 0; fertiliser is not a significant predictor of yield.\n\\(H_1\\): Slope \\(\\neq\\) 0; fertiliser is a significant predictor of yield.\nIf P &gt; 0.05, we fail to reject the null hypothesis that the true slope (\\(\\beta_1\\)) is equal to 0. If this is the case, it means our model does not predict better than the mean of our observations, and so there is no advantage to using our model over the mean of y (\\(\\bar{y}\\)).\nIf we find there is a high probability of the slope not being equal to 0 (P &lt; 0.05), we can reject the null hypothesis and conclude our model is better at predicting than the mean of our observations.\nNow we understand what we are testing for, we can fit the model.\n\n\n1.3 Fit the model\nAfter checking the correlations and scatterplot, we need to fit the model using the lm() function. Remember to tell R the name of the object you want to store it as (in this case, model.lm &lt;-), then state the name of the function. The arguments within the function (i.e. between the brackets) will be yield ~ fert, with yield being the response variable and fert being the predictor.\n\n# Run your model\n## yield = response variable (x)\n## fert = predictor variable (y)\nmodel.lm &lt;- lm(yield ~ fert)\n\n\n\n1.4 Check assumptions\nThis time, before obtaining our model summary, we need to check our assumptions.\nSmaller sample size (n = 6) makes it harder to check whether the assumptions have been met, but we will still run through the check.\nLooking at each plot, we can see that the residual plots don’t look the best;\n\nResiduals vs fitted: Will tell us if the relationship is linear. We are looking for an even scatter around the mean, and red line should be reasonably straight. In this case the red line is not too straight, but the scatter seems even.\nNormal Q-Q: If the residuals are normally distributed, most of the points should lie along the dotted line. Our points follow the line, but do not lie on it.\nScale-Location: This is for testing whether the variance is equal in the residuals at each value of x. If the variance is equal, then we would expect to see an even scatter and no fanning. In this case, there is no fanning.\nResiduals vs Leverage: This will help us identify whether there are any single points influencing the slope or intercept of the model. We can see in the output plot there is a point sitting in the bottom-right corner, outside the dotted line, indicating that it may be having an influence on the model.\n\nThese plots are only useful as an example of how to obtain and interpret output. If we wanted to obtain a more reliable check of our assumptions (and a more reliable model), we would need a larger sample size (n &gt; 10).\n\n# Check your assumptions!!\npar(mfrow = c(2, 2)) # sets plots to show as 2x2 grid\nplot(model.lm)\n\nIn this case, we will assume the assumptions have been met and continue to assess the model output.\n\n\n1.5 Model output\nUse the summary() function to obtain output for your model:\n\n# Obtain model summary\nsummary(model.lm)\n\nIn the model output obtained from summary(model.lm) the model parameters will be listed under ‘Estimate’ for the intercept and ‘fert’. Last week we concluded the equation to be:\n\\[\nYield = 4.7619 + 5.4286*fert\n\\]\nFurthermore, from our model estimate, we can say that as fertiliser increases by 1, yield will increase by 5.4286.\n\n\n1.6 Is the model useful?\nWhen looking at the model summary output, we obtain the p-value from the coefficients table. We are interested in the P-value for fert and not the intercept.\nThe significance of the intercept P-value depends on our scientific question. We only really look at our intercept P-value when we want to extrapolate our line to the intercept, and know if the intercept is equal to zero (\\(H_0\\)) or not (\\(H_1\\)). This depends on your dataset and whether it makes sense to do so.\nAlso notice how the p-values for the F-test at the bottom of the summary output, and the t-test p-values we are using are the same. The F-test gives us an idea whether our overall model is significant and in this case, as we are only using a single predictor, the P-values will be the same.\nTherefore we can say the following:\nObserving the model output, we can see that the P-value for fert is significant (P = 0.00196) and we can say that as P &lt; 0.05, we reject the null hypothesis. We can conclude our slope is not 0 and our model is a better way to predict yield than the mean of our observations.\n\n\n1.7 How good is the model?\nTo assess how well the model fits the data, we need to look at the Residual standard error (3.147) and the r-squared value (0.9287).\n\nWe can say that our residual standard error is relatively low in terms of our response variable.\nOur r-squared indicates that fertiliser accounts for 92.9% of variation in yield. That’s pretty good!\n\nNote how Multiple R-squared and Adjusted R-squared are similar. For simple linear models we can opt for the multiple r-squared, but when using multiple predictors we need to use adjusted r-squared.\nFinally, to visually present our results, we can provide a scatterplot with the model overlaid.\n\n# Add the linear model to your scatterplot\nplot(fert, yield, xlab = \"fertiliser applied\", ylab = \"Yield\")\nabline(model.lm, col = \"red\")\n\n\n\n1.8 Our conclusions\nNow we can put our interpretations together to form the conclusion:\nObserving the model output, we can see that the P-value for fert is significant (P = 0.00196) and we can say that as P &lt; 0.05, we reject the null hypothesis. We can conclude our slope is not 0 and our model is a better way to predict yield than the mean of our observations.\nWe can therefore conclude that fertiliser is a significant predictor of crop yield as the slope is not equal to zero (P &lt; 0.05), and it accounts for 92.9% of the variation in yield.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-2-toxicity-in-peanuts",
    "href": "labs/Lab10.html#exercise-2-toxicity-in-peanuts",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 2: Toxicity in peanuts",
    "text": "Exercise 2: Toxicity in peanuts\nData: Peanuts spreadsheet\nThe data comprise of, for 34 batches, the average level of the fungal contaminant aflatoxin in a sample of 120 pounds of peanuts and the percentage of non-contaminated peanuts in the whole batch.\nThe data were collected with the aim of being able to predict the percentage of non-contaminated peanuts (Peanuts$percent) from the aflatoxin level (Peanuts$toxin) in a sample. We will now investigate whether this is the case.\nFirst thing’s first! Let’s read in the data using read_xlsx command:\n\nlibrary(readxl)\nPeanuts &lt;- read_xlsx(\"data/ENVX1002_wk10_practical_data_Regression.xlsx\", sheet = \"Peanuts\")\nhead(Peanuts)\n\n\n2.1 Scatter plot\nMake a scatter plot of the data.\n\nplot(Percent ~ Toxin, data = Peanuts)\n#Alternate syntax:\n#plot(Peanuts$Percent, Peanuts$Toxin)\n\n\nDescribe the relationship between the two variables.\n\n\nWould you say that the percentage of non-contaminated peanuts in a batch could be predicted accurately from the level of aflatoxin in a sample via a linear relationship?\n\n\n\n2.2 State Hypotheses\n\nWhat are the hypotheses we are testing? State them as the formulae and in the context of the study.\n\n\n\n2.3 Fit a linear model\nUse fit a linear model (lm()) to the Peanut data.\n\n# fit a linear model using lm()\nmod &lt;- lm(Percent ~ Toxin, data = Peanuts)\n\n\n\n2.4 Check assumptions\n\nInspect and comment on the residual plots- have the assumptions been met?\n\n\npar(mfrow = c(2, 2))\nplot(mod)\n\n\n\n2.5 Observe model output\nOnce you are certain the assumptions are met, you can proceed to look at the regression output.\n\nComment on the overall fit of the regression, i.e. Is the model fit good? Is the model significant, and how much variation in percentage of non-contaminated peanuts does aflatoxin level account for?\n\n\n# Look at output with summary\nsummary(mod)\n\n\nIs toxin a significant predictor of percentage non-contaminated peanuts? If so, how can we tell?\n\n\nInterpret the slope parameter in terms of quantifying the relationship between toxin and percent.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-3-dippers",
    "href": "labs/Lab10.html#exercise-3-dippers",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 3: Dippers",
    "text": "Exercise 3: Dippers\nData: Dippers spreadsheet\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\nTwenty-two sites were included in the survey. For the purpose of fitting a simple linear model, the dataset has been reduced to two variables:\n\nThe number of breeding pairs of Dippers per 10 km of river\nThe numbers of caddis fly larvae (Log(Number+1) transformed)\n\nNow it is your turn to work through the steps as above. Does the number of caddis fly larvae influence the number of breeding pairs of Dippers?\n\nRead in the data from today’s Excel sheet, the corresponding sheet name is “Dippers”\n\n\nObtain a scatterplot, are there signs of a relationship between breeding pair density and caddis fly larvae?\n\n\nWhat are the hypotheses we are testing? State them as the formulae and in the context of the study.\n\n\nLet’s investigate further. Run the model, but before looking at our model output, are the assumptions ok?\n\n\n# Run model\ndipper.lm &lt;- lm(Br_Dens ~ LogCadd, data = Dippers)\n\n# Check assumptions\npar(mfrow = c(2, 2))\nplot(dipper.lm)\n\n\nOnce you are happy assumptions are good, you can use summary() to interpret the model output.\n\n\nWhat is the equation for our model, incorporating our coefficients?\n\n\nBased on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd?\n\n\nIs LogCadd a significant predictor of Dipper breeding pair density? How can we tell?\n\n\nHow good is the fit of our model?\n\n\nWhat conclusions can we make from this model output?\n\n\nA final thought; Does our result make sense within the context? i.e. why might the Dipper breeding pair density be related to LogCadd?\n\nGreat work fitting simple linear models! Next week we step it up with multiple linear regression.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-1-cars-stopping-distance",
    "href": "labs/Lab10.html#exercise-1-cars-stopping-distance",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 1: Cars stopping distance",
    "text": "Exercise 1: Cars stopping distance\nUse the cars dataset from last week to test if speed (mph) is a predictor of stopping distance (fft).\n\nhead(cars)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-2-penguins",
    "href": "labs/Lab10.html#exercise-2-penguins",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 2: Penguins",
    "text": "Exercise 2: Penguins\nUse the palmer penguins dataset to test if flipper length is a significant predictor of bill length.\n\n#Load libraries\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n#Clean data\npenguins &lt;- penguins%&gt;%\n  na.omit()#remove missing data\n\nhead(penguins)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab10.html#exercise-3-old-faithful-geyser-data",
    "href": "labs/Lab10.html#exercise-3-old-faithful-geyser-data",
    "title": "Lab 10 - Linear Functions",
    "section": "Exercise 3: Old Faithful Geyser Data",
    "text": "Exercise 3: Old Faithful Geyser Data\nUsing the inbuilt faithful dataset, test whether waiting time is a significant predictor of eruption time.\n\nhead(faithful)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 10 - Linear Functions"
    ]
  },
  {
    "objectID": "labs/Lab06.html",
    "href": "labs/Lab06.html",
    "title": "Lab 06 – Two-sample t-test",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to use R to calculate a 2-sample t-test\n\nindependent samples with constant variance\nindependent samples with unequal variance\npaired samples\ndata transformations\n\nApply the steps for hypothesis testing from lectures\nLearn how to interpret statistical output",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#before-you-begin",
    "href": "labs/Lab06.html#before-you-begin",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-06.Rmd or similar. The following data files are required:\n\nBarley.csv\nPlant_growth.csv\nTurbidity.csv\n\nThe following external packages are used in this lab. Install them if you have not done so already.\ninstall.packages(c(\"tidyverse\", \"car\"), \n  repo = \"https://cloud.r-project.org\")\nFinally, try to complete today’s lab exercises in pairs and try out pair programming, where one person writes the code and the other person reviews each line as it is written. You can swap roles every 10 minutes or so. This is a great way to learn from each other and to improve your coding skills.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-1-barley-walk-through",
    "href": "labs/Lab06.html#exercise-1-barley-walk-through",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 1: barley (walk-through)",
    "text": "Exercise 1: barley (walk-through)\n\nBackground\nAn experiment was designed to compare two varieties of spring barley. Thirty four plots were used, seventeen being randomly allocated to variety A and seventeen to variety B. Unfortunately five plots were destroyed. The yields (t/ha) from the remaining plots were as they appear in the file Barley.csv.\n\n\nInstructions\nFirst, quickly explore the data; then, utilise the HATPC process and test the hypothesis that the two varieties give equal yields, assuming that the samples are independent.\n\n\nHATPC:\n\nHypothesis\nAssumptions\nTest (statistic)\nP-value\nConclusion\n\n\n\n\n\n\n\nLevel of significance\n\n\n\nThe level of significance is usually set at 0.05. This value is generally accepted in the scientific community and is also linked to Type 2 errors, where choosing a lower significance increases the likelihood of failing to reject the null hypothesis when it is false.\n\n\n\n\nData exploration\nFirst we load the data and inspect its structure to see if it needs to be cleaned or transformed. The glimpse() function is a tidy version of str() that provides a quick overview of the data that focuses on the variables, ignoring data attributes.\n\n\nTry to compare str() and glimpse() to see what the differences are.\n\nbarley &lt;- readr::read_csv(\"data/Barley.csv\") # packagename:: before a function lets you access a function without having to load the whole library first\ndplyr::glimpse(barley)\n\nRows: 29\nColumns: 2\n$ Variety &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ Yield   &lt;dbl&gt; 4.6, 4.3, 3.8, 3.4, 3.9, 3.9, 3.9, 4.4, 3.6, 3.6, 4.7, 3.9, 3.…\n\n\nThe Variety column is a factor with two levels, A and B, but it is defined as a character. We can convert it to a factor using the mutate() function from the dplyr package, but it is not necessary for the t-test since R will automatically convert it to a factor.\n\nlibrary(tidyverse)\nbarley &lt;- mutate(barley, Variety = as.factor(Variety))\n\nQuickly preview the data as a plot to see if there are any trends or unusual observations.\n\nbarley %&gt;%\n  ggplot(aes(x = Variety, y = Yield)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nA trained eye will anticipate that the data may not meet the assumption of equal variance; however, we will test this assumption later. Otherwise, there appear to be no unusual observations in the data.\n\n\nHypothesis\nWhat are the null and alternative hypotheses? We can use the following notation:\n\\[H_0: \\mu_A = \\mu_B\\] \\[H_1: \\mu_A \\neq \\mu_B\\]\nwhere \\(\\mu_A\\) and \\(\\mu_B\\) are the population means of varieties A and B, respectively.\n\n\nIt is important that when using mathematical symbols to denote the null and alternative hypotheses, you should always define what the symbols mean. Otherwise, the reader may not understand what you are referring to.\nThe equations above are written in LaTeX, a typesetting system that is commonly used in scientific writing. You can learn more about LaTeX here. The raw syntax used to write the equations are shown below:\n$$H_0: \\mu_A = \\mu_B$$\n$$H_1: \\mu_A \\neq \\mu_B$$\nWhy do we always define the null and alternative hypotheses? In complex research projects or when working in a team, it is important to ensure that everyone is on the same page. By defining the hypotheses, you can avoid misunderstandings and ensure that everyone is working towards the same goal as the mathematical notation is clear and unambiguous.\n\n\nAssumptions\n\nNormality\nThere are many ways to check for normality. Here we will use the QQ-plot. Use of ggplot2 is preferred (as a means of practice) but since we are just exploring data, base R functions are not a problem to use.\n\nUsing ggplot2Using base R\n\n\n\nggplot(barley, aes(sample = Yield)) +\n  stat_qq() +\n  stat_qq_line() +\n  facet_wrap(~Variety) #facet_wrap ensures there are separate plots for each variety rather than one plot with all the data in Yield \n\n\n\n\n\n\n\n\n\n\n\npar(mfrow = c(1, 2))\nqqnorm(barley$Yield[barley$Variety == \"A\"], main = \"Variety A\") # square brackets to subset the data by variety\nqqline(barley$Yield[barley$Variety == \"A\"])\nqqnorm(barley$Yield[barley$Variety == \"B\"], main = \"Variety B\")\nqqline(barley$Yield[barley$Variety == \"B\"])\n\n\n\n\n\n\n\n\n\n\n\nQuestion: Do the plots indicate the data are normally distributed?\nAnswer: Yes, the data appear to be normally distributed as the QQ-plot shows that the data points are close to the line.\n\n\nHomogeneity of variance\nFrom the boxplot, we can see that there is some indication that the variances are not equal. We can test this assumption using Bartlett’s test or Levene’s test; here we will just use Bartlett’s test.\n\nbartlett.test(Yield ~ Variety, data = barley)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Yield by Variety\nBartlett's K-squared = 14.616, df = 1, p-value = 0.0001318\n\n\nQuestion: Does the Bartlett’s test indicate the two groups have equal variances? What effect will that have on the analysis?\nAnswer: The two groups have unequal variance (Bartlett’s test: \\(X^2 = 14.6\\), \\(p &lt; 0.01\\)). This means that we will need to use the Welch’s t-test, which does not assume equal variances.\n\n\n\nTest statistic\nWe can now calculate the test statistic using the t.test() function in R. Since the variances are unequal, we do not have to specify the var.equal argument – the default test for t.test() is the Welch’s t-test which does not assume equal variances.\n\nt.test(Yield ~ Variety, data = barley)\n\n\n    Welch Two Sample t-test\n\ndata:  Yield by Variety\nt = -4.9994, df = 19.441, p-value = 7.458e-05\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -0.9293569 -0.3814274\nsample estimates:\nmean in group A mean in group B \n       4.052941        4.708333 \n\n\n\n\nP-value\nSince the p-value is &lt; 0.05, we can reject the null hypothesis that the mean yield of both varieties is equal.\n\n\nConclusion\nThe conclusion needs to be brought into the context of the study. In a scientific report or paper, you would write something like this:\n\nThe mean yield of barley variety A was significantly different from that of variety B (\\(t = -5.0\\), \\(df = 19.4\\), \\(p &lt; 0.01\\)).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-2-plant-growth",
    "href": "labs/Lab06.html#exercise-2-plant-growth",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 2: plant growth",
    "text": "Exercise 2: plant growth\n\nBackground\nIn a test of a particular treatment aimed at inducing growth, 20 plants were grouped into ten pairs so that the two members of each pair were as similar as possible. One plant of each pair was chosen randomly and treated; the other was left as a control. The increases in height (in centimetres) of plants over a two-week period are given in the file Two week plant heights. We wish to compare whether the treatment is actually inducing improved growth, as compared to the control.\n\n\nInstructions\nHere, we have two samples, and the samples are paired as it is a before-after experiment. So we’d like to conduct a paired t-test.\nFor paired t-tests the analysis is performed as a 1-sample t-test on the difference between each pair so the only assumption is the normality assumption.\nCopy the structure below and perform your analysis in your document.\n## Exercise 2: plant growth\n### Data exploration\n### Hypothesis\n### Assumptions\n#### Normality\n#### Homogeneity of variance\n### Test statistic\n### P-value\n### Conclusion\nNote that the data is not tidy. The code below will convert the data to the long format and assign it to tidy_plant.\n\nplant_growth &lt;- readr::read_csv(\"data/Plant_growth.csv\")\n\ntidy_plant &lt;- plant_growth %&gt;%\n  pivot_longer(cols = c(Treated, Control), names_to = \"Treatment\", values_to = \"Height\")\n\nYou may also need to perform a Shapiro-Wilk test to check for normality. To do this for each group, you can use the tapply() function.\n\ntapply(tidy_plant$Height, tidy_plant$Treatment, shapiro.test)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-3-turbidity",
    "href": "labs/Lab06.html#exercise-3-turbidity",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 3: turbidity",
    "text": "Exercise 3: turbidity\n\nBackground\nA new filtering process was installed at a dam which provided drinking water for a nearby town. To check on its success, a number of water samples were taken at random times and locations in the weeks before and after the process was installed. The following are the turbidity values (units = NTU) of the water samples.\n\n\nInstructions\nNow we consider further examples of a two-sample t-test, but where the assumption of equal variance and normality may not be met for the raw data. Sometimes after applying a data transformation the analysis can proceed assuming equal variances – but always check after a data transformation.\nThe data can be read with the code below:\n\nturbidity &lt;-read_csv(\"data/Turbidity.csv\")\n\nFor data transformation, you may need to create a new variable in your dataset to store the transformed data. For example, to create a new variable TurbLog10 that stores the log10 transformed turbidity values, you can use the following code:\n\nturbidity$TurbLog10 &lt;- log10(turbidity$Turbidity)\n\nTo interpret the results for your conclusions, you may need to back-transform the mean and/or confidence interval values. To back transform log10 data you use:\n\\[10^{\\text{mean or CI}}\\]\nTo back-transform natural log, loge, you use:\n\\[e^{\\text{mean or CI}}\\]",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-1-tooth-growth",
    "href": "labs/Lab06.html#exercise-1-tooth-growth",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 1: Tooth growth",
    "text": "Exercise 1: Tooth growth\nTooth GRowth is an inbuilt dataset that shows the effect of vitamin c in Guinea pig tooth growth. It has three variables:\n\nlen = tooth length\nsupp = type of supplement (Orange juice or ascorbic acid)\ndose = mg/day gieven to the guinea pigs\n\n\nhead(ToothGrowth)\nstr(ToothGrowth)\n\nUsing the HATPC framework, test whether the type of supplent affects tooth length.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-2-adelie-penguin-bill-length",
    "href": "labs/Lab06.html#exercise-2-adelie-penguin-bill-length",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 2: Adelie penguin bill length",
    "text": "Exercise 2: Adelie penguin bill length\nFor this exercise, we will be using a subset of the palmer penguins dataset.\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nadelie &lt;-penguins%&gt;%\n  filter(species == \"Adelie\")%&gt;%\n  na.omit()%&gt;%\n  droplevels()\n\nhead(adelie)\n\nUsing the HATPC framework, test whether male and female penguins have the same length bill",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab06.html#exercise-3-penguin-body-mass",
    "href": "labs/Lab06.html#exercise-3-penguin-body-mass",
    "title": "Lab 06 – Two-sample t-test",
    "section": "Exercise 3: Penguin body mass",
    "text": "Exercise 3: Penguin body mass\nFor this exercise, we will use a subset of the palmer penguins data set again. This time, we will be comparing two different penguin species.\n\nlibrary(palmerpenguins)\n\npenguins2&lt;- penguins%&gt;%\n  filter(species != \"Adelie\")%&gt;%\n  na.omit()%&gt;%\n  droplevels()\n\nUsing the HATPC framework, test whether chinstrap and gentoo penguins have different body masses.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 06 -- Two-sample *t*-test"
    ]
  },
  {
    "objectID": "labs/Lab08.html",
    "href": "labs/Lab08.html",
    "title": "Lab 08 – 🚫 No exercises",
    "section": "",
    "text": "There are no exercises this week to make room for the evaluation task.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 08 -- 🚫 No exercises"
    ]
  },
  {
    "objectID": "labs/Lab07.html",
    "href": "labs/Lab07.html",
    "title": "Lab 07 – Chi-squared test",
    "section": "",
    "text": "Learning Outcomes\n\n\n\n\nLearn to use R to calculate a chi-squared test for:\n\nTest of proportions\nTest of independence\n\nLearn how to interpret statistical output.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#before-we-begin",
    "href": "labs/Lab07.html#before-we-begin",
    "title": "Lab 07 – Chi-squared test",
    "section": "Before we begin",
    "text": "Before we begin\nCreate your Quarto document and save it as Lab-07.qmd or similar. There are no data files to download for this lab.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#quick-introduction",
    "href": "labs/Lab07.html#quick-introduction",
    "title": "Lab 07 – Chi-squared test",
    "section": "Quick introduction",
    "text": "Quick introduction\nThe chi-square test is used to compare the observed distribution to an expected distribution, in a situation where we have two or more categories in a discrete data. In other words, it compares multiple observed proportions to expected probabilities.\nThe formula is:\n\\[\\chi^2 = \\sum_{i=1}^{k}\\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) is the observed frequency, \\(E_i\\) is the expected frequency for each categorym and \\(k\\) is the number of categories.\nFor more information about the technique, consult your lecture slides and tutorial 7.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-1-wild-tulips-walk-through",
    "href": "labs/Lab07.html#exercise-1-wild-tulips-walk-through",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 1: wild tulips (walk-through)",
    "text": "Exercise 1: wild tulips (walk-through)\n\nBackground\nSuppose we collected wild tulips and found that 81 were red, 50 were yellow and 27 were white. Are these colours equally common?\nIf these colours were equally distributed, the expected proportion would be 1/3 for each of the colour. Therefore, we want to test if the observed proportions are significantly different from the expected proportions.\nThe data is below.\n\ntulip &lt;- c(81, 50, 27)\n\n\n\nInstructions\nUtilise the HATPC process and test the hypothesis that the proportion of flower colours of tulips are equally common, assuming that the samples are independent. We can explore the data as we check the assumptions of the test.\n\n\nHATPC:\n\nHypothesis\nAssumptions\nTest (statistic)\nP-value\nConclusion\n\n\n\n\n\n\n\nLevel of significance\n\n\n\nThe level of significance is usually set at 0.05. This value is generally accepted in the scientific community and is also linked to Type 2 errors, where choosing a lower significance increases the likelihood of failing to reject the null hypothesis when it is false.\n\n\n\n\nHypotheses\nWhat are the null hypothesis and alternative hypotheses?\n\n\nClick here to view answer\n\n\nH0: There is no significant difference between the observed and the expected proportions of flower colours.\n\nH1: There is a significant difference between the observed and the expected proportions of flower colours.\n\n\n\n\n\nAssumptions\nRecall that the assumptions of the \\(\\chi^2\\) test are:\n\nNo cell has expected frequencies less than 1\nNo more than 20% of cells have expected frequencies less than 5\n\n\n\n\n\n\n\nNote\n\n\n\nIn the case that the above assumptions are violated then the probability of a type 1 error occurring (rejecting the null hypothesis when it is true, i.e. false positive) increases.\n\n\nTo calculate expected frequencies, we first calculate the total number of tulips and then divide by the number of categories.\n\nexpected &lt;- rep(sum(tulip) * 1 / 3, 3) #rep function replicated the value we're calculating inside the brackets\nexpected\n\nDoes the data satisfy the assumptions of a \\(\\chi^2\\) test?\n\n\nClick here to view answer\n\nYes, as expected frequencies &gt; 5.\n\n\n\nTest statistic\nThe chisq.test() function in R is used to calculate the chi-squared test.\n\nres &lt;- chisq.test(tulip, p = c(1 / 3, 1 / 3, 1 / 3))\nres\n\nNote that we coud check our assumptions post-analysis by checking the expected frequencies stored in the expected object of the output:\n\nres$expected\n\n\n\nP-value\nWrite down how you should report the critical value, p-value and df in a scientific paper?\n\n\nClick here to view answer\n\n\\(\\chi^2 = 27.9, d.f. =2, p &lt; 0.01\\)\n\n\n\nConclusions\nBased on the p-value, do we accept or reject the null hypothesis?\n\n\nClick here to view answer\n\nWe reject the null hypothesis as the p-value is less than 0.05.\n\nNow write a scientific (biological) conclusion based on the outcome.\n\n\nClick here to view answer\n\nThere is a significant difference in the proportion of flower colours of tulips (\\(\\chi^2 = 27.9, d.f. =2, p &lt; 0.01\\)).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-2-hermit-crabs",
    "href": "labs/Lab07.html#exercise-2-hermit-crabs",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 2: hermit crabs",
    "text": "Exercise 2: hermit crabs\n\nBackground\nIn a study of hermit crab behaviour at Point Lookout, North Stradbroke Island, a random sample of 3 types of gastropod shells was collected. Each shell was then scored as being either occupied by a hermit crab or empty. Do hermit crabs prefer a certain shell?\n\n\n\nShell species\nOccupied\nEmpty\n\n\n\n\nAustrochochlea\n47\n42\n\n\nBembicium\n10\n41\n\n\nCirithid\n125\n49\n\n\n\nThe data is stored in a table object in R below. Note that it is different from a data.frame object. You can verify this by using the str() or class() functions.\n\ncrabs &lt;- as.table( #Make a table with values for each row\n  rbind(\n    Aus = c(47, 42),\n    Bem = c(10, 41),\n    Cir = c(125, 49)\n  )\n)\n\ncolnames(crabs) &lt;- c(\"Occupied\", \"Empty\") #Add column names to table\nstr(crabs)\ncrabs\n\n\nData exploration\nSince we have a multi-dimensional dataset, we can try to plot the data to visualise it.\nA mosaic plot is a graphical representation of the data in a two-way contingency table. It is a way of visualising the relationship between two categorical variables.\nTry the code below. Can you interpret the plot?\n\n# mosaic plot of crabs\nmosaicplot(crabs, main = \"Hermit crabs and shell species\")\n\n\n\nClick here to view interpretation\n\nThe plot shows that the distribution of hermit crabs in the different shell species is not equal. The majority of hermit crabs are found in the Cirithid shell species, followed by Austrochochlea and Bembicium. This can be observed by the width of the boxes in the plot.\nThere are also differences in the number of empty shells in the different shell species. The Bembicium shell species has the highest number of empty shells, followed by Austrochochlea and Cirithid. This can be observed by the height of the boxes in the plot.\n\n\n\n\nHATPC\nNow it is your turn to test the hypothesis that the three shell species are equally preferred by hermit crabs. Follow the HATPC process with the following questions in mind (but you don’t have to answer them individually):\n\nWhat are the null hypothesis and alternative hypotheses?\nDoes the data satisfy the assumptions of a \\(\\chi^2\\) test?\nHow should you report the critical value, p-value and df in a scientific paper?\nBased on the p-value, do we accept or reject the null hypothesis?\nWrite a scientific (biological) conclusion based on the outcome.\n\nUse the markdown structure below to answer the questions (if you wish):\n## Exercise 2: hermit crabs\n### Hypothesis\n### Assumptions\n### Test statistic\n### P-value\n### Conclusions\nTake your time, and when you are ready, check your answers with your demonstrators.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#done",
    "href": "labs/Lab07.html#done",
    "title": "Lab 07 – Chi-squared test",
    "section": "Done!",
    "text": "Done!\nThis is the end of the lab. Remember to save your work, render your document and ask your demonstrators for feedback if you are unsure about your answers.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-1-national-rspca-statistics",
    "href": "labs/Lab07.html#exercise-1-national-rspca-statistics",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 1: National RSPCA statistics",
    "text": "Exercise 1: National RSPCA statistics\nThe RSPCA releases statistics on the number of animals they receive, reclaim and rehome every year. In the 2023-24 financial year, the RSPCA received 17468 dogs, 26704 cats, and 37497 other animals. The “other” category includes horses, small animals, livestock and wildlife.\nUsing the HATPC framework, test whether these animals were received in equal proportions.\n\nreceived &lt;- c(17468, 26704, 37497)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab07.html#exercise-2-uc-berkeley-admissions",
    "href": "labs/Lab07.html#exercise-2-uc-berkeley-admissions",
    "title": "Lab 07 – Chi-squared test",
    "section": "Exercise 2: UC Berkeley Admissions",
    "text": "Exercise 2: UC Berkeley Admissions\nFor the two exercises we’re going to use simplified versions of the inbuilt dataset ‘UCBAdmissions’, which has data on student admissions for Berkeley. The dataset shows how many students were rejected and admitted to the university by both department and gender.\n\n2.1 Admissions by department\nDid every department at UC Berkeley admit students in equal proportions?\n\ndept_admissions &lt;- c(601, 370, 322, 269, 147, 46)\n\n\n\n2.2\nAre male and female students admitted and rejected in the same proportion?\n\ngender &lt;- as.table( #Make a table with values for each row\n  rbind(\n    admitted = c(1198, 557),\n    rejected = c(1493, 1278)\n  )\n)\n\ncolnames(gender) &lt;- c(\"Male\", \"Female\") #Add column names to table",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 07 -- Chi-squared test"
    ]
  },
  {
    "objectID": "labs/Lab02.html",
    "href": "labs/Lab02.html",
    "title": "Lab 02 – Quarto",
    "section": "",
    "text": "This exercise encourages students to discuss academic integrity, and in particular the grey areas often present. Your demonstrator will provide you with a number of scenarios to discuss with each other in smaller groups, and then with the class.\nIf you are interested in more information on Academic Integrity at the University of Sydney, see the following link: Academic Integrity. Also ensure you have completed the Academic Honesty Education Module (AHEM). This must be complete before your first assessment is due (next week for ENVX1002).\n\n\n\n\n\n\nTip\n\n\n\nLearning Outcomes\nAt the end of this practical students should be able to:\n\nUse R to calculate simple summary statistics\nCreate basic plots using default R and ggplot\nDevelop your coding and Quarto skills\nDo basic data wrangling using dplyr\nProduce your own knitted Markdown document",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#walk-through-exercise---ggplot",
    "href": "labs/Lab02.html#walk-through-exercise---ggplot",
    "title": "Lab 02 – Quarto",
    "section": "Walk through Exercise - ggplot",
    "text": "Walk through Exercise - ggplot\nIn this case, the package called ggplot2 to make awesome looking plots, create 1) plot the raw data using a strip chart and 2) a jitter boxplot for the SO4 data set from the tutorial. For each graphical summary you have used, describe what you see and relate that to the data i.e. is it symmetrical, what are the range of values and also comment on the advantages and disadvantages of each plot for describing the SO4 data set?\n\nStrip chart\n\n\n# install.packages(ggplot2)\nlibrary(ggplot2)\np &lt;- ggplot(water, aes(y = SO4, x = \"\")) +\n  geom_jitter(position = position_jitter(0))\np\n\n\nBoxplot with jittered points - the following is how to create a boxplot with the data points jittered\n\n\np &lt;- ggplot(water, aes(y = SO4, x = \"\")) +\n  geom_boxplot() +\n  geom_jitter(position = position_jitter(0.1))\np\n\nCheck out the cheat sheets here https://www.rstudio.com/resources/cheatsheets/ for more on making plots as well as manipulating data in R!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#walk-through-exercise---skewness",
    "href": "labs/Lab02.html#walk-through-exercise---skewness",
    "title": "Lab 02 – Quarto",
    "section": "Walk through Exercise - skewness",
    "text": "Walk through Exercise - skewness\nIn this case, the package called moments contains a function for calculating skew, called skewness. The skewness (\\(g_1\\)) of a data set gives an indication of its symmetry. The sign of the skewness tells us whether the data is positively or negatively skewed. It is useful as one source of evidence for determining whether the data has a symmetrical distribution, particularly when having to assess this for many variables at once. First we must install the package using the install.packages function and the load it using the library function. Note that I have put a comment # in front as I have already installed the package and you only need to install a package once!\n\n# install.packages(\"moments\")\nlibrary(moments)\n\nNow we can calculate the skewness of sulphate.\n\nskewness(water$SO4)",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-1",
    "href": "labs/Lab02.html#exercise-1",
    "title": "Lab 02 – Quarto",
    "section": "Exercise 1",
    "text": "Exercise 1\nLithology\nWhat is the most commonly sampled lithology?\nHint: use the table function to find out the frequency of each lithology. You could also create a bar plot of the lithology data (see land-use example above).",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-2",
    "href": "labs/Lab02.html#exercise-2",
    "title": "Lab 02 – Quarto",
    "section": "Exercise 2",
    "text": "Exercise 2\nFor each of the clay and EC properties (at all depths),\n\nGive the most appropriate estimate of centre.\nIs the data symmetrical?\nAre there any unusual observations? Justify your answers.\n\nHint: use the summary values from above, use the skewness function from the moments package and also use hist and boxplot function to look for unusual observations and skewness.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-3",
    "href": "labs/Lab02.html#exercise-3",
    "title": "Lab 02 – Quarto",
    "section": "Exercise 3",
    "text": "Exercise 3\nIs clay content more variable in the 0-30cm layer or in the 30-60cm layer? For the 0-30cm layer is clay or EC the most variable property. Justify your answers.\nHint: you may calculate the sd and/or var and/or IQR and/or the coefficient of variation (CV) for each property.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-4",
    "href": "labs/Lab02.html#exercise-4",
    "title": "Lab 02 – Quarto",
    "section": "Exercise 4",
    "text": "Exercise 4\nUsing an appropriate measure of centre, which land use has the largest EC and clay content for each depth layer?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab02.html#exercise-5",
    "href": "labs/Lab02.html#exercise-5",
    "title": "Lab 02 – Quarto",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate boxplots of clay 0-30cm and clay 30-60cm for the different lithological classes. Are there any differences between the lithological classes based on the boxplots.\nThis is the end of the R component of the practical. Remember to save your files so you can access in the future.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 02 -- Quarto"
    ]
  },
  {
    "objectID": "labs/Lab11.html",
    "href": "labs/Lab11.html",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "",
    "text": "Learning outcomes\n\n\n\n\nLearn to perform MLR and interpret the results using R;\nUndertake hypothesis testing to determine if model is significant\nUndertake hypothesis testing to determine if the true partial regression slope \\(\\neq\\) 0\nCheck assumptions are filled prior to assessing model output\nAssess model summary in terms of fit and P-values\nConsider more parsimonious models",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#before-you-begin",
    "href": "labs/Lab11.html#before-you-begin",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Before you begin",
    "text": "Before you begin\nCreate your Quarto document and save it as Lab-11.Rmd or similar. The following data files are required:\n\nENVX1002_wk11_practical_data_Regression.xlsx\n\nLast week you explored simple linear regression and assessed the output of your models.\nThis week we will build upon this and venture into multiple linear regression.\nBefore you begin, ensure you have a project set up in your desired folder. Then open up a fresh R markdown and save the file within this folder.\nDon’t forget to save as you go!",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-1-corn-yields",
    "href": "labs/Lab11.html#exercise-1-corn-yields",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 1: Corn yields",
    "text": "Exercise 1: Corn yields\nData: Corn spreadsheet\nIn this data\n\ny = P content of corn\n\nx1 = inorganic P content of soil\n\nx2 = organic P content of soil\n\nn = 17 sites (The original data had 18 sites, one is removed here.)\n\nAim of our investigation: Understand the relationship between Organic and Inorganic phosphorous contents in the soil, and the phosphorous content of corn. This will allow us to see which type of phosphorous is being taken up by the corn.\n\nlibrary(readxl)\nCorn &lt;- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Corn\")\nhead(Corn)\n\n\n\n1.1 Examine the correlations\nSome people find it difficult to visually interpret graphical summaries of data in more than 2 dimensions; however, 3-dimensional surface plots are reasonably common in statistics although not usually in descriptive statistics.\nInitially we will examine the pairwise correlations to “get a feel” for the data. We will then make a 3-dimensional surface plot using the lattice package .\nUsing R, we can calculate the correlation matrix quite easily.\nNote the use of round() to limit the number of significant digits.\n\nround(cor(Corn),3)\n\n\nWhat do the results of the correlation matrix tell you?\n\n\nBased on the correlation matrix, if we were to fit a single predictor model involving EITHER InorgP OR OrgP, then which model would be more successful?\n\n          (Hint, the r2 is exactly that for a single predictor regression, the square of the correlation, r).\nThe pairs plot creates scatterplots between each possible pair of variables. Like a single scatterplot the pairs plot allows us to visually observe any trends.\n\nObserving the pairs plot below, do you see any strong trends? How well does this link to your correlation matrix?\n\n\npairs(Corn)\n\n\n\nSimple 3-D plot\nUnlike a simple plot we can creat for a simple linear regression, it is a bit more complex to visualise a model with more predictors. One way we can visualise the relationship is with a 3-D plot, which can be made using the function levelplot() in lattice.\nHere we plot the OrgP and InorgP in the axes and the levels in the plot are CornP.\nNote the package Viridis has been called, this is through personal choice.\nThe Viridis package has a range of assembled colour ramps which are easier for the reader to differentiate the colours, especially when printed in grayscale, or if the reader is colourblind.\n\nlibrary(lattice, quiet = T)\nlibrary(viridis, quiet = T) # will need to install.packages(\"viridis\")   \n\nlevelplot(CornP ~ InorgP + OrgP, data = Corn\n          , col.regions = viridis(100))\n\nThe level plot shows us the x (InorgP) and y variable (OrgP), with the colour scale representing the z variable, which in this case is Phosphorous being taken up by the corn (cornP). From the plot we can see that with higher levels of orgP and inorgP in the soil, the Phosphorous content in the corn is generally higher.\nIt is clear that the 3-D surface plot does not have colours everywhere, but this relates of course to the underlying data. In this case we don’t have continuous data in both directions, so the response (the colour) is only plotted where we have input variables.\nIf we did have continuous data in both directions the plot would look more like a heatmap, here are some examples.\n\n\n\n\n1.2 Fit the model\nWe will now use regression to estimate the joint effects of both inorganic phosphorus and organic phosphorus on the phosphorus content of corn.\n\\(CornP = \\beta_0 + \\beta_1 InorgP + \\beta_2 OrgP + error\\)\nThis is fairly simple and follows the same structure as simple linear regression and uses lm().\n\nMLR.Corn &lt;- lm(CornP ~ InorgP + OrgP, data=Corn)\n\n\n\n\n1.3 Check assumptions\nLet’s check the assumptions of regression are met via residual diagnostics.\n\nAre there any apparent problems with normality of CornP residuals or equality of variance for this small data set?\n\n\npar(mfrow=c(2,2))\nplot(MLR.Corn)\n\n\n\n\n1.4 Model output\nAfter checking our assumptions and we are happy with them, we can interpret our model output.\n\nIncorporating the partial regression coefficient estimates, what is the model equation?\n\n\nsummary(MLR.Corn)\n\n\nSimple linear regression allowed us to describe the relationship incorporating our regression coefficient estimate. We would interpret it as follows:\n*“As* \\(x\\) increases by 1, \\(y\\) decreases by \\(b_1\\) units” (depending on the direction of the relationship).\nThis week it is a bit different because we are dealing with partial regression coefficients instead.\nInstead, we would say:\n*“as* \\(x_1\\) increases by 1, \\(y\\) decreases by \\(b_1\\) units given all other partial regression coefficients are held constant”.\nApplied to our model, if we wanted to describe the relationship between InorgP and CornP, we would say:\n“As InorgP increases by 1, CornP increases by 1.2902, given OrgP is held constant.”\n\nGiven the above, how would you interpret the relationship between OrgP and CornP?\n\n\n\n\n1.5 Is the model useful?\nRemember now that the F-test and T-test are testing slightly different things.\n\n\nF-test\n\\(H_0:\\) all \\(\\beta_k = 0\\), i.e. \\(\\beta_1 = \\beta_2 = 0\\)\n\\(H_1:\\) at least 1 \\(\\beta_k \\neq 0\\), i.e. our model is significant\nWe will find the P-value for this test at the end of the summary output.\n\n\n\nt-test\n\\(H_0: \\beta_k = 0\\)\n\\(H_1: \\beta_k \\neq 0\\)\nWhere \\(\\beta_k\\) refers to one of the model partial regression coefficients. In the case of our model we only have 2: \\(b_1\\) (InorgP) and \\(b_2\\) (OrgP).\n\nNow it is your turn:\n\nLooking at the summary() output, is our overall model significant?\n\n\nWhich independent variable is a significant predictor of corn yield?\n\n\n\n\n\n1.6 How good is the model?\n\nHow much of the variation in CornP content is explained by the two independent variables?\n\n\nRun the model again but this time with only the significant independent variable. How do the model performance criteria (r2-adj, P-values, Residual Standard Error) change?\n\n\n\n\n1.7 Our conclusions\nWriting up conclusions for multiple linear regression are similar to simple linear regression, just with a couple of extra P-values to state.\nWe would first mention that our overall model is significant as we rejected the null hypothesis (P = 0.05). We could then describe the hypothesis test results for our predictor variables. Finally, we would describe the model fit and our adjusted-r2.\nRemember the scientific conclusion then relates our findings back to the context, answering aims.\n\nWhat would our statistical conclusion be?\n\n\nWhat would our Scientific conclusion be?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-2-water-quality",
    "href": "labs/Lab11.html#exercise-2-water-quality",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 2: Water quality",
    "text": "Exercise 2: Water quality\nData: Microbes spreadsheet\nThis exercise will use data from the NOAA Fisheries data portal. The dataset contains the results of microbial study of Pudget Sound, an estuary in Seattle, U.S.A.\nThe dataset contains the following variables:\n\ntotal_bacteria = Total bacteria (cells per ml) –&gt; this will be our response variable\nwater_temp = Water temperature (°C)\ncarbon_L_h = microbial production (µg carbon per L per hour)\nNO3 = Nitrate (µm)\n\nFirst thing to do, is read in the data. This time we will be using the Microbes spreadsheet:\n\nmic &lt;- read_xlsx(\"data/ENVX1002_practical_wk11_data_Regression.xlsx\", \"Microbes\")\n\n# Check the structure\nstr(mic)\n\n\n\n2.1 Examine the correlations\nFor this dataset we may expect to see some correlations;\n\nWarmer water temperature we would expect to see a higher amount of bacterial growth\nCarbon is a proxy for microbial production, so if we see a higher rate of carbon production, we would expect to see higher levels of bacteria\nNO3 (Nitrate) is an essential nutrient for plants and some bacteria species metabolise this\n\n\nLet’s test this. Observe the correlation matrix and pairs plots. Do you notice any strong correlations?\n\n\ncor(mic)\n\npairs(mic)\n\n\n\n\n2.2 Fit the model\nWe can now fit the model to see how much these predictors account for the variation in total bacteria.\n\\(total bacteria = \\beta_0 + \\beta_1 watertemp + \\beta_2 carbon + \\beta_3 NO3 + error\\)\nThere are two forms the lm code can take; you can either specify which variables you want to include by naming each one, or if only your desired variables are within your dataset, you can use the ~. to specify all columns.\n\nnames(mic) # tells us column names within the dataset\n\n# Form 1:\nmic.lm &lt;- lm(total_bacteria ~ water_temp + carbon_L_h + NO3, data = mic)\n\n# Form 2\nmic.lm &lt;- lm(total_bacteria ~ ., data = mic)\n\n\n\n\n2.3 Check assumptions\nLet’s check the assumptions of regression are met via residual diagnostics.\n\nAre there any apparent problems with normality of total_bacteria residuals or equality of variance for this data set?\n\n\npar(mfrow=c(2,2))\nplot(mic.lm)\n\n\n\n\n2.4 Model output\nAfter investigating the assumptions, they seem to be ok, so we can move onto the model summary.\n\nsummary(mic.lm)\n\n\nIncorporating the partial regression coefficient estimates, what is the equation for this model?\n\n\nLike you did in Exercise 1.4, how would you interpret the relationship between total_bacteria and water_temp?\n\n\n\n\n2.5 Is the model useful?\n\nObserving the P-value of the F-statistic in the summary, can we say our model is significant?\n\n\nAre any predictors significant?\n\n\n\n\n2.6 How good is the model?\n\nHow much of the variation in total bacteria is explained by the three independent variables?\n\n\nRun the model again but this time excluding the variable with the largest P-value. How do the model performance criteria (r2-adj, P-values, Residual Standard Error) change?\n\n\nsummary(lm(total_bacteria ~ water_temp + NO3, data = mic))\n\n\n\n\n2.7 Conclusions\n\nWhat would the statistical conclusion be for this model?\n\n\nWhat would our scientific conclusion be?",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "labs/Lab11.html#exercise-3-dippers",
    "href": "labs/Lab11.html#exercise-3-dippers",
    "title": "Lab 11 - Multiple Linear Regression",
    "section": "Exercise 3: Dippers",
    "text": "Exercise 3: Dippers\nData: Dippers spreadsheet\nWe will revisit the Dippers dataset from last week, but now incorporating other factors which may be influencing the distribution.\nThe file, Breeding density of dippers, gives data from a biological survey which examined the nature of the variables thought to influence the breeding of British dippers.\nDippers are thrush-sized birds living mainly in the upper reaches of rivers, which feed on benthic invertebrates by probing the river beds with their beaks.\nTwenty-two sites were included in the survey. Variables are as follows\n\nwater hardness\nriver-bed slope\nthe numbers of caddis fly larvae\nthe numbers of stonefly larvae\nthe number of breeding pairs of dippers per 10 km of river\n\nIn the analyses, the four invertebrate variables were transformed using a Log(Number+1) transformation.\n\nNow it is your turn to work through the steps as above. What other factors are influencing the number of breeding pairs of Dippers?\n\nRead in the data from today’s Excel sheet, the corresponding sheet name is “Dippers”\n\n\nInvestigate a correlation matrix and pairs plot of the dataset, are there signs of a relationship between breeding pair density and other independent variables?\n\n\npairs(Dippers)\ncor(Dippers)\n\n\nLet’s investigate further. Run the model incorporating all of our predictors, but before looking at our model output, are the assumptions ok?\n\n\n# Run model\ndipper.lm &lt;- lm(Br_Dens ~ ., data=Dippers)\n\n# Check assumptions\npar(mfrow = c(2,2))\nplot(dipper.lm)\n\n\nOnce you are happy assumptions are good, you can interpret the model output using summary().\n\nWhat is the equation for our model, incorporating our partial regression coefficients?\n\n\nBased on the F-statistic output, is the model significant? How can we tell? Is it different to the significance of LogCadd this time?\n\n\nIs LogCadd still a significant predictor of Dipper breeding pair density?\n\n\nWhat are the significant predictors of this model?\n\n\nHow good is the fit of our model?\n\n\nWhat might we do to improve the model fit?\n\n\nWhat statistical and scientific conclusions can we make from this model output?\n\n\nAnother week of linear models done! Great work fitting Multiple Linear Regression! Next week we break away and explore non-linear functions.",
    "crumbs": [
      "**🖥️ Computer labs**",
      "Lab 11 - Multiple Linear Regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVX1002 Handbook",
    "section": "",
    "text": "Welcome to the ENVX1002 lab handbook. This handbook is designed to be an optional companion to the lectures, labs and assessments for the course."
  },
  {
    "objectID": "index.html#how-to-use-this-handbook",
    "href": "index.html#how-to-use-this-handbook",
    "title": "ENVX1002 Handbook",
    "section": "How to use this handbook",
    "text": "How to use this handbook\nRead this handbook before, or after each lecture to better understand certain concepts. Lab exercises are also included in this handbook, which you can use to prepare for the lab sessions.\nTo begin, we strongly suggest that you read our introduction chapter, which will provide you with an overview of why ENVX1002 is important for your degree and includes some introductory statistical concepts.\n\n\n\n\n\n\nAcknowledgements\n\n\n\n\n\nWe would like to acknowledge the work of previous generations of teaching staff who created the bulk of the teaching material within this manual. In particular, the work of:\n\nAssoc. Prof. Mick O’Neill\nDr. Kathryn Aufflick\nAssoc. Prof. Peter Thomson\nProf. Thomas Bishop\n\n\n\n\n\n\n\n\n\n\nLicense\n\n\n\n\n\nThis handbook is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nWhen you are ready, check the sidebar to get started."
  },
  {
    "objectID": "module03/042-linear_functions_multi.html",
    "href": "module03/042-linear_functions_multi.html",
    "title": "Linear functions – multiple predictors",
    "section": "",
    "text": "Linear functions – multiple predictors\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**📘 Module 3**",
      "Linear functions -- multiple predictors"
    ]
  },
  {
    "objectID": "module03/043-nonlinear.html",
    "href": "module03/043-nonlinear.html",
    "title": "Non-linear functions",
    "section": "",
    "text": "Non-linear functions\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**📘 Module 3**",
      "Non-linear functions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html",
    "href": "module01/02-probability_distributions.html",
    "title": "Probability distributions",
    "section": "",
    "text": "Biological systems are variable, and we consequently need statistical methods to cope with this variability in drawing valid inferences from our data. As a consequence of this variability, we can never say anything with absolute certainty based on biological data: there will always be some “doubt” due to chance variation. Not surprisingly, we need the tools and language of probability to assist us with quantifying our level of “doubt” when we make our conclusions.\nWe won’t deal with probability theory to any great degree in this unit of study, but just outline some of the key concepts HSC Maths concepts that we will build on.\n\n\nSimple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\n\n\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\nComplementary Events\nProbability of an event not occurring = 1 – probability of event occurring.\n\\(P(E) =1− P(E)\\)\nExercises\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) – P(A \\text{ and } B)\\)\nExercise\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\nProduct Rule\nWhen we do more than one thing (e.g. toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.\n\n\n\n\nProbability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.\n\n\n\nJust as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g. normal distribution, or discrete e.g. binomial distribution. In this unit of study we will only consider continuous distributions.\n\n\n\nFor discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g. the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don’t mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ≤ Y ≤ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student’s T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these… If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = \"Standard Normal\")\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = \"Log-Normal\")\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) +\n  geom_line() +\n  facet_wrap(~Distribution, scales = \"free_x\") +\n  theme_minimal() +\n  labs(\n    title = \"Log-Normal PDF vs. Normal PDF\",\n    x = \"Value\",\n    y = \"Density\"\n  )\n\n\n\n\nWe began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the “bell-shaped” distribution commonly observed in histograms of biological and environmental data e.g. height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at μ and σ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ’the variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e. \\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g. \\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e. probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e. 50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1 - pnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(–1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; –1) \\approx  0.1587\\) (from R)\n\\(P(–1 &lt; Z &lt; 2) \\approx  0.9772 – 0.1587 \\approx  0.8185\\)\n\npnorm(2)\npnorm(-1)\npnorm(2) - pnorm(-1)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e. \\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n\n\n\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we’ve obtained these probabilities we’ll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‘z’; ‘PDF via formula’; ‘PDF via NORMDIST’.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we’re going to obtain the corresponding probabilities in 2 different ways – you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don’t need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution…\n\n\n\nNormality tests are the first introduction you’ll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe’ll expand these steps soon…\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e. whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"data/TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "**📕 Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#probability",
    "href": "module01/02-probability_distributions.html#probability",
    "title": "Probability distributions",
    "section": "",
    "text": "Simple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\n\n\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\nComplementary Events\nProbability of an event not occurring = 1 – probability of event occurring.\n\\(P(E) =1− P(E)\\)\nExercises\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) – P(A \\text{ and } B)\\)\nExercise\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\nProduct Rule\nWhen we do more than one thing (e.g. toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.",
    "crumbs": [
      "**📕 Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#probability-density-functions",
    "href": "module01/02-probability_distributions.html#probability-density-functions",
    "title": "Probability distributions",
    "section": "",
    "text": "Probability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.",
    "crumbs": [
      "**📕 Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#types-of-distributions",
    "href": "module01/02-probability_distributions.html#types-of-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "Just as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g. normal distribution, or discrete e.g. binomial distribution. In this unit of study we will only consider continuous distributions.",
    "crumbs": [
      "**📕 Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#probability-for-continuous-distributions",
    "href": "module01/02-probability_distributions.html#probability-for-continuous-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "For discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g. the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don’t mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ≤ Y ≤ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student’s T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these… If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = \"Standard Normal\")\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = \"Log-Normal\")\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) +\n  geom_line() +\n  facet_wrap(~Distribution, scales = \"free_x\") +\n  theme_minimal() +\n  labs(\n    title = \"Log-Normal PDF vs. Normal PDF\",\n    x = \"Value\",\n    y = \"Density\"\n  )",
    "crumbs": [
      "**📕 Module 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/02-probability_distributions.html#the-normal-distribution",
    "href": "module01/02-probability_distributions.html#the-normal-distribution",
    "title": "Probability distributions",
    "section": "",
    "text": "We began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the “bell-shaped” distribution commonly observed in histograms of biological and environmental data e.g. height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at μ and σ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ’the variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e. \\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g. \\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e. probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e. 50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1 - pnorm(1.85)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\\(P(–1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; –1) \\approx  0.1587\\) (from R)\n\\(P(–1 &lt; Z &lt; 2) \\approx  0.9772 – 0.1587 \\approx  0.8185\\)\n\npnorm(2)\npnorm(-1)\npnorm(2) - pnorm(-1)\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e. \\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4 * sd, mean + 4 * sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(\n    stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n    xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2\n  ) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n\n\n\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we’ve obtained these probabilities we’ll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‘z’; ‘PDF via formula’; ‘PDF via NORMDIST’.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we’re going to obtain the corresponding probabilities in 2 different ways – you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don’t need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution…\n\n\n\nNormality tests are the first introduction you’ll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe’ll expand these steps soon…\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e. whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"data/TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "**📕 Module 1**",
      "Probability distributions"
    ]
  }
]