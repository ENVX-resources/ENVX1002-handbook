[
  {
    "objectID": "labs/Lab04.html",
    "href": "labs/Lab04.html",
    "title": "Lab 04 - Sampling distributions",
    "section": "",
    "text": "Project 1 is due in week 5 and for many of you this may be your first university assignment; some may be nervous, while others may be more relaxed. Your demonstrators will use the start of this practical to discuss how you may be feeling for this first assessment and share ways you might want to approach and prepare for the assessment.\nThis is the last reflective activity for now, thank you all for contributing so far and we hope you have found some benefit in the activities. Now for some probability!\n\n\n\n\n\n\nLearning outcomes\n\n\n\nAt the end of this computer practical, students should be able to:\n\ncalculate tail, interval and inverse probabilities associated with the Normal distribution\ncalculate probabilities associated sampling distribution of the sample mean by using simulation in R and using R commands.\n\n\n\nLink to data is below:\n\nENVX1002_Data4.xlsx\nAlternatively download from Canvas"
  },
  {
    "objectID": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "href": "labs/Lab04.html#exercise-1---class-activity---how-tall-is-envx1002",
    "title": "Lab 04 - Sampling distributions",
    "section": "Exercise 1 - Class activity - How tall is ENVX1002??",
    "text": "Exercise 1 - Class activity - How tall is ENVX1002??\n\nSet up a new PROJECT for Lab 4 and create a quarto document called Lab_4.qmd and save it in your project directory.\nRecord the height of all male and female students in the class up on the board (we will try to provide a tape if you are unsure about your height)\nEnter the data into r, for example:\n\n\nfemale_height &lt;- c(176, 180, 187, 168)\nmale_height &lt;- c(175, 183, 163, 190)\n\n\nCalculate the mean and standard deviation using R and graph the distribution for both genders, for example\n\n\nf_mean &lt;- mean(female_height)\nf_sd &lt;- sd(female_height)\nhist(female_height) \n\n\n\n\n\n\n\n\n\nDiscuss with your neighbour or post on the zoom chat, which model (distribution function) do you think would fit the data\nHow does your class compare to the Australian statistics. For this we will look at the mean and standard deviation of measured heights for men and women aged 18 - 24 from the ABS for 1995 see page 13 of\n\nhttps://www.ausstats.abs.gov.au/Ausstats/subscriber.nsf/Lookup/CA25687100069892CA256889001F4A36/$File/43590_1995.pdf\nNote that both reported and measured heights are provided and not surprisingly reported heights are bigger that the measured :o)"
  },
  {
    "objectID": "labs/Lab04.html#exercise-2---milkfat-example",
    "href": "labs/Lab04.html#exercise-2---milkfat-example",
    "title": "Lab 04 - Sampling distributions",
    "section": "Exercise 2 - Milkfat example",
    "text": "Exercise 2 - Milkfat example\n\nPart 1\nThe milkfat content in milk (in %) for 120 cows are presented in the worksheet called ENVX1002_Data4.xlsx. Copy the file into your project directory and:\n\nImport the data into R.\n\n\nlibrary(readxl)\nmilkfat &lt;- read_excel(\"data/ENVX1002_Data4.xlsx\", sheet = \"Milkfat\")\n\n\nCalculate the summary statistics of Milkfat (mean, median and sd)\n\nNote that we use $ColumnName to select a column from the data\n\nmean(milkfat$Milkfat)\n\n[1] 4.166083\n\n\n\nWhat type of cows could they be? Compare your data to the table in the following link:\n\nhttps://lactalis.com.au/info-center/different-breeds-of-cows/\n\nWhat state could they be from? Check some of the recent Milk Production reports from Dairy Australia. The data can be found in the Average Milkfat & Protein (%) section of the PDF report: The reports can be found at the following link:\n\nhttps://www.dairyaustralia.com.au/resource-repository/2020/09/25/milk-production-report\n\nCould the data be normally distributed?\n\n\nCreate a histogram and boxplot of the milk fat data. Is the data Normally distributed?\n\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nggplot(milkfat, aes(x=Milkfat)) +  \n  geom_histogram(binwidth = 0.1, fill=\"lightblue\", color=\"black\") +\n  xlab(\"Milkfat (%)\") \n\n\n\n\n\n\n\n\n\nIn the UK, breakfast milk' (orChannel Island milk’) has 5.5% fat content. What percentage of the cows in this data set is yielding breakfast milk with \\(\\ge\\) 5.5%?\n\n\ns=sort(milkfat$Milkfat) # Sorts the data\ns # Look at the sorted data\n\n  [1] 3.47 3.56 3.58 3.66 3.66 3.70 3.70 3.72 3.74 3.74 3.77 3.81 3.82 3.83 3.86\n [16] 3.86 3.87 3.88 3.89 3.89 3.89 3.89 3.91 3.91 3.92 3.93 3.94 3.95 3.96 3.96\n [31] 3.97 3.97 3.97 3.97 3.97 3.98 3.99 3.99 4.00 4.00 4.00 4.02 4.03 4.05 4.05\n [46] 4.05 4.06 4.06 4.07 4.08 4.09 4.09 4.09 4.09 4.10 4.10 4.10 4.11 4.12 4.14\n [61] 4.15 4.16 4.16 4.17 4.17 4.18 4.20 4.20 4.20 4.20 4.22 4.23 4.24 4.24 4.24\n [76] 4.24 4.25 4.27 4.28 4.28 4.29 4.29 4.30 4.31 4.32 4.32 4.33 4.33 4.33 4.34\n [91] 4.35 4.36 4.38 4.38 4.38 4.38 4.38 4.40 4.41 4.42 4.42 4.46 4.48 4.49 4.51\n[106] 4.52 4.58 4.58 4.60 4.60 4.66 4.67 4.67 4.70 4.71 4.81 4.82 4.88 4.91 5.00\n\nlength(s[s&gt;=5.5]) # Counts how many are &gt;= 5.5\n\n[1] 0\n\n\n\nIn Australia, full cream milk has greater than 3.2% milk fat content. What percentage of these cows is yielding full cream milk?\n\n\n## Your turn\n\n\n\nPart 2\nLet \\(X\\) represent the milk fat content for the population of this breed of cows.\n\nAssuming the population is normal, use the sample mean and standard deviation from the previous question as estimates of the population parameters. So \\(X\\sim (\\mu =..., \\sigma^2 = ...)\\).\nDraw a picture of the curve representing \\(X\\). The below example uses ggplot2 to draw the curve for \\(N(4.16,0.30^2)\\).\n\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(4.16-4*0.3, 4.16+4*0.3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.16, sd=0.30)) +\n  xlab(\"x\") +\n  ylab(expression(N(4.16,0.30^2)~pdf))\n\n\n\n\n\n\n\n\n\nWhat is the probability that 1 cow has a fat content less than 4%? We will adapt the ggplot command above a picture of this probability and then use R to find the probability.\n\nHint: You may need to use the stat_function command to draw the curve and then use the pnorm command to find the probability.\n\nggplot(data.frame(x = c(4.16-4*0.3, 4.16+4*0.3)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 4.16, sd=0.30)\n                ,geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 4.16, sd=0.30)\n    , xlim = c(4.16-4*0.3, 4), geom = \"area\", fill = \"red\") +\n  xlab(\"x\") +\n  ylab(expression(N(4.16,0.30^2)~pdf))\n\n\n\n\n\n\n\n\n\npnorm(4,4.16,0.30)\n\n[1] 0.2969014\n\n\n\nWhat is the probability that 1 cow (randomly sampled) has a fat content greater than 4.5%? Try and adapt the ggplots above to draw a picture of this probability and then use R to find the probability.\nFor a sample of 10 cows (randomly sampled), what is the probability that the sample mean milk fat content is greater than 4.2%?\n\nHint: First find the distribution of the sample mean \\(\\overline{X}\\). Then find \\(P(\\overline{X}&gt;4.2)\\)"
  },
  {
    "objectID": "labs/Lab04.html#exercise-3---skin-cancer",
    "href": "labs/Lab04.html#exercise-3---skin-cancer",
    "title": "Lab 04 - Sampling distributions",
    "section": "Exercise 3 - Skin cancer",
    "text": "Exercise 3 - Skin cancer\nA dermatologist investigating a certain type of skin cancer induced the cancer in nine rats and then treated them with a new experimental drug. For each rat she recorded the number of hours until remission of the cancer. The rats had a mean remission time of 400 hours and a standard deviation of 30 hours. From this data, calculate the standard error of the mean."
  },
  {
    "objectID": "labs/Lab04.html#exercise-4---soil-carbon",
    "href": "labs/Lab04.html#exercise-4---soil-carbon",
    "title": "Lab 04 - Sampling distributions",
    "section": "Exercise 4 - Soil carbon",
    "text": "Exercise 4 - Soil carbon\nAn initial soil carbon survey of a farm based on 12 observations found that the sample mean \\(\\overline{X}\\) was 1.2% and the standard deviation s was 0.4%. How many observations would be needed to estimate the mean carbon value with a standard error of 0.1%?"
  },
  {
    "objectID": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "href": "labs/Lab04.html#exercise-5---whats-in-the-media---looming-state-election",
    "title": "Lab 04 - Sampling distributions",
    "section": "Exercise 5 - What’s in the media - looming state election",
    "text": "Exercise 5 - What’s in the media - looming state election\nAn article was published in the Sydney Morning Herald on Saturday 20.3.2010 about statistics related to opinion polls. Read it and find the sentences related to (i) populations versus samples (ii) standard error formula (iv) the effect of sample size on standard errors.\nhttp://www.smh.com.au/national/demystifying-the-dark-art-of-polling-20100319-qmai.html"
  },
  {
    "objectID": "labs/Lab04.html#exercise-6---extra-practice",
    "href": "labs/Lab04.html#exercise-6---extra-practice",
    "title": "Lab 04 - Sampling distributions",
    "section": "Exercise 6 - Extra practice",
    "text": "Exercise 6 - Extra practice\nThe average Australian woman has height (in cms) of 161.8 with a standard deviation of 6.\n\nThe Australian Institute of Sport ran a netball training camp for the best Australian young players. How tall were the goal position players? http://www.abc.net.au/news/2015-06-14/tall-athletes-get-support-at-ais-to-stand-as-proud-netballers/6544642\nWhat is the probability of finding an Australian woman of this height or taller?\n\nHints:\nStep 1: Using ggplot, draw a sketch of the Normal curve with the probability identified. You may need to draw a section of the right tail as the probability is small! We have provided the solution for the plotting to assist you.\nStep 2: Calculate the probability in R.\n\n1-pnorm(189, 161.8, 6)\n\n[1] 2.903004e-06\n\nggplot() +\n  stat_function(fun = dnorm, args = list(mean = 161.8, sd=6), \n                geom = \"area\", fill = \"white\", xlim = c(180, 161.8+4*6)) +\n  stat_function(fun = dnorm, args = list(mean = 161.8, sd=6), \n                geom = \"area\", fill = \"red\", xlim = c(161.8+4*6, 189)) +\n  xlab(\"x\") +\n  ylab(expression(N(161.8,6^2)~pdf)) +\n  scale_x_continuous(breaks = 189)\n\n\n\n\n\n\n\n\n\nDharshani Sivalingam is the tallest netball player in the world. How tall is Dharshani? https://en.wikipedia.org/wiki/Tharjini_Sivalingam What is the probability of finding an Australian woman of Dharshani’s height?\nMadison Brown is one of the the shortest Australian International players. How tall is Madision? https://en.wikipedia.org/wiki/Madison_Browne What percentage of Australian women are between Madison and Dharshani’s heights?\nIf 80% of Australian women are above a certain height, what is that height?"
  },
  {
    "objectID": "labs/Lab01.html",
    "href": "labs/Lab01.html",
    "title": "Lab 01 - Introduction",
    "section": "",
    "text": "Tip\n\n\n\nLearning Outcomes\nAt the end of this practical students should be able to:\n\nuse Microsoft Word for writing equations\nuse Excel and RStudio to calculate simple summary statistics\nUnderstand the link between R and RStudio\nProduce your own rendered Quarto document\n\n\n\n\n\nMake sure you have access to:\n\nMicrosoft Word and Excel\nR and RStudio\nThe data set and code file for this computer lab:\n\nGithub: download Lead_content.csv file\nCanvas: download Lead_content.csv file\nCanvas: download Code_Topic1.R file\n\n\n\n\n\nAt the beginning of the next few weeks we will be doing some short activities before getting into the stats to help you foster a sense of belonging, learn more about your peers, and help better prepare you for your studies. This week we will start with a simple introduction, but before we do this, we would like to acknowledge those who were here before us:\n\nWe would like to acknowledge and pay respect to the traditional owners of the land on which we meet; the Gadigal people of the Eora Nation. It is upon their ancestral lands that the University of Sydney is built. As we share our own knowledge, teaching, learning and research practices within this university may we also pay respect to the knowledge embedded forever within the Aboriginal Custodianship of Country.\n\nTo learn more about why we do Acknowledgement of Country, and the difference to Welcome to Country, see the following page: Welcome and Acknowledgement.\n\n\n\nWe are all from diverse backgrounds and have followed different paths to get to where we are today. To help you get to know your peers, your demonstrator will lead a class discussion, posting a number of questions on AnswerGarden, where you can then anonymously post your answer to the questions. Links will be provided once your demonstrator has set up the question.\nAfter about 20 minutes of discussion, we can get started on the Stats! Welcome to ENVX1002!"
  },
  {
    "objectID": "labs/Lab01.html#before-you-begin",
    "href": "labs/Lab01.html#before-you-begin",
    "title": "Lab 01 - Introduction",
    "section": "",
    "text": "Make sure you have access to:\n\nMicrosoft Word and Excel\nR and RStudio\nThe data set and code file for this computer lab:\n\nGithub: download Lead_content.csv file\nCanvas: download Lead_content.csv file\nCanvas: download Code_Topic1.R file"
  },
  {
    "objectID": "labs/Lab01.html#settling-in",
    "href": "labs/Lab01.html#settling-in",
    "title": "Lab 01 - Introduction",
    "section": "",
    "text": "At the beginning of the next few weeks we will be doing some short activities before getting into the stats to help you foster a sense of belonging, learn more about your peers, and help better prepare you for your studies. This week we will start with a simple introduction, but before we do this, we would like to acknowledge those who were here before us:\n\nWe would like to acknowledge and pay respect to the traditional owners of the land on which we meet; the Gadigal people of the Eora Nation. It is upon their ancestral lands that the University of Sydney is built. As we share our own knowledge, teaching, learning and research practices within this university may we also pay respect to the knowledge embedded forever within the Aboriginal Custodianship of Country.\n\nTo learn more about why we do Acknowledgement of Country, and the difference to Welcome to Country, see the following page: Welcome and Acknowledgement."
  },
  {
    "objectID": "labs/Lab01.html#answergardens",
    "href": "labs/Lab01.html#answergardens",
    "title": "Lab 01 - Introduction",
    "section": "",
    "text": "We are all from diverse backgrounds and have followed different paths to get to where we are today. To help you get to know your peers, your demonstrator will lead a class discussion, posting a number of questions on AnswerGarden, where you can then anonymously post your answer to the questions. Links will be provided once your demonstrator has set up the question.\nAfter about 20 minutes of discussion, we can get started on the Stats! Welcome to ENVX1002!"
  },
  {
    "objectID": "labs/Lab01.html#excel-worksheets-and-cells",
    "href": "labs/Lab01.html#excel-worksheets-and-cells",
    "title": "Lab 01 - Introduction",
    "section": "Excel worksheets and cells",
    "text": "Excel worksheets and cells\nExcel files come in series of worksheets where data is stored in cells. The columns are given letters and the rows are given numbers, enabling a particular cell to be referenced by a combination a letter and number. In the screenshot below the number 2 in the orange cell could be referenced by B3. In a blank worksheet type 2 in the B3 cell.\nIn cell C3 type =B3\nThe equals sign tells Excel you are calculating something or referring to a cell. You should now have 2 in cell C3.\n\n\n\nScreenshot of Excel Typing number 2 in cell B3\n\n\nAt the bottom of the Excel page you will see references to each of the worksheets in the file, for example ‘Sheet1’, ‘Sheet2’, ‘Sheet3’. This enables you to store multiple data sets in the one file. In this unit the data sets for each exercise will be stored in separate worksheets but in the same file."
  },
  {
    "objectID": "labs/Lab01.html#basic-arithmetic-in-excel",
    "href": "labs/Lab01.html#basic-arithmetic-in-excel",
    "title": "Lab 01 - Introduction",
    "section": "Basic arithmetic in Excel",
    "text": "Basic arithmetic in Excel\nWhen typing equations, make sure you start by typing = . This tells Excel you want to solve the input equation.\nThe basic arithmetic operators return numeric values:\n\n\n\nKey\nOperation\n\n\n\n\n+\nAddition\n\n\n-\nsubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponentiation\n\n\n\nThese can be used in combination with numbers or cell references.\nFor example, to get a value of 4 in cell D3 you can type either\n=2*2 (type numbers)\nor\n=B3*C3 (reference cells)\nIt is better to reference cells so that if you change the values the same equations can be applied."
  },
  {
    "objectID": "labs/Lab01.html#basic-functions-in-excel",
    "href": "labs/Lab01.html#basic-functions-in-excel",
    "title": "Lab 01 - Introduction",
    "section": "Basic functions in Excel",
    "text": "Basic functions in Excel\nSome basic functions are:\n\n\n\nFunction\nOperation\n\n\n\n\nSUM\nSums a range of cells\n\n\nCOUNT\nCounts a range of cells\n\n\nLN\nNatural Log\n\n\nEXP\nExponent\n\n\n\nThese can be used in combination with numbers or cell references.\nFor example in cell E3 you can type either\n=EXP(4) or =EXP(D3),\nAnother example is in cell F3 type\n=COUNT(B3:D3)\nNote that Excel has an auto-complete function that allows you to select from a list of functions after typing the first letter i.e. =C. Selecting the function gives a brief description of what the function does.\n\n\n\nScreenshot of Excel Typing =C to show function description\n\n\nOnce the function has been selected, you can proceed to type the opening bracket and enter in the cell reference, cell range or numeric value. Excel aids you in showing what the required input is as you type the opening bracket (see image below). The square bracket indicates an optional value, in this case if only one cell is selected =COUNT(E3), then the function will return the value 1.\n\n\n\nScreenshot of Excel showing required input arguments for COUNT function"
  },
  {
    "objectID": "labs/Lab01.html#simple-summary-statistics-in-excel",
    "href": "labs/Lab01.html#simple-summary-statistics-in-excel",
    "title": "Lab 01 - Introduction",
    "section": "Simple summary statistics in Excel",
    "text": "Simple summary statistics in Excel\nThere are functions for calculating summary statistics in Excel. Click on a cell where you want the answer to be entered and then use the menu by Formulas &gt;&gt; Insert Function. A screenshot for calculating the sum is shown below.\n\n\n\nScreenshot of Excel Insert Function\n\n\nOn the next screen you can then select the cells where the observations are located from which then median will be calculated.\nAfter a while you should get to know the name of the functions in Excel and be able to write the arguments in manually. In the screenshot below the function is MEDIAN and it refers to cells between (and including) A2 and A9. A row or column of cells can be represented by the starting cell, then colon, then final cell (A2:A9).\n\n\n\nScreenshot of Excel calculating the Median of cells A2 to A9\n\n\nSome of examples of the functions that can be accessed in Excel are shown below. Note that the .S and .P extensions for variance and standard deviation are from later excel versions.\n\n\n\nStatistic\nFunction\n\n\n\n\nMinimum\n=MIN\n\n\nMaximum\n=MAX\n\n\nArithmetic mean\n=AVERAGE( )\n\n\nMedian\n=MEDIAN( )\n\n\npopulation variance\n=VAR.P()\n\n\nSample variance\n=VAR( ) or =VAR.S()\n\n\nPopulation standard deviation\n=STDEV.P()"
  },
  {
    "objectID": "labs/Lab01.html#calculating-simple-summary-statistics-in-excel",
    "href": "labs/Lab01.html#calculating-simple-summary-statistics-in-excel",
    "title": "Lab 01 - Introduction",
    "section": "Calculating simple summary statistics in Excel",
    "text": "Calculating simple summary statistics in Excel\nIn this exercise you will use the Lead_content.csv See start of lesson for link to download the file.\nThis data was collected from a recreational parkland in Sydney and is a measurement of the lead concentration (mg/kg) detected in the soil, measured through chemical analysis (ICP-OES). There are a total of 60 samples collected from around the park. The park was originally a municipal landfill but remediated in 1990, so we expected to find low levels of lead. The guide value set by the Australian Government is 300 mg/kg and this is where further investigation is needed (potential to cause harm).\nIn excel, calculate the following:\n\nminimum value\nMaximum value\nmean\nmedian\nrange\nsample variance\nsample standard deviation\n\nFrom these statistics,\n\nWere there any samples higher than the guide value?\nWere there any samples where no lead was detected?\nWhat is the mean value?"
  },
  {
    "objectID": "labs/Lab01.html#getting-a-copy-of-r-rsudio",
    "href": "labs/Lab01.html#getting-a-copy-of-r-rsudio",
    "title": "Lab 01 - Introduction",
    "section": "Getting a copy of R & RSudio",
    "text": "Getting a copy of R & RSudio\nRemember that:\nR = Engine and RStudio = Interface\nBoth are free & opensource and downloadable from https://posit.co/download/rstudio-desktop/.\nMake sure you have installed both - it is best to have the latest version of R and RStudio. If you have not done this yet, please do so now."
  },
  {
    "objectID": "labs/Lab01.html#r-basics",
    "href": "labs/Lab01.html#r-basics",
    "title": "Lab 01 - Introduction",
    "section": "R basics",
    "text": "R basics\n\nTo begin with we are going to open R (“The Engine”) from the program files menu on your computer.\nWhen you open R, a window containing the R console will open. It will look slightly different depending on the operating system you use. The screenshot is from a Mac.\n\n\n\n\nThe R Console\n\n\n\nAt the bottom of the screen is the command prompt &gt;. Commands are typed at the command prompt and followed by the ENTER key.\nFrom now type all the command you see into R. If you type in an expression, when you hit the ENTER key, the expression will be evaluated and the result returned, for example, lets add 5 and 5 together.\n\n\n5+5\n\n[1] 10\n\n\n\nR is an object-oriented programming language and the basic unit in R is called an object. Objects can store single numbers, columns of data, modelling output, functions and other kinds of information.\n\nThe class of the object determines the way in which commands are executed on an object and the way in which data can be stored by the object. For example, vectors store a single column of data, a matrix can store multiple columns of data and a data frame can store multiple columns of data where the columns may be of different data types (e.g. numbers and text).\nWe can also save our result above into a named object using the assignment operator &lt;- or =. I like the arrow because it points in the direction of the object being created. For example, the following command saves the value 5+5 as an object called myData.\n\n\nmyData &lt;- 5+5\n\n\nYou can view the contents of an object by typing the object name:\n\n\nmyData\n\n\nWhen you hit the RETURN Key, you will see the following output:\n\n\n\n[1] 10\n\n\n\nObject names can be made up of letters, numbers and , and _ symbols. A name must start with . or a letter. If it starts with . the second character must not be a number.\nR is case sensitive, so calling mydata is not the same as myData and will generate an error:\n\n\nmydata\n\nError in eval(expr, envir, enclos): object 'mydata' not found\n\n\n\nTo see a list of all the named objects you’ve created in R, use the objects function:\n\n\nobjects()\n\n[1] \"myData\"          \"pandoc_dir\"      \"quarto_bin_path\"\n\n\n\nTo delete an object, use the remove function:\n\n\nremove(myData)\n\n\nIf you type and enter an incomplete command, a continuation prompt will appear on the next line: +. You can continue typing the command followed by the ENTER Key.\n\n\nmyData3 &lt;-\n+ 8\n\n\nTo cancel a command at the continuation prompt (or during execution of a command), press the ESC key.\nThe up and down arrow keys can be used to scroll through previous commands.\nComments can be indicated by a hash mark (#) - everything on the line following the hash mark will be ignored by R. This can be after R code on a line or on a separate line as shown below.\n\n\n#I am adding 6+6 and saving it to an object called my.Data\nmy.Data &lt;- 6+6"
  },
  {
    "objectID": "labs/Lab01.html#basic-arithmetic-in-r",
    "href": "labs/Lab01.html#basic-arithmetic-in-r",
    "title": "Lab 01 - Introduction",
    "section": "Basic arithmetic in R",
    "text": "Basic arithmetic in R\n\nThe symbols for basic arithmetic operators are shown in the table below.\n\n\n\n\n\n\nOperators\nOperation\n\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^ or **\nexponentiation\n\n\n\n\n\n\nParentheses ( ) can be used to specify order of operations.\nYou can perform basic calculations by typing expressions into the command line.\n\n\n(5*10) ^2\n\n[1] 2500\n\n\n\nBetter still you can assign results to a named object to be used at a later date.\n\n\nmyresult &lt;- 20/10 + 6 - 1\n\n\nFor example, We can then halve the value of myresult.\n\n\nmyresult &lt;- myresult/2"
  },
  {
    "objectID": "labs/Lab01.html#text-editors-in-r",
    "href": "labs/Lab01.html#text-editors-in-r",
    "title": "Lab 01 - Introduction",
    "section": "Text editors in R",
    "text": "Text editors in R\n\nUntil now you have copied and pasted commands which you may wish to use again. This is particularly important as you begin to write series of commands to perform a certain task. One option is to save these commands to text files and copy the relevant commands into R as needed. You can save logical groupings of commands into different text files.\nA better option is to use text editors – one example is RStudio and unlike notepad it allows syntax highlighting of R commands. When an R session is open, RStudio includes an additional menu and toolbar and it allows the user to interact with R by submitting code in whole or in part.\nFrom now on you should start to use RStudio by copying the commands into a R file and then submitting them to R. By doing this you will have a record of the commands you have used. From now you will be using R through RStudio and not the console directly. Over time you will develop a library of code to perform analyses and create graphics.\nThe screenshot below shows RStudio, the top left window shows your code and the bottom left window shows the input and output in R. The top right hand window side shows the objects you have created, for example myData. The bottom left hand corner shows graphics, in this case a histogram, but can also show other useful features such as the help menu.\n\n\n\n\nR Studio"
  },
  {
    "objectID": "labs/Lab01.html#simple-data-analysis-in-rstudio",
    "href": "labs/Lab01.html#simple-data-analysis-in-rstudio",
    "title": "Lab 01 - Introduction",
    "section": "Simple data analysis in RStudio",
    "text": "Simple data analysis in RStudio\n\nDownload and open the file Code_Topic1.r see link at start of lesson. Once open using the File &gt; Open menu items in RStudio, you will see most of the commands you have been typing in the code editor. Rather than typing commands directly into into R we will now start to use RStudio code editor.\n\nYou send code from your open R code file which the top left hand pane of to the R console which is found in the bottom left hand pane. The output, e.g. the mean, will also appear in the bottom left hand window.\n\nTo send a line of code to R from your R file (the Code_Topic1.R file), click anywhere in the line and click on the Run icon or use the short cut CTRL+ENTER (Windows) or COMMAND+ENTER (Mac) or click on Run to run all code at once. The output is shown in the bottom left hand window.\n\nYou can also use the # symbol to write comments which R will ignore. It is really important to comment throughout your code to help you and others who may use it to understand what the code does. It is recommended you copy the output into your R file and comment it out using # so you have a complete record of your work."
  },
  {
    "objectID": "labs/Lab01.html#getting-data-into-rstudio",
    "href": "labs/Lab01.html#getting-data-into-rstudio",
    "title": "Lab 01 - Introduction",
    "section": "Getting data into RStudio",
    "text": "Getting data into RStudio\n\nRStudio can accept data from many different sources; for example directly from scientific instruments or even scraping the internet for data. In this topic we are only considering small data sets so we will enter the data manually via the keyboard.\nA vector (or list) of numbers can be manually entered using the assignment operator and the c function which essentially means combine, an example is below.\n\n\nmyDataset &lt;- c(5,12,52,32,14,6.1)\n\n\nNow it is your turn. Similar to above, use the c function to enter a soil carbon data set (48, 56, 90, 78, 86, 71, 42) as an object called Carbon. We will then calculate some basic statistics on this data set.\n\n\nCarbon&lt;-c(48, 56, 90, 78, 86, 71, 42)"
  },
  {
    "objectID": "labs/Lab01.html#summary-statistics-in-rstudio",
    "href": "labs/Lab01.html#summary-statistics-in-rstudio",
    "title": "Lab 01 - Introduction",
    "section": "Summary statistics in RStudio",
    "text": "Summary statistics in RStudio\n\nNow we have entered the data in R we want to do something with it, such as calculate summary statistics.\nR functions behave differently depending on the data type.\nSome functions will work only on specific data types, other functions will use different methods on different data types.\n\nTo find the mean of data set we use the mean function.\n\nmean(Carbon)\n\n[1] 67.28571\n\n\nOther commands related to summary statistics include:\n\nmedian - median\nvar - sample variance\nsd - sample standard deviation\nmin - minimum value\nmax - maximum value\nlength - number of observations (length of the vector)\n\nCalculate all of the statistics above using R.\n\nmedian(Carbon)\n\n[1] 71\n\nvar(Carbon)\n\n[1] 355.5714\n\nsd(Carbon)\n\n[1] 18.8566\n\nmin(Carbon)\n\n[1] 42\n\nmax(Carbon)\n\n[1] 90\n\nlength(Carbon)\n\n[1] 7\n\n\n\nRather than using all of these indiviudally you can use the summary function which gives the minimum, maximum, mean and median values. We will consider the 1st Qu. and 2nd Qu. in the next practical.\n\n\nsummary(Carbon)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  42.00   52.00   71.00   67.29   82.00   90.00 \n\n\n\nNote, that it does not calculate the standard deviation, variance or number of observations.\nThat is all we will do in R in this practical. Remember to save your code so you can refer and reuse this for your online quiz and other assessments in the future."
  },
  {
    "objectID": "labs/Lab01.html#projects",
    "href": "labs/Lab01.html#projects",
    "title": "Lab 01 - Introduction",
    "section": "Projects",
    "text": "Projects\nHaving all your files organised into a logical folder structure will be super beneficial in this course and also your day-to-day life. Also Super important is that you back up your work in the cloud. You can do this using One Drive which you can access as through your university account, you can also use your own cloud storage (Dropbox, iCloud,…) and or you can use your GitHub account.\nYour tutors can assist you to get your files organised. The following is my recommendation:\n\nSet up a course folder called ENVX1002 on your desktop/network drive/USB/cloud storage.\nSet up a project folder called Lab_1 in your ENVX1002 folder.\n\n\nCreate a folder called ENVX1002 on your laptop or the class desktop (prefereably in a folder backed up to the cloud. If you are using a class computer you will need to save the file to upload to your cloud storage or USB/External Hard Drive or zip the folder and email to yourself at the end of the class.\nOpen R Studio go to the file drop down menu and select New Project\n\n\n\n\nNew project\n\n\n\nSelect New directory and navigate to your class folder.\n\n\n\n\nNew project\n\n\n\n\nEnter on the directory name, for example Lab_1, and click on Create Project.\nWell done! you have now set up a project.\n\n 6. You should now see a folder called Lab_1 in your ENVX1002 folder. If you open that folder you will see a file called Lab_1.Rproj. This is your project file and it will open RStudio with the working directory set to the Lab_1 folder."
  },
  {
    "objectID": "labs/Lab01.html#coding-in-quarto",
    "href": "labs/Lab01.html#coding-in-quarto",
    "title": "Lab 01 - Introduction",
    "section": "Coding in Quarto",
    "text": "Coding in Quarto\nYour tutor will assist you to open a Quarto (.qmd) file. You will do this for each Lab and your also for your reports:\n\nOpen a qmd file, and save it as, for example, Prac1.qmd.\nRun the file using render.\nView filename.html in a browser.\nNow experiment with editing the file (both text and code chunks) and then re-run using render.\nUse this file to store your summary of today’s lab work.\n\nThe screenshots will help you do this.\n\nNavigate to New File &gt; Quarto Document\nEnter in an appropriate name for your file such as ENVX1002_lab_1 and enter in your name. Select HTML format.\n\n\n\n\nNew Quarto\n\n\n\nWell done! you have created a new Quarto file. First save your file by navigating to the File menu i.e. File &gt; Save as. Name your file ENVX1002_lab_1.qmd it should automatically save in your Project folder.\nFinally you can Render the file by selecting the render button - see if you can spot it - it’s a blue block arrow pointing to the right above the text editor.\nGo to your project folder and open the HTML file by double clicking on it, it will open in your default browser."
  },
  {
    "objectID": "labs/Lab01.html#summing-up",
    "href": "labs/Lab01.html#summing-up",
    "title": "Lab 01 - Introduction",
    "section": "Summing up",
    "text": "Summing up\nWell done!\nYou now know how to:\n\nwrite equations in a word document\ndo basic operations in Excel\ndo basic operations in R and RStudio and\nconsider how to manage files and folders and\nset up a project as well as generate an Quarto Document\n\nTo do by next week:\n\ncomplete anything you have missed from today’s lab\ncomplete the practice O-quiz - this is to help you get familiar with Canvas quizzes."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "A practical definition of ENVX is “The use of statistical and computing methods to answer quantitative biological questions.”\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.\n\n\n\nENVX is at the interface of applied statistics, data science and life and environmental science"
  },
  {
    "objectID": "introduction.html#what-is-envx",
    "href": "introduction.html#what-is-envx",
    "title": "Introduction",
    "section": "",
    "text": "A practical definition of ENVX is “The use of statistical and computing methods to answer quantitative biological questions.”\nIt sits at the interface of applied statistics, data science and life and environmental sciences. It is a field that is growing rapidly in importance as the amount of data being collected in the biological and environmental sciences increases.\n\n\n\nENVX is at the interface of applied statistics, data science and life and environmental science"
  },
  {
    "objectID": "introduction.html#introduction-to-research",
    "href": "introduction.html#introduction-to-research",
    "title": "Introduction",
    "section": "Introduction to Research",
    "text": "Introduction to Research\nYou will soon learn that the University of Sydney has a great tradition in research (with many of our efforts earning international recognition). Your teachers will include some experiences of or findings from their research in their classes, and you will have an opportunity to participate in research projects throughout your degree. ENVX comprises of three vertically integrated units designed to equip you to confidently approach the design and data analysis aspects of your research. The diagram below shows one view of the process of research (or experimentation).\n\n\n\nThe research process\n\n\n\nThe Use of Statistics in Research\nStatistics provides us with an avenue for exploring and reporting the findings of the research. In terms of reporting, it allows us to give a measure (generally a probability) of the extent to which our conclusion could be wrong. Statistics is a means of informing the decision-making process."
  },
  {
    "objectID": "introduction.html#data",
    "href": "introduction.html#data",
    "title": "Introduction",
    "section": "Data",
    "text": "Data\nData is the information we collect on the subjects, factors and variables we are interested in studying. Data may be textual (i.e. words) or numerical.\n\nObservations, samples and populations\nPopulations may be real and therefore able to be listed e.g. all veterinarian practices in the Sydney region. They may be hypothetical and unable to all be listed e.g. all dogs possible that could have a particular treatment applied to them.\nEach sampling unit (or observation) in the population (e.g. plot of land, animal, etc.) has a value y (e.g. nematode count, gestational length): \\(y_1,y_2,y_3,...y_N\\). Typically the population size (N) is very large - even infinite! The population can be described by population parameters e.g. population mean = \\(\\mu\\), population variance = \\(\\sigma^2\\). These are generally characters from the Greek alphabet. Since populations are large, we usually cannot determine these values exactly.\nOften we wish to make generalisations about populations that are too large or too difficult to survey completely. In these cases we sample the population and use characteristics of the sample to extrapolate to characteristics of the larger population. We can define a sample (of size n) as drawn from population “at random”. We call each piece of information recorded about a sampling unit or subject (e.g. plant, person, animal, 1x1m plot of land) an observation.\n\n\n\nn = 20 units (v) sampled at random from population of N = 1,000 (O)\n\n\nThe sample is taken to be representative of the population – but there are no guarantees! It is easier/quicker/cheaper to take a sample than study the entire population (since n &lt;&lt; N). The results obtained from the sample not important in themselves - the importance is in how it can be used to estimate population parameters i.e. \\(\\bar{y}\\) estimates \\(\\mu\\); \\(s^2\\) estimates \\(\\sigma^2\\).\nExample: For a sample of \\(n = 10\\) cows, gestation length in cattle was measured in days. The sample mean \\(\\bar{y} = 345\\) days estimates the population mean \\(\\mu = ???\\) days. The sample standard deviation \\(s = 10\\) days estimates population standard deviation \\(\\sigma = ???\\) days.\n\n\nVariables (types of data)\nThe table below shows fresh weights of cabbages that were included in a field trial that was investigating the effects of irrigation frequency and plant spacing on cabbage yields. This fresh weight measurement is on a continuous scale.\nGenerally in an experiment, several different characteristics of the subjects are measured/recorded. We may score the level of insect damage to leaves of the plant on a scale of 0 to 3 (0 = no damage, 1 = slight damage, 2 = moderate damage, 3 = heavy damage). This is a discrete categorical scale as only certain values on the scale are defined, but it is also an ordered scale. If we had difficulty deciding how to categorize plants using this scale, we might choose to classify plants as either damaged or not damaged, which is a binary measurement scale.\nAlready you can begin to see that from just one fairly simple field trial quite a lot of data can be generated, also data of differing properties/types.\n\n\n\nYields of cabbage (mean fresh weight per head in kg) for 24 plots (Source: Mead, Curnow & Hasted (2003)).\n\n\nData may also be classified as either quantitative (e.g. root length) or qualitative (e.g. plant species). Quantitative observations are based on some sort of measurement e.g. length, weight, temperature, pH. Qualitative observations are based on categories reflecting a quality or characteristic of the observed event e.g. male vs. female, diseased vs. healthy, mutant vs. wild type.\nThe most common types of variables are:\n\nContinuous (and interval) data can assume any value in some (possible unbounded) interval of real numbers. Examples are length, weight, temperature, volume, height.\nDiscrete variables assume only isolated values. E.g. trees per hectare, items per quadrat, number of diseased plants in a section of a glasshouse. They arise from counting – usually either the number of successes in n trials (binary data) OR the number of occurrences of the event in an interval of time or space (count data).\nCategorical variables\n\nBinary variables (listed above as discrete variables) may also be thought of as categorical variables since the subject falls into either of 2 mutually exclusive categories (yes/no, alive/dead, diseased/not diseased etc.).\nOrdinal variables are not measured but nevertheless have a natural ordering. E.g. candidates for political office can be ranked by individual voters. The rank values have no inherent meaning outside the “order” that they provide. That is, a candidate ranked 2 is not twice as preferable as the person ranked 1. (Compare this with measurement variables where a plant 2 feet tall is twice as tall as a plant1 foot tall. With measurement variables such ratios are meaningful, while with ordinal variables they are not.)\nNominal data is qualitative data. Some examples are species, gender, genotype, phenotype, healthy/diseased. Unlike ranked data, there is no “natural” ordering that can be assigned to these categories.\n\n\nNote that some applied statistics texts will define the types of data slightly differently to that shown above.\nIn this unit of study most emphasis will be on the analysis of continuous measurement variables. However a few basic analyses for discrete and categorical variables will be covered.\nRecognising the type of data we have measured is REALLY important as it helps to determine the choice of analysis (and even the descriptive statistics we undertake e.g. the mean of a score doesn’t make sense, but the median is a good alternative measure of central tendency).\n\n\nRelationships between variables\nSometimes you will want to explore the relationship between two (or more) variables that you have measured in your research. You will explore the strength of the relationship and the nature of it (e.g. linear, exponential etc.).\nIn the most simple case, we examine the amount of variability in one variable (Y, the dependent variable) that is explained by changes in another variable (X, the independent variable). Often the X variable is called the predictor and the Y variable the response.\n\n\nBiological and Environmental Variability\nVariation is the norm. It occurs in both observational studies as well as designed experiments. For example, river flow varies from sampling time to sampling time and from sampling location to sampling location. Also, levels of soil contamination on a site vary from site to site.\nThis type of variation distinguishes the biological and environmental sciences from the physical sciences, which shows relatively little variability:\n\nDropping a ball from a certain height: the time to reach the ground is (nearly) identical each time;\nAmount of a chemical product produced when reagents are mixed are (nearly) identical and predictable.\n\nBiological data is far more variable due to environmental and genetic effects. To interpret biological data, we need to control variation by an appropriate experimental design, and adjust for variation by means of statistical analyses."
  },
  {
    "objectID": "introduction.html#statistical-distributions",
    "href": "introduction.html#statistical-distributions",
    "title": "Introduction",
    "section": "Statistical distributions",
    "text": "Statistical distributions\nWe use statistical distributions to determine the probability of occurrence of our particular sets of observations. A statistical distribution is a representation (using either mathematical formula or a table) of all possible outcomes of a given event. The most well-known distribution is the normal (or Gaussian) distribution for which the probability distribution function is the familiar bell-shaped curve."
  },
  {
    "objectID": "introduction.html#sampling",
    "href": "introduction.html#sampling",
    "title": "Introduction",
    "section": "Sampling",
    "text": "Sampling\nThe aim of sampling is to gain a representative picture of the population. There are various methods and strategies for doing this. Experimental groups/samples must be constructed without bias and must be large enough to give the researcher an acceptable level of confidence in the results."
  },
  {
    "objectID": "introduction.html#hypotheses",
    "href": "introduction.html#hypotheses",
    "title": "Introduction",
    "section": "Hypotheses",
    "text": "Hypotheses\nA hypothesis is a tentative explanation for the initial or ad hoc observations made. It suggests a cause and effect or associative relationship that is testable, e.g. yield response to nitrogen (N) fertilizer. The purpose and design of an experiment is to test the hypothesis."
  },
  {
    "objectID": "introduction.html#statistical-tests",
    "href": "introduction.html#statistical-tests",
    "title": "Introduction",
    "section": "Statistical tests",
    "text": "Statistical tests\nWe decide whether or not a research outcome if significant by conducting a statistical test. Firstly we set arbitrary critical thresholds of probability (P-values). The occurrence of an event whose estimated probability is less than a critical threshold is regarded as a statistically significant outcome. The usual significance level chosen is P&lt;0.05. You will also see P&lt;0.01 and P&lt;0.001 used in research literature. Which statistical test you use (there are many!) depends on the type of data you have collected and the question you wish to ask."
  },
  {
    "objectID": "introduction.html#software",
    "href": "introduction.html#software",
    "title": "Introduction",
    "section": "Software",
    "text": "Software\nIn this unit of study, we will focus on the use of one statistical package called R. It has been designed specifically for statistical analysis and is freely available. It is a powerful tool for data analysis and is widely used in the biological and environmental sciences.\nYou will also learn to use Microsoft Excel to organize and summarise your data. In the next section some of the basic features of Excel are presented. See computer lab 1."
  },
  {
    "objectID": "module01/022-sampling_distributions.html",
    "href": "module01/022-sampling_distributions.html",
    "title": "Sampling distributions",
    "section": "",
    "text": "First some definitions…\nA simple random sample is a sample of size n drawn from a population of size \\(N\\) in such a way that every possible sample of size \\(n\\) has the same probability of being selected. Variability among the simple random samples drawn from the same population is called sampling variability, and the probability distribution that characterizes some aspect of the sampling variability, usually the mean but not always, is called a sampling distribution.\nSample means are extremely important because in experiments we apply treatments to samples of material and use the mean results or yields as measures of the effects of the treatments.\n\\(\\bar y\\) is just one estimate of \\(\\mu\\), say \\(\\bar y_1\\). If another sample of size \\(n\\) were drawn from the population, we would have a slightly different estimate of \\(\\mu\\), say \\(\\bar y_2\\). For example, if we take a sample of 20 from the herd of 100 cows in the Table below we may find that the mean is 16.025. If we calculate the mean for a second sample (say the values in columns 5 & 7 of the Table), we have \\(\\bar y  = 17.055\\). If we take other samples we could get more values of the sample mean, in general all different, and by taking sufficient samples we could obtain the distribution of the values of the sample means for a given size of sample. That is, this process could be repeated, giving many different estimates of \\(\\mu\\), say \\(\\bar y_1, \\bar y_2, \\bar y_3, \\bar y_4,...\\) This forms a distribution of possible sample means, \\(\\bar y\\)’s.\nProperties we would expect the distribution of the sample mean to have:\n\nThe mean value of the distribution of the sample mean would be the same as the mean value of the distribution of the original observations (since there is no reason for expecting it to be either greater or smaller). (Population mean of the \\(\\bar y\\)’s is \\(\\mu\\).)\nThe mean of a number of observations should be a better estimate of the population mean so that we would expect the spread of the distribution of the sample means to be less than that of the distribution of the original observations.\n\nTable Average weekly milk yields (in gallons) of a herd of 100 cows [p. 10 Mead et al. (1993)]\n A result (without mathematical proof)…\nIf a variable y is distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the variable \\(\\bar y\\) which is the mean of a sample of \\(n\\) observations of \\(y\\), is distributed with mean \\(\\mu\\) and variance \\(sigma^2/n\\). The variance of the distribution of the sample mean is, as expected, less than the variance of an individual observation.\nThe square root of the variance of a sample mean is called the standard error of the mean rather than the standard deviation of the mean. This term is used to avoid confusion with the standard deviation of the distribution of individual observations.\nThe standard error of the mean is given by\n\\(\\sigma/\\sqrt n\\) or \\(\\sqrt{\\frac{\\sigma^2}{n}}\\).\nwhich is usually estimated by the standard deviation of the sample mean,\n\\(se(\\bar y)=s/\\sqrt n\\) = or \\(\\sqrt{\\frac{s^2}{n}}\\)\n\n$se(y) is a measure of accuracy for estimating \\(\\mu\\) such that:\n\n\\(se(\\bar y)\\) \\(\\uparrow\\) as \\(s\\) \\(\\uparrow\\)\n\\(se(\\bar y)\\) \\(\\downarrow\\) as \\(n\\) \\(\\uparrow\\).\n\nTo halve the size of a standard error, the sample size (n) needs to be increased \\(4\\times\\); a reduction to 1/3 requires a \\(9\\times\\) increase in \\(n\\), etc.\n\nExample: Root growth of rye grass\nSince the sample mean of \\(\\bar y = 305\\) mg and standard deviation of \\(s = 46\\) mg was based on \\(n = 10\\) pots, the standard error of \\(\\bar y\\) is\n\\(se(\\bar y) = \\frac{46}{\\sqrt{10}} = 14.5\\) mg.\n\ns &lt;- 46\nn &lt;- 10\nse &lt;- s/sqrt(n)\nse\n\n[1] 14.54648\n\n\nHow many observations are required to have a standard error of no more than 5 mg?\nSolution:\n\\(se(\\bar y) = 5 = \\frac{46}{\\sqrt n}\\) \\(\\sqrt n = 46/5 = 9.2\\) \\(n = 9.2^2 = 85\\)\nSo n = 85 observations would be required\nNotes - We now can also think of \\(\\bar y\\) being an entire distribution of values, but in practice we usually only observe one value of\\(\\bar y\\) - If the original data is normally distributed, \\(y \\sim N(\\mu,\\sigma^2)\\), the distribution of the sample mean is also normal, \\(y \\sim N(\\mu,\\sigma^2/n)\\)\nExample:\n\nRye grass root growth (mg dry weight), \\(y \\sim N(300, 50^2)\\)\nBased on samples of size n = 10, \\(\\bar y \\sim N(300, 50^2 / 10)\\), i.e. \\(\\bar y \\sim N(300, 15.82)\\).\n\n Two other mathematical results concerning the distribution of the sample mean emphasize the importance of the normal distribution:\n\nIf the distribution of the original observation, \\(y\\), is normal, then the distribution of the sample mean, \\(\\bar y\\), is also normal.\nFor almost all forms of distribution of the original observation, \\(y\\), the distribution of the sample mean, \\(\\bar y\\), for a sample of \\(n\\) observations, tends to the normal distribution as \\(n\\) increases.\n\nContinuing from the 2nd result…\nIn fact the tendency for distributions of sample means to become normal is so strong that, for many forms of original distribution, if n is more than 5 the distribution of the sample mean is almost indistinguishable from the normal distribution. The result, which is known as the Central Limit Theorem, is extremely useful. It can be used to explain why so many biological quantities have an approximately normal distribution.\nExample: Distribution of the Sample Mean\nRye grass root growth (mg dry weight), \\(y \\sim N(300, 502)\\) A single root measurement is made. How likely is it that the dry weight exceeds 320 mg? (i.e. \\(P(Y &gt; 320)\\))\nSolution:\n\\(P(Y &gt; 320) = 1 - P(Y \\leq 320)\\)\n\nmu &lt;- 300\nsigma &lt;- 50\n1 - pnorm(320, mu, sigma)\n\n[1] 0.3445783\n\n\nBased on samples of size n = 10, \\(\\bar y ~ N(300, 502 / 10)\\), i.e. \\(\\bar y \\sim N(300, 15.82)\\). How likely is it that a sample mean based on 10 observations exceeds 320 mg? (i.e. P(\\(\\bar y &gt; 320\\)))\n\nmu &lt;- 300\nsigma &lt;- 50\nn &lt;- 10\n1 - pnorm(320, mu, sigma/sqrt(n))\n\n[1] 0.1029516\n\n\nNote that it is less likely for a mean of 10 observations to be this “far” from the population mean, compared with a single value.\n\n\nThe Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p. 23 Mead et al. (1993)]\n\nCreate a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\n\n[1] 301.8958\n\nsd(carrots$Weight_g)\n\n[1] 221.3131\n\nggplot(carrots, aes(x=Weight_g)) + \n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\n\n[1] 301.435\n\nsd(sample_means)\n\n[1] 104.5059\n\nggplot() + \n  geom_histogram(aes(x=sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\n\n[1] 300.7962\n\nsd(sample_means)\n\n[1] 43.70914\n\nggplot() + \n  geom_histogram(aes(x=sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "**MODULE 1**",
      "Sampling distributions"
    ]
  },
  {
    "objectID": "module01/022-sampling_distributions.html#the-central-limit-theorem",
    "href": "module01/022-sampling_distributions.html#the-central-limit-theorem",
    "title": "Sampling distributions",
    "section": "",
    "text": "The Central Limit Theorem states that if a sample of size n is drawn from a population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), then the distribution of the sample mean, \\(\\bar y\\), tends to the normal distribution as \\(n\\) increases, with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\).\nILLUSTRATION of the Central Limit Theorem using Weights of 144 carrots (grams) in an arbitrary order [p. 23 Mead et al. (1993)]\n\nCreate a histogram of the individual weights (see below). Obviously the distribution is not normal.\n\n\nlibrary(ggplot2)\ncarrots &lt;- read.csv(\"Carrot_weights.csv\")\n\nmean(carrots$Weight_g)\n\n[1] 301.8958\n\nsd(carrots$Weight_g)\n\n[1] 221.3131\n\nggplot(carrots, aes(x=Weight_g)) + \n  geom_histogram(binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFor these individual observations, mean = 301.896g and standard deviation = 221.313g.\nTake 50 random samples each of 4 weights; then average each of these 50 samples. Generate another histogram of these means (where n = 4). How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 4\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\n\n[1] 301.435\n\nsd(sample_means)\n\n[1] 104.5059\n\nggplot() + \n  geom_histogram(aes(x=sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFor this column of means, the mean = 301.435g and the standard deviation = 104.506g.\nNOTE The standard deviation here is actually the standard error (see earlier section in these notes).\n\nRepeat the process, but now with n = 16. How would you compare this to the histogram of the original weights?\n\n\nset.seed(1)\nn &lt;- 16\nn_samples &lt;- 50\nsample_means &lt;- rep(NA, n_samples)\nfor (i in 1:n_samples) {\n  sample_means[i] &lt;- mean(sample(carrots$Weight_g, n))\n}\n\nmean(sample_means)\n\n[1] 300.7962\n\nsd(sample_means)\n\n[1] 43.70914\n\nggplot() + \n  geom_histogram(aes(x=sample_means), binwidth = 20, fill = \"lightblue\", color = \"black\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFor this column of means (where n = 16), the mean = 300.796g and the standard deviation = 43.709g.\nThe means of our set of samples are of course not equal to the mean of the 144 individual observations but from our mathematical result that the variance of the distribution of means of sample of \\(n\\) observations is \\(\\sigma^2/n\\), we would expect the variance of the three distributions to be \\(\\sigma^2\\), \\(\\sigma^2/4\\) and \\(\\sigma^2/16\\), so that the standard deviations should be \\(\\sigma\\), \\(\\sigma/2\\) and \\(\\sigma/4\\) respectively. Do our estimated values agree tolerably with this expectation?",
    "crumbs": [
      "**MODULE 1**",
      "Sampling distributions"
    ]
  },
  {
    "objectID": "module03/032-nonparametric1.html",
    "href": "module03/032-nonparametric1.html",
    "title": "Chi-squared tests",
    "section": "",
    "text": "Chi-squared tests\nThe chi-squared distribution (where chi is pronounced ‘ky’) is a very widely used distribution in statistics. Its symbol is 2. It has MANY applications. Here we will consider only two of these applications – tests of agreement with expected outcomes, and contingency tables.\n7.1 Notes on the 2 Distribution\nThe density function for a 2 distribution is positively skewed, that is, it has a long tail to the right. The typical shape of the 2 density function is that shown for the 4 df case in Figure 7.1 below. When df is very low e.g. 1 or 2, the curve changes shape dramatically. When df are very large (say greater than 100), the 2 distribution approaches the shape (and properties) of a normal distribution.\nFigure 7.1 The shape of the 2 density function for various degrees of freedom.\nThe mean and variance of a 2 variable are simple functions of the degrees of freedom of the distribution. If we express the general degrees of freedom as  (Greek n), then\nMean of   variable =   (i.e. mean = df)\nVariance of   variable = 2 (i.e. variance = twice the df)\nCritical values of a 2 distribution are given in the 2 probability table that appears as Appendix A.5.\n7.2 Testing Agreement of Frequency Data with Expectation Models\n7.2.1 Steps in Chi-Squared Tests of Agreement\nThe process for performing a goodness of fit test is the similar to that of the other hypothesis tests you have encountered thus far, that is,\n\nChoose an appropriate hypothesis test for the type of data you have, and the type of question you’re asking.\nChoose the level of significance for the test.\nWrite null and alternate hypotheses. Here (as for normality tests) the null hypothesis is always that the data can be assumed to follow the distribution under consideration.\nCalculate the expected values. To do this, we assume that the null hypothesis is true and generate the expected values accordingly\nCheck the assumptions or requirements of the test. For observed versus expected chi square goodness of fit tests, the requirements of the test are that a) no cell should have an expected value of less than 1 and b) no more than 20% of cells should have expected values less than 5. To overcome either of these problems, we tend to collapse cells together before calculating the test statistic – however there are alternative tests designed to accommodate these situations.\nCalculate the test statistic and degrees of freedom.\nObtain the P-value.\nDraw a statistical conclusion, and use this to generate a biological conclusion.\n\n7.2.2 Examples: Testing Whether Outcomes are Equally Probable\nEXAMPLE 1\nSometimes the simplest form of hypothesis is that different outcomes are equally probable. For example, we expect that when a “fair” coin is tossed that the heads and tails outcomes are equally probable. However, we would see different results if the coin is biased and we can conduct a formal hypothesis test to see whether the outcomes are deviating significantly from our expectation of a “fair” coin.\nEXAMPLE 2 (from Mead et al, 2003)\nSuppose that 40 testers were asked to compare four different cheeses produced by different procedures and identified only by the letters A, B, C, D. Assume that each tester makes one choice and the preferences were as follows.\nCheese First preference A 5 B 7 C 18 D 10 Total 40\nWe might suspect that this shows an overall preference for C. To test the simple model that testers are equally likely to prefer A, B, C, or D, we would calculate the expected frequency for each cheese to be preferred as the total number of testers divided by 4 = 40/4 = 10. Then we calculate:\n =  =9.80.\nThis time we have four frequencies with one overall restriction that they total 40, and so there are 3 df. The 5% point of the distribution on 3df is 7.82, so the unevenness of the preferences is significant (given that the value of 9.80 is greater than the value of 7.82. The evidence suggests that the model of equally likely choices is incorrect. [Equivalently, we could produce a chi-squared probability via =CHISQ.DIST.RT(9.80,3) in Excel which returns P = 0.0203. We reject H0 since P &lt; 0.05.)\nTo assess the extent to which it is the preference for cheese C that contradicts the model, we might decide to do a further test to compare whether the preference for C only is different to the preference for all the other cheeses. In a model of likely choices, our expected values are C = 10 and all other = 30. We observed C = 18, and all other = 22. You can proceed with the test as per above starting with\n =  =…      etc.\n7.2.3 Example: Testing Whether Outcomes are in Expected Proportions (from Mead et al, 2003)\nA total of 560 primula plants were classified by the type of leaf (flat or crimped) and the type of eye (normal or Primrose Queen).\nThe figures obtained for the primula plants follow. Normal eye Primrose Queen eye Total Flat leaves 328 122 450 Crimped leaves 77 33 110 Total 405 155 560\nOn the hypothesis of a Mendelian 3:1 ratio, we would expect, for each characteristic, ¾ of the total 560 observation in the first class of the characteristic and the remaining ¼ in the second class. Further, this model predicts that ¾ of the flat-leaved plants should have normal eyes, resulting in ¾ × ¾ of all the plants or 9/16 with flat leaves and normal eyes; the remaining ¼ of the flat-leaved plants, which is ¼ × ¾ or 3/16, should have Primrose Queen eyes. Similarly, 3/16 of the plants should have crimped leaves and normal eyes; and 1/16 crimped leaves and Primrose Queen eyes.\nThe calculation of these expected or predicted proportions is shown below.\nNormal eye  Primrose Queen eye\nFlat leaves ¾ × ¾ = 9/16 ¼ × ¾ = 3/16 Crimped leaves ¾ × ¼ = 3/16 ¼ × ¼ = 1/16\nHence, the hypothesis predicts ratios of 9:3:3:1 for the four classes (flat normal: flat Primrose Queen: crimped normal: crimped Primrose Queen). The expected frequencies are calculated as 9/16, 3/16, 3/16, and 1/16 of 560, producing 315, 105, 105, and 35.\nThe observed and expected frequencies are summarized in the table below.\nNormal eye  Primrose Queen eye\nFlat leaves 328 (315) 122 (105) Crimped leaves 77 (105) 33 (35)\n    =  \n    = 0.54 + 2.75 + 7.47 + 0.11 = 10.77. \nWe compare 10.77 with the 5% point of the distribution on 3df (7.82). We conclude that the 9:3:3:1 model is not acceptable.\nSee pp. 332-333 of Mead et al, 2003 for what to do next… after rejecting the model.\n7.3 Contingency Tables\n7.3.1 Example: (2 x 2) Contingency Table\nConsider an experiment in which two surgical procedures are to be compared by observing the recovery rates of animals receiving either Procedure 1 or Procedure 2. Twenty animals were randomly allocated to receive Procedure 1 and twenty animals to receive Procedure 2.\nRecovered   \nYes No  Total\nProcedure 1 14 6 20 Procedure 2 8 12 20 Total 22 18 40\nThis is one form of a 22 contingency table, since there are two rows and two columns (ignoring the totals). It appears that Procedure 1 leads to a higher recovery rate. Is this due to chance?\nSolution: We will perform a statistical hypothesis test:\nH0: There is no difference in the true recovery rates for animals on either procedure H1: The recovery rates do differ.\nIn terms of parameters, let p1 be the probability that an animal recovers under Procedure 1, and p2 the probability that an animal recovers under Procedure 2. Then the hypotheses are equivalent to\nH0: p1 = p2\nH1: p1 ≠ p2\nEstimates of individual recovery rates are = 14/20 = 0.7 and = 8/20 = 0.4. Is this difference due to chance?\nIf H0 is true, there is a common recovery rate (which we label p). Assuming H0 is true, the best estimate of p is\n  =  .\nSo the expected frequency (under H0) of recoveries for Procedure 1 would be 20 22/40 = 20 0.55 = 11 animals. In general this can be written as:\nSo the expected frequencies for the cells in the table are:\n  Expected frequencies are written on the contingency table in parentheses, allowing comparisons with observed frequencies:\nRecovered   \nYes No  Total\nProcedure 1 14 (11) 6 (9) 20 Procedure 2 8 (11) 12 (9) 20 Total 22 18 40\nThe table shows observed (and expected) frequencies. The 2 test statistic is then calculated using:\nLarge values of 2 indicate discrepancies between observed and expected frequencies, i.e. large values indicate that H0 should be rejected in favour of H1.\nThe df of this 2 test is 1 for a 22 contingency table. In general,\nIf H0 is true, the observed 2 is just one observation from a 2 distribution with 1 df:\nSince , there is (just) not sufficient evidence to reject H0. Thus, while Procedure 1 has a higher recovery rate, it just fails to reach statistical significance. At this stage, the difference in individual recovery rates appears to be chance. Increasing the numbers of animals in a new experiment will determine the question with higher precision.\n  7.3.2 Example: (4 x 3) Contingency Table\nThe second example is a 43 contingency table. Three vaccines for a disease were compared with a control. The number of animals with no, mild, and severe infection was recorded after 24 months. Data were recorded in the following table:\n    Disease Status      \nVaccine No Mild Severe Total Control 100 (137.3) 71 (42.6) 29 (20.1) 200 A 146 (133.9) 32 (41.6) 17 (19.6) 195 B 149 (132.5) 28 (41.2) 16 (19.3) 193 C 146 (137.3) 37 (42.6) 17 (20.1) 200 Total 541 168 79 788\nThe table shows observed (and expected) frequencies.\nWe test H0 that there is no association between disease status and vaccination given, i.e. all vaccinations have equal effectiveness.\nAssuming H0 is true, the expected frequencies are calculated as follows:\ne.g. Expected frequency for an animal in the Control, No disease group:\n.\nAs before, the test statistic is and this will have (4-1)(3-1) = 6 df.\nThis is how to do the test in the Stats menu in GenStat:\nStats &gt; Statistical Tests &gt; Contingency Tables…\nGenStat requires the data to be set up as a 43 Table type of spreadsheet (as opposed to a Variate or Matrix). It then reports the X2 test statistic and P-Value by selecting the Pearson method.\nPearson chi-squared value is 45.22 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001\nThe other available method is known as the maximum likelihood (ML) method. The two answers are usually very similar:\nLikelihood chi-squared value is 43.34 with 6 df.\nProbability level (under null hypothesis) p &lt; 0.001   The ML 2 is calculated as follows:\nThe degrees of freedom are (r – 1)(c – 1) as before, where r and c are the numbers of rows and columns respectively.\nIn general, the 2 approximation should only be used if the sample size is relatively large. As a general rule, there should be few expected frequencies below 5 and none below 1.0. Most packages will print out a warning when this occurs. Some situations allow exact probabilities to be calculated, but we will not pursue that in this course.\nAn example of low numbers would be the following, (these are basically 1/10th the numbers in the previous example):\nDisease Status  \nVaccine No Mild Severe Total Control 10 7 3 20 A 15 3 2 20 B 15 3 2 20 C 15 4 2 21 Total 55 17 9 81",
    "crumbs": [
      "**MODULE 3**",
      "Chi-squared tests"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html",
    "href": "module03/031-ttest2.html",
    "title": "Two-sample \\(t\\)-test",
    "section": "",
    "text": "In the one-sample tests just considered, we compared a population mean, \\(\\mu\\) (where our experimental sample represents the population) with a fixed numeric value of interest. In practice, this is fairly rare. More frequently we have samples drawn from two (or more) populations and we compare the two means to see if the treatments produce similar results.\nThe null hypothesis is that the two samples come from a population with the same true mean, \\(\\mu\\). This is commonly expressed as\n\\[H_0: \\mu_1 = \\mu_2\\]\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are the means of the two populations. The alternative hypothesis is that the two means are different, i.e.\n\\[H_1: \\mu_1 \\neq \\mu_2\\]",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#assumptions",
    "href": "module03/031-ttest2.html#assumptions",
    "title": "Two-sample \\(t\\)-test",
    "section": "Assumptions",
    "text": "Assumptions\nThe two-sample \\(t\\)-test assumes that the data are :\n\nContinuous,\nat least approximately normally distributed, and\nthe variances of the two sets are homogeneous (i.e. the same).\n\nIdeally these assumptions should be tested before you carry about the two-sample \\(t\\)-test.",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#what-if-assumptions-are-not-met",
    "href": "module03/031-ttest2.html#what-if-assumptions-are-not-met",
    "title": "Two-sample \\(t\\)-test",
    "section": "What if assumptions are not met?",
    "text": "What if assumptions are not met?\nIf the data do not meet either of these assumptions, then you may choose an appropriate transformation or look for another technique to use e.g. a non-parametric method (see non-parametric sections of this book).\nIf the variances cannot be assumed to be equal (but the data are normally distributed), there is a way of adjusting the two-sample \\(t\\)-test to compensate for this – it’s called the Satterthwaite’s approximation – more on this later!",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#three-variants-of-the-two-sample-t-test",
    "href": "module03/031-ttest2.html#three-variants-of-the-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Three Variants of the Two-Sample \\(t\\)-test",
    "text": "Three Variants of the Two-Sample \\(t\\)-test\nThere are 3 different ways in which a two-sample \\(t\\)-test can be used. They ALL assume that the data is approximately normally distributed.\n\nIndependent samples, equal variance - run a two-sample \\(t\\)-test assuming equal variances.\nIndependent samples, unequal variance - run a two-sample \\(t\\)-test assuming unequal variances.\nPaired samples - run a paired \\(t\\)-test.\n\nIn choosing which variant of the \\(t\\)-test is applicable in your situation, you first need to decide whether your two samples are paired or unpaired (independent).",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#paired-data",
    "href": "module03/031-ttest2.html#paired-data",
    "title": "Two-sample \\(t\\)-test",
    "section": "Paired Data",
    "text": "Paired Data\nPaired samples arise when we measure pairs of similar experimental units. In this pair of experimental units, one unit receives Treatment 1 and the other receives Treatment 2.\nIn some cases, treatments are applied to the same experimental unit e.g. one half of a piece of fruit receives Trt 1 and the other half Trt 2; two plants of different varieties are grown in the same pot (here variety is the treatment).\nIn recognising this pairing in our analysis we are taking into account the fact that biological variation between pairs is likely to be larger than within pairs. This way, we get a clearer picture of the difference that is due to the treatment factor.\n\nExamples of paired data\n\nObservations at two times on the same experimental unit\n\nBefore and after readings of particle matter in the air on 3 sites near a new power station (before the station was built, and after it became operational). (Dytham 2003, 80)\nMeasurements of water flow on two consecutive days at 6 sites along a river. (Dytham 2003, 83)\n\nObservations on 2 halves/parts of the same experimental unit\n\nOne half of each (uncut) grapefruit was exposed to sunlight, and the other half was shaded (McConway et al 1999, p. 198).\nA standard (recommended) variety of wheat is compared with a new variety via 2 similar plots on each of 8 farms (Clewer and Scarisbrick 2001, p. 46).",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#two-sample-t-test-for-independent-samples",
    "href": "module03/031-ttest2.html#two-sample-t-test-for-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Two-Sample t-Test for Independent Samples",
    "text": "Two-Sample t-Test for Independent Samples\nProcedure for the test:\n\nSet up the null and alternate hypotheses\nDecide on the level of significance, 5%, 1%, 0.01% etc.\nCheck the assumptions of normality and equal variance. We use an F-test to formally test for equality of variance.\nCalculate the test statistic \\[t = \\frac{\\bar{y}_1 - \\bar{y}_2}{SED}\\] where \\(\\bar{x}_1\\) and \\(\\bar{x}_2\\) are the sample means, and \\(SED\\) is the standard error of the difference between the means.\nCalculate the degrees of freedom (df).\nFind the P-value in printed statistical tables or via GenStat or Excel.\nMake a statistical conclusion by comparing this P-value to your chosen level of significance (if P &lt;α, then reject null hypothesis).\nCalculate the confidence interval.\nInterpret your results biologically.\n\n\n\n\n\n\n\nCalculating the test statistic\n\n\n\nWhen calculating the test statistic, use \\(y_1\\) as the larger mean. This will give a positive value for \\(t\\).",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "href": "module03/031-ttest2.html#confidence-interval-for-mu_1---mu_2-independent-samples",
    "title": "Two-sample \\(t\\)-test",
    "section": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)",
    "text": "Confidence Interval for \\(\\mu_1 - \\mu_2\\) (Independent Samples)\nA two-sample t-test shows whether there is evidence of a difference in population means. The magnitude of this difference can be estimated with a confidence interval.\nA 95% confidence interval for the true difference \\(\\mu_1 - \\mu_2\\) is given by \\[\\bar{y}_1 - \\bar{y}_2 \\pm t^{\\alpha/2}_{df} \\times SED\\]\nwhere \\(\\bar{y}_1 - \\bar{y}_2\\) is the difference between the sample means, \\(t^{\\alpha/2}_{df}\\) is the critical value from the t-distribution for the chosen level of significance and degrees of freedom, and \\(SED\\) is the standard error of the difference between the means.\nThe df and SED need to take into account whether or not you are assuming equal variances.",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "href": "module03/031-ttest2.html#sed-and-df-for-independent-samples-with-equal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with EQUAL Variances",
    "text": "SED and df for Independent Samples with EQUAL Variances",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "href": "module03/031-ttest2.html#sed-and-df-for-independent-samples-with-unequal-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "SED and df for Independent Samples with UNequal Variances",
    "text": "SED and df for Independent Samples with UNequal Variances\nWhat do you do if when you’re checking your assumptions for a two- sample t-test you find that the variances are not equal? You can go ahead with a modified t-test or you can choose a different test.\nThis modified t-test used in the case of unequal variances is often called Satterthwaite’s approximate t-test.\n\nSatterthwaite’s Approximate t-Test\nThe null and alternate hypotheses remain the same i.e. \\[H_0: \\mu1 = \\mu2\\] or \\[H_0: \\mu1 - \\mu2 = 0\\]\nThe formula for the test statistic, \\(t\\), in Satterthwaite’s approximate test is a little different to that for the t-test with equal variances. The s.e.d. changes because we can no longer use a pooled estimate of the variance (since the variances cannot be assumed equal).\nA correction for unequal variance is made to the degrees of freedom.\nThen proceed as usual through the rest of the test - i.e. find the P-value; draw a statistical conclusion about your hypothesis; interpret your results biologically.",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#f-test-for-equality-of-variances",
    "href": "module03/031-ttest2.html#f-test-for-equality-of-variances",
    "title": "Two-sample \\(t\\)-test",
    "section": "F-Test for Equality of Variances",
    "text": "F-Test for Equality of Variances\nOne of the conditions for the independent sample t-test to be valid is that the population variances σ12 and σ22 are equal.\nTo test the null hypothesis that σ12 = σ22 divide the larger s2 value by the smaller s2 to obtain the variance ratio, v.r.:\nTo undertake this two-tailed test at the 5% level you need to carry out the one-tailed test at the 2.5% level. You can use the 2.5% F table of critical values, or you can use GenStat via the menus Data&gt;Probability Calculations… to find the P-value.\nIf you use the printed statistical table, you will need to compare the critical value you find there with the variance ratio you calculated. If the calculated variance ratio &gt; critical value, reject H0 and conclude that the variances are significantly different.",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/031-ttest2.html#example-two-sample-t-test",
    "href": "module03/031-ttest2.html#example-two-sample-t-test",
    "title": "Two-sample \\(t\\)-test",
    "section": "Example: Two-Sample t-Test",
    "text": "Example: Two-Sample t-Test\nWeights of two breeds of cattle are to be compared: 15 cattle from Breed 1 and 12 cattle from Breed 2 were randomly sampled. Their recorded weights (kg) are shown.\nBreed 1 Breed 2 148.1 187.6 146.2 180.3 152.8 198.6 135.3 190.7 151.2 196.3 146.3 203.8 163.5 190.2 146.6 201.0 162.4 194.7 140.2 221.1 159.4 186.7 181.8 203.1 165.1\n165.0\n141.6\nThe following descriptive statistics are obtained from these data.\nBreed 1 Breed2\nSample mean (kg) 153.700 196.175 Sample s.d. (kg) 12.301 10.616\nIs there any systematic difference in their weights? The question, “Is there any systematic difference in their weights?”, is asking whether or not there is any significant difference between the 2 samples – and in effect if there is any significant difference between the 2 breeds because the factor that distinguishes the 2 samples is breed.\nWe could answer this question by testing whether or not the population mean weights for the 2 breeds can be assumed to be equal:\nH0: μbreed 1 = μBreed 2 versus H1: μBreed 1 ≠ μBreed 2\nTo test this particular hypothesis about the equality of the means (as an avenue for answering our broader question), we would use a two-sample t-test.\nHowever, it is REALLY important to remember that there are other tests and hypotheses that we could use to answer our broader question (about whether or not there is any statistical difference between the weights of the 2 breeds). We’ll consider some non-parametric alternatives in Biometry 2.\nTo proceed with a two-sample t-test, there are 2 assumptions that we need to check: • Normality of Breed 1 data and normality of Breed 2 data • Equality of variances of the 2 samples\nBoth of these checks are hypothesis tests in their own right and contain the usual elements of a hypothesis test i.e. null & alternate hypothesis; test statistic; df; P-value or critical value; conclusion.\nTesting the Assumption of Normality If you are doing the test by hand, you would need to assume that the data are normally distributed (and hope this is true). At least the data in this example is continuous… which is one small step towards normality.\nTesting the Assumption of Equality of Variance H0: σ2Breed 1 = σ2Breed 2 versus H1: σ2Breed 1 ≠ σ2Breed 2\nTest statistic:\nThere are 2 degrees of freedom to calculate for an F test. They are called the numerator df, 1 and the denominator df, 2. (Remember from school days, numerator is the top half of a fraction, denominator is the bottom half of a fraction.) The  that looks like a curly ‘v’ is the Greek letter ‘nu’.\nFor an F test used to test equality of variance, df are: 1 = n1 – 1, 2 = n2 – 1.\nHere, 1 = nBreed 1 -1 = 15 - 1 = 14 and 2 = nBreed 2 - 1 = 12 – 1 = 11.\nUsing the 2.5% F table, we can compare (at the upper tail) the Fcritical and Fobserved values. If Fobs &gt; Fcrit, we reject H0 and conclude that the variances of the 2 samples are NOT equal.\nFrom these tables, it is not possible to find exactly, so we will make do with the closest possible value .\nWe find Fcrit ≈ 3.33 and Fobs = 1.34. Since Fcrit &gt; Fobs ,we CAN assume that the variances are equal.\nSo… assuming normality and equal variances, we can complete a “pooled” two-sample t-test where\nTesting the null hypothesis that H0: μBreed 1 = μBreed 2, we find that there is a significant difference in the mean weights of the two breeds of cattle (T = 9.46, df = 25, P &lt; 0.001). The mean weight of Breed 2 is significantly higher and we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1.\nThis last piece of information, “we are 95% confident that the mean weight for Breed 2 is between 33.2 and 51.7 kg higher than the mean weight for Breed 1”, is obtained from the 95% confidence interval for the true difference (μ1 – μ2).\nNB. Recall that a 95% confidence interval for the true difference (μ1 – μ2) is .\n6.10 Paired t-Test\nWe can re-write the generic null hypothesis for a two-sample test of means, H0: μ1 = μ2 as… H0: μ1 – μ2 = 0, or H0: μd = 0.\nThe paired t-test is actually a one-sample t-test of the differences between pairs of observations (from 2 different samples).\nAssumption: • Data is approximately normally distributed.\nSince we are doing a one-sample t-test on the differences then the assumption of equal variances is not relevant. Firstly, create a single column of data to use in the t test. Each value in the column corresponds to the difference between the 2 values for a particular matched pair.\nFarm Yield Variety A (kg) Yield Variety B (kg) Difference 1 17.8 14.7 3.1 2 18.5 15.2 3.3 3 12.2 12.9 -0.7 4 19.7 18.3 1.4 5 10.8 10.1 0.7 6 11.9 12.2 -0.3 7 15.6 13.5 2.1 8 12.5 9.9 2.6\nThe difference for Farm 1 = 17.8 – 14.7 = 3.1.\nData Source: Clewer and Scarisbrick (2001, 46)\nSecondly, find some summary statistics of the differences so you can complete the t test:\nNo. of values, n = 8        Sum = 12.2          Mean = 1.525\nVariance = 2.299            Std Dev = 1.516\nand df = n -1, where d is usually 0.\n6.11 Confidence Interval for 1 – 2 (Paired Samples)\nA 95% confidence interval for the mean difference is",
    "crumbs": [
      "**MODULE 3**",
      "Two-sample $t$-test"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "module02/041-linear_functions.html",
    "href": "module02/041-linear_functions.html",
    "title": "Linear functions",
    "section": "",
    "text": "Linear functions\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**MODULE 2**",
      "Linear functions"
    ]
  },
  {
    "objectID": "module02/042-linear_functions_multi.html",
    "href": "module02/042-linear_functions_multi.html",
    "title": "Linear functions – multiple predictors",
    "section": "",
    "text": "Linear functions – multiple predictors\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**MODULE 2**",
      "Linear functions -- multiple predictors"
    ]
  },
  {
    "objectID": "module02/043-nonlinear.html",
    "href": "module02/043-nonlinear.html",
    "title": "Non-linear functions",
    "section": "",
    "text": "Non-linear functions\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**MODULE 2**",
      "Non-linear functions"
    ]
  },
  {
    "objectID": "module02/040-describing_relationships.html",
    "href": "module02/040-describing_relationships.html",
    "title": "Describing relationships",
    "section": "",
    "text": "Describing relationships\nIn summary, this book has no content whatsoever.",
    "crumbs": [
      "**MODULE 2**",
      "Describing relationships"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Welcome to the ENVX1002 handbook. This handbook is designed to be an optional companion to the lectures, labs and assessments for the course."
  },
  {
    "objectID": "index.html#how-to-use-this-handbook",
    "href": "index.html#how-to-use-this-handbook",
    "title": "Preface",
    "section": "How to use this handbook",
    "text": "How to use this handbook\nRead this handbook before, or after each lecture to better understand certain concepts. You can also use this book as a reference when working on lab exercises and assessments.\nYou may download a PDF copy of this handbook on the sidebar of the HTML version of this book. You may also download the source code for this book from the GitHub repository."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Preface",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to acknowledge the work of previous generations of teaching staff who created the bulk of the teaching material within this manual. In particular, the work of:\n\nAssoc. Prof. Mick O’Neill\nDr. Kathryn Aufflick\nAssoc. Prof. Peter Thomson\nProf. Thomas Bishop\n\nWhen you are ready, check the sidebar to get started."
  },
  {
    "objectID": "module03/030-ttest1.html",
    "href": "module03/030-ttest1.html",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.\n\n\n\n(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n[1] -1.959964\n\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\n[1] 0.5977601\n\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci\n\n[1] 0.3233181 0.4833485\n\n\n\n\n\nThere are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\n\n[1] -2.063899\n\nqt(0.975, 24)\n\n[1] 2.063899\n\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).\n\n\n\n\nIf an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.\n\n\n\nAs you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\n\n[1] 0.529368\n\nkurtosis(bcc_df$BloodCellCount)\n\n[1] 3.292761\n\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$BloodCellCount\nW = 0.96042, p-value = 6.607e-09\n\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\n\n[1] -0.1373212\n\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n[1] 2.854581\n\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$sqrt_BloodCellCount\nW = 0.97117, p-value = 4.13e-07\n\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\n\n[1] -0.8655114\n\nkurtosis(bcc_df$log_BloodCellCount)\n\n[1] 3.684001\n\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$log_BloodCellCount\nW = 0.92515, p-value = 3.025e-13\n\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\n\n[1] 2.691321\n\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n[1] 13.53575\n\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tccb_df$sqrt_TcCB_ppb\nW = 0.76119, p-value = 1.869e-11\n\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\n\n[1] -0.02521078\n\nkurtosis(tccb_df$log_TcCB_ppb)\n\n[1] 3.449717\n\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tccb_df$log_TcCB_ppb\nW = 0.99555, p-value = 0.9868\n\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.\n\n\n\n\nSometimes research questions are framed not as “What is a plausible range of values for such and such a parameter?” but rather “Are the data consistent with this particular value for the parameter?”. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the “health” of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don’t - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause , the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g. in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou’ll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).\n\n\n\n\n\n\nThe liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e. (population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e. more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\n\n\n\n\n\n\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\n\n\n\n\n\n\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let’s say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n[1] 0.01140625\n\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).\n\n\n\n\nOften researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case…\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!\n\n\n\n\n\nFor the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e. we assume we don’t know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs. \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the “old school” t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\n[1] 0.07064936\n\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R’s t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  toxic\nt = 2.2891, df = 5, p-value = 0.07073\nalternative hypothesis: true mean is not equal to 0.3\n95 percent confidence interval:\n 0.2872928 0.5193739\nsample estimates:\nmean of x \n0.4033333 \n\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has “heavier” tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).\n\n\n\n\nHypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  toxic\nt = 2.2891, df = 5, p-value = 0.07073\nalternative hypothesis: true mean is not equal to 0.3\n95 percent confidence interval:\n 0.2872928 0.5193739\nsample estimates:\nmean of x \n0.4033333 \n\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI’s.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance – the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI’s as they are more informative than the p-value alone. For example, the width of the CI’s says something about the precision of the estimate.\n\n\n\nExample\nThere is evidence that total nitrogen levels in the river – like many other environmental quality data – are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical “smiley” shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  310.0   610.0   800.0   855.9  1020.0  2130.0 \n\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n[1] 1.078249\n\n\n\nshapiro.test(tn$TN)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tn$TN\nW = 0.92582, p-value = 0.04293\n\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\n[1] 2.886528\n\n\nWe can use the mean of this transformed data i.e. the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nshapiro.test(tn$log10_TN)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tn$log10_TN\nW = 0.97279, p-value = 0.6375\n\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet’s test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  tn$log10_TN\nt = 4.8768, df = 28, p-value = 3.884e-05\nalternative hypothesis: true mean is not equal to 2.69897\n95 percent confidence interval:\n 2.807748 2.965309\nsample estimates:\nmean of x \n 2.886528 \n\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#confidence-intervals-for-mu",
    "href": "module03/030-ttest1.html#confidence-intervals-for-mu",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Confidence intervals (CI) are also known as “confidence limits”. Most commonly we generate a confidence interval (CI) for \\(\\mu\\) (the population mean) but you may also see CI’s for the population variance \\(\\sigma\\), or for the population probability \\(p\\) in literature.\nA confidence interval consists of two values (an upper and a lower limit). It is generally written as the two values separated by a comma within brackets e.g. (3.3, 4.1), with the lower value on the left, and the upper value on the right. We must specify a degree of likelihood or confidence that the population mean \\(\\mu\\) is located in this interval. To be more confident that the interval includes \\(\\mu\\), the width of the interval must be increased e.g. 99% CI. The most commonly chosen level or confidence is 95%, but you will also see 90% and 99% CI’s in literature.",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "href": "module03/030-ttest1.html#confidence-interval-for-mu-when-sigma-is-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "(From Glover & Mitchell, 2002.) The sample mean \\(\\bar y\\) is an unbiased estimator of the population mean \\(\\mu\\). \\(\\bar y\\)’s are not all the same due to sampling variability. Their scatter depends on both the variability of the y’s, measured by \\(\\sigma\\), and the sample size \\(n\\). Recall that the standard error of the mean is \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) and we also know that the random variable \\(\\frac{\\bar y -\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}\\) is distributed as the standard normal or the Z distribution.\nEXAMPLE\nFor the sampling distribution of this Z variable, consider what two values of capture the middle 95% of the distribution? That is, for \\(P(a \\le Z \\le b) = 0.95\\), what are a and b?\n\nIf \\(P(Z \\le a) = 0.025\\), then looking up 0.025 in R or the body of the standard normal table we find \\(a \\approx -1.960\\).\n\n\nqnorm(0.025)\n\n[1] -1.959964\n\n\n\nIf \\(P(Z \\le b) = 0.975\\) then looking up 0.975 in R or the body of the standard normal table we find \\(b \\approx 1.960\\).\n\n\nqnorm(0.725)\n\n[1] 0.5977601\n\n\nSo \\(P(-1.960 \\le Z \\le 1.960) = 0.95\\), or the values ± 1.960 capture the middle 95% of the Z distribution.\nTherefore we capture the middle 95% of the \\(\\bar y\\)’s if\n\\(P\\left(-1.960 \\leq \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\leq 1.960\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(-1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\bar{X} - \\mu \\leq 1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\n\\(\\leftrightarrow P\\left(\\bar{X} -1.960 \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X} +1.960 \\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\\)\nFrom the final equation above, we can say that the probability that the sample mean will differ by no more than 1.960 standard errors \\(\\sqrt{\\frac{\\sigma^2}{n}}\\) from the population mean \\(\\mu\\) is 0.95.\nMore commonly the equation for a CI is given as\n\\(\\text{95 % CI} = \\bar{y} \\pm z_{0.025} \\times s.e.\\)\nwhere \\(z^{0.025}\\) is a critical value from the standard normal distribution (also known as the z distribution). 2.5% of data lies to the right of \\(z^{0.025}\\). Equivalently, 97.5% of data lies to the left of \\(z^{0.025}\\). To find this value, you would look up a cumulative probability of 0.975 in the standard normal table or use the formula =NORMINV(0.975,0,1) to find it in Excel. As we have seen above in R we can use the function qnorm(0.975)\n\nqnorm(0.975)\n\n[1] 1.959964\n\n\nFor 90% or 99% confidence intervals, the only element of the CI formula that changes is the z critical value that is being used. So a \\(\\text{95 % CI} = \\bar{y} \\pm z_{0.05} \\times s.e.\\) and a \\(\\text{99 % CI} = \\bar{y} \\pm z_{0.01} \\times s.e.\\)\n### Interpreting the Confidence Interval for \\(\\mu\\)\n\\(\\bar y\\) is a random variable with a sampling distribution. Because there is an infinite number of values of \\(\\bar{y}\\), there is an infinite number of intervals of the form \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\). The probability statement says that 95% of these intervals will actually include \\(\\mu\\) between the limits. For any one interval, \\(\\bar{y} \\pm 1.96\\frac{\\sigma}{\\sqrt{n}}\\), we say that we are 95% confident that \\(\\mu\\) lies between these limits.\nEXAMPLE\nThe following data shows the concentration of a toxic substance was measured in six ‘samples’ of effluent output. The readings were:\n0.48 0.25 0.29 0.51 0.49 0.40\nThe mean for these six values is \\(\\bar y=0.403\\) \\(\\mu g/L\\). Let’s assume that the concentration of this toxic substance follows a normal distribution and that \\(\\sigma = 0.1\\) \\(\\mu g/L\\). These assumptions allow us to calculate a 95% z-based confidence interval:\n\\(\\bar{y} \\pm z^{0.025}\\frac{\\sigma}{\\sqrt{n}}\\)\nwhere \\(z^{0.025} = 1.96\\) is the upper 2.5% point of the standard normal distribution.\nSo the 95% CI for the current example is\n\\(0.403 \\pm 1.96 \\times \\sqrt{\\frac{0.1}{6}} = 0.403 \\pm 0.080 = (0.323, 0.483)\\)\nWe can say that we are 95% confident that the (population) mean concentration is somewhere in the range 0.323 to 0.483 \\(\\mu g/L\\), although the best single estimate is 0.403 \\(\\mu g/L\\).\n\ny &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nn &lt;- length(y)\nmu &lt;- mean(y)\nsigma &lt;- 0.1\nse &lt;- sigma/sqrt(n)\nz &lt;- qnorm(0.975)\nci &lt;- mu + c(-1,1)*z*se\nci\n\n[1] 0.3233181 0.4833485",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "href": "module03/030-ttest1.html#confidence-interval-for-mu-when-sigma-is-not-known-and-data-is-normally-distributed",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "There are very few times in a real world situation when you would know \\(\\sigma\\) and not know \\(\\mu\\), so a z-based CI is very rarely used in practice. The more likely real-world situation would be that we take a sample from a population with unknown shape, mean, and standard deviation. From this sample we calculate \\(\\bar y\\) and \\(s\\). By the Central Limit Theorem, we assume \\(\\bar y\\)’s sampling distribution is approximately normal. We can now use \\(\\frac{s}{\\sqrt{n}}\\), the sample standard error, as our estimate of \\(\\frac{\\sigma}{\\sqrt{n}}\\), the population standard error. When \\(\\frac{s}{\\sqrt{n}}\\) replaces \\(\\frac{\\sigma}{\\sqrt{n}}\\) in the formula \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\), we have \\(\\frac{\\bar y - \\mu }{\\frac{s}{\\sqrt{n}}}\\).\nWhile the distribution of \\(\\frac{\\bar y - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}\\) is known to be the standard normal distribution or Z distribution, replacing \\(\\sigma\\) with \\(s\\) will generate a different sampling distribution. This distribution is called the T distribution. (It is sometimes called the Student’s T distribution). A man named W.S. Gosset first published this sampling distribution in 1908.\n\n\nThe T-distribution has the following properties:\n\nIt is bell-shaped, symmetrical about the mean, median and mode, which are all of equal value. [It is a little flatter than the normal distribution.]\nThe area under the curve = 1, as is the case for all continuous probability distributions.\nThe probability density function is defined by three parameters, the mean \\(\\mu\\), the standard deviation \\(\\sigma\\) and the sample size \\(n\\). Note that the shape of the t distribution depends on the sample size, unlike that of the normal distribution (which only depends on \\(\\mu\\) and \\(\\sigma\\)).\nThe exact shape of the t distribution depends on the quantity called degrees of freedom, \\(df\\). The \\(df = n – 1\\) for any t distribution.\nIt approximates normality as \\(n \\rightarrow \\infty\\). The approximation is reasonably good for \\(n &gt; 30\\) and can be regarded as exact for \\(n &gt; 120\\). You can see in Figure 5.1 (below) that for low sample sizes (and therefore small df) the T distribution is more spread out and flatter than the normal distribution. However, as the sample size (and df) increases the T curve becomes virtually indistinguishable from the Z e.g. T49 curve in Figure 8.1 where the degrees of freedom is 49.\n\nIf you look at the “old school” t-tables, you will note that the T table is presented differently to the tables you have encountered before for the binomial, Poisson and normal distribution. Here the values in the body of the table are critical values from the T distribution rather than cumulative probabilities (as was the case for the tables for the other distributions). The same information is still available, just in a more restricted format.\n\n\n\nFig. Comparing the shapes of the Student’s T and the Z curves.\n\n\n\n\n\nThe general formula for a CI for \\(\\mu\\) when \\(\\sigma\\) is not known is\n\\(\\text{95 % CI} = \\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times s.e.(\\bar y)\\)\nHere \\(\\alpha\\) is the level of significance (or the probability of being incorrect in our estimation that we are willing to bear). For a 95% confidence interval, the corresponding level of significance is 5% (usually expressed in decimal format as 0.05). Also \\(n-1\\) is the degrees of freedom. For example, the critical value \\(t^{\\alpha/2}_{n-1}\\) (or more simply t^{0.025}_{24}) is equal to 2.064.\n\nqt(0.025, 24)\n\n[1] -2.063899\n\nqt(0.975, 24)\n\n[1] 2.063899\n\n\nNote that s.e., s.e.(\\(\\bar y\\)) and s.e.m. are all equivalent expressions for the standard error of the mean. You will see them used interchangeably among scientists and the literature they write. Remember that the s.e. in the more common case when \\(\\sigma\\) is unknown is calculated as \\(\\frac{s}{\\sqrt(n)}\\).",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#what-is-meant-by-confidence-interval",
    "href": "module03/030-ttest1.html#what-is-meant-by-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "If an experiment were to be repeated many times, on average, 95% of all 95% confidence intervals would include the true mean, \\(\\mu\\).\nThe following graph shows 100 confidence intervals produced from computer simulated data. The simulated data are 100 studies, each containing n = 6 observations of a fictitious toxic substance concentration (\\(\\mu g/L\\)) assumed to be \\(N(0.3, 0.1^2)\\).\nFor each computer generated “sample”, the sample mean \\(\\mu\\) and standard deviation (s) are calculated, then the 95% confidence interval calculated \\(\\bar{y} \\pm t^{\\alpha/2}_{n-1} \\times \\sqrt{s^2/n}\\) .\n Approximately 95% of these simulated samples have a confidence interval that includes the true value of 0.3 \\(\\mu g/L\\). In the graph above, a confidence interval includes the true mean value of 0.3 if the vertical line (representing the width of the CI) crosses the horizontal line.\nHowever in practice, when we calculate a CI from a single sample of data, we do not know if it is a confidence that includes \\(\\mu\\), but we are 95% confident that it does! 99% confidence intervals would be wider and more likely to include \\(\\mu\\), so it seems more logical to opt for the widest confidence interval possible. However, as we will learn in the next Section, there are opposing errors that are introduced when we make \\(\\alpha\\) small i.e. when we make the CI wide.",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "href": "module03/030-ttest1.html#data-transformation-and-the-lognormal-distribution",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "As you know, data are not always normally distributed. However, the most common statistical techniques assume normality of data. In situations where you wish to use one of these techniques (and the data are not normally distributed) a “transformation” is required.\nThe most common transformation in environmental modelling is the logarithm (to base 10 or base e). Other common transformations include the square root and arcsine (or angular) transformations.\nThe process of transformation is that each of the data values has the same mathematical function applied to them. For example,\nSquare root: \\(y`=\\sqrt{y}\\) or \\(y`=\\sqrt{y+ \\frac{1}{2}}\\)\nLogarithmic: \\(y`=\\log_e y\\) or \\(y`=\\log_e (y+1)\\)\nArcsine (angular) for a percentage \\(p(0 &lt;p &lt; 100)\\):\n\\(x = (180/\\pi) \\times \\arcsin(\\sqrt{p/100})\\)\nThe log transformation is often used in growth studies involving a continuous variable such as length or weight. This transformation is also useful in ecological studies involving counts of individuals when the variance of the sample count data is larger than the mean. If the sample data contain the value zero, then a modification to the \\(\\log(x)\\) transformation is the \\(\\log (x+1)\\) transformation. This transformation eliminates the mathematical difficulty that the logarithm of 0 is undefined. The square root transformation is useful when the variance of the sample data is approximately equal to the sample mean. The arcsine transformation is appropriate for data which are expressed as proportions.\nAfter the data has been transformed, all subsequent analyses take place on the transformed scale. Results may be back-transformed to original scale.\nThe following examples show how to select the optimum transformation of data.\nExample 1: Number of blood cells observed in 400 areas on a microscope slide (haemocytometer) (Fisher, 1990 p56)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of blood cells:\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\nFrequency:\n0\n20\n43\n53\n86\n70\n54\n37\n18\n10\n5\n2\n\n\n\nQuestion: Can we assume this data follows a normal distribution?\n#q: use ggplot to draw histogram, boxplot and qqnormal plot of the data\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nbcc_df &lt;- read.csv(\"BloodCellCount.csv\")\np1 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$BloodCellCount)\n\n[1] 0.529368\n\nkurtosis(bcc_df$BloodCellCount)\n\n[1] 3.292761\n\n\n\nshapiro.test(bcc_df$BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$BloodCellCount\nW = 0.96042, p-value = 6.607e-09\n\n\nObservations:\n\nThis is count data. From statistical theory, we don’t expect this data to follow a normal distribution (since it is discrete data, and the normal distribution is continuous).\nThe histogram and boxplot show that the data has a long tail to the right (appears positively skewed).\nThe skewness and kurtosis values differ from zero.\nThe formal normality test indicates that the null hypothesis of the data following a normal distribution should be rejected.\n\nConclusion:\n\nWe cannot assume this data follows a normal distribution. Distribution is POSITIVELY skewed.\n\nQuestion: Is there any transformation we can perform (that is fit a mathematical function to the data) where the data (on the transformed scale) will approximately follow a normal distribution?\nA. Square Root Transformation\n\nbcc_df$sqrt_BloodCellCount &lt;- sqrt(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(sqrt_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=sqrt_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$sqrt_BloodCellCount)\n\n[1] -0.1373212\n\nkurtosis(bcc_df$sqrt_BloodCellCount)\n\n[1] 2.854581\n\n\n\nshapiro.test(bcc_df$sqrt_BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$sqrt_BloodCellCount\nW = 0.97117, p-value = 4.13e-07\n\n\nIn spite of the fact that the Shapiro Wilks test shows this distribution is significantly different to normal the normal probability plot shows a sufficiently linear match and the histogram appears symmetric. The distribution is symmetric, transformation successful.The test is significant, but the Q-Q plot and histogram look good. The skewness and kurtosis values are close to zero.\nNote: The Shapiro Wilks Test is very sensitive to large sample sizes, i.e. n &gt; 50. In this case we use the Q-Q plot and histogram to assess normality.\nA. Log Transformation\n\nbcc_df$log_BloodCellCount &lt;- log(bcc_df$BloodCellCount)\np1 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"Frequency\")\np2 &lt;- ggplot(bcc_df, aes(log_BloodCellCount)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of blood cell counts\", x=\"Transformed Number of blood cells\", y=\"\")\np3 &lt;- ggplot(bcc_df, aes(sample=log_BloodCellCount)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of blood cells\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(bcc_df$log_BloodCellCount)\n\n[1] -0.8655114\n\nkurtosis(bcc_df$log_BloodCellCount)\n\n[1] 3.684001\n\n\n\nshapiro.test(bcc_df$log_BloodCellCount)\n\n\n    Shapiro-Wilk normality test\n\ndata:  bcc_df$log_BloodCellCount\nW = 0.92515, p-value = 3.025e-13\n\n\nTransformation is TOO STRONG - outlier(s) on left hand tail.\nExample 2: Tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples\nNote: We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed. Data is stored in the file TcCB.csv.\nA. Square root transformation\n\ntccb_df &lt;- read.csv(\"TcCB.csv\")\ntccb_df$sqrt_TcCB_ppb &lt;- sqrt(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(sqrt_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=sqrt_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(tccb_df$sqrt_TcCB_ppb)\n\n[1] 2.691321\n\nkurtosis(tccb_df$sqrt_TcCB_ppb)\n\n[1] 13.53575\n\n\n\nshapiro.test(tccb_df$sqrt_TcCB_ppb)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tccb_df$sqrt_TcCB_ppb\nW = 0.76119, p-value = 1.869e-11\n\n\nTransformation not powerful enough - still Positively Skewed\nA. Log transformation\n\ntccb_df$log_TcCB_ppb &lt;- log(tccb_df$TcCB_ppb)\np1 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_histogram(bins = 12) + \n  labs(title=\"Histogram of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"Frequency\")\np2 &lt;- ggplot(tccb_df, aes(log_TcCB_ppb)) + \n  geom_boxplot() + \n  labs(title=\"Boxplot of TcCB concentration (ppb)\", x=\"Transformed TcCB\", y=\"\")\np3 &lt;- ggplot(tccb_df, aes(sample=log_TcCB_ppb)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(title=\"QQ plot of TcCB concentration (ppb)\", x=\"Theoretical quantiles\", y=\"Sample quantiles\")\ngrid.arrange(p1, p2, p3, ncol=2)\n\n\n\n\n\n\n\n\n\nlibrary(moments)\nskewness(tccb_df$log_TcCB_ppb)\n\n[1] -0.02521078\n\nkurtosis(tccb_df$log_TcCB_ppb)\n\n[1] 3.449717\n\n\n\nshapiro.test(tccb_df$log_TcCB_ppb)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tccb_df$log_TcCB_ppb\nW = 0.99555, p-value = 0.9868\n\n\nTransformation successful - symmetric distribution\n\n\nContinuing on from Example 2 where the transformation chosen is \\(log_e\\), we see that the normal probability plot is approximately linear and all test statistics (for the normality tests) are lower than their corresponding critical values, so we can assume the log-transformed data are normally distributed. (Or equivalently that the original data are log-normally distributed.)\nOn the log scale, the mean is –0.598. So the back-transformed mean is \\(e^{–0.598} = 0.550\\) ppb.\nWhen a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nNote that the geometric mean is usually defined as\n\\(GM = \\left( y_1 \\times y_2 \\times \\ldots \\times y_n \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} y_i \\right)^{\\frac{1}{n}}\\)\nwhich is the same as \\(\\exp(\\bar {y`})\\) where \\(\\bar{y}^{\\prime} = \\frac{1}{n} \\sum_{i=1}^{n} y_i^{\\prime}\\) and \\(y_i^{\\prime} = \\log y_i\\).\nThis can be shown for a simple case involving n = 3 observations:\n\\(\\exp(\\bar {y^{\\prime}}) = \\exp\\left[\\frac{1}{3}(y^{\\prime}_1 + y^{\\prime}_2 + y^{\\prime}_3)\\right]\\)\n\\(\\exp\\left[\\frac{1}{3}(\\log y_1 + \\log y_2 + \\log y_3)\\right]\\) \\(y^{\\prime}_i = \\log y_i\\)\n\\(\\left[\\exp(\\log y_1 + \\log y_2 + \\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{ab}=(e^a)^b = (e^b)^a\\)\n\\(\\left[\\exp(\\log y_1) \\times \\exp(\\log y_2) \\times \\exp(\\log y_3)\\right]^{\\frac{1}{3}}\\) \\(e^{a+b} = e^a \\times e^b\\)\n\\(\\left[y_1 \\times y_2 \\times y_3\\right]^{\\frac{1}{3}} = \\sqrt[3]{y_1 \\times y_2 \\times y_3}\\) \\(e^{\\log a} = a\\) \\(= GM\\)\nJust as the geometric mean is calculated as \\(\\exp(\\bar {y^{\\prime}})\\), some books refer to \\(exp(s^{\\prime})\\) as the geometric standard deviation, where \\(s^{\\prime}\\) is the standard deviation of the \\(y_i^{\\prime} = \\log y_i\\). However, this is not a very useful concept, so it won’t be used here.\nSince we have concluded log TcCB has a normal distribution, then TcCB has a lognormal distribution. If a variable log \\(y = y{\\prime}\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a lognormal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), \\(y \\sim LN(\\mu,\\sigma^2)\\). Note that \\(\\mu\\) and \\(\\sigma^2\\) are the parameters for the log variable. It can be shown (no proof here) the mean and variance for the lognormal \\(y \\sim LN(\\mu,\\sigma^2)\\). distribution are\n\nMean = \\(exp(\\mu+1/2\\sigma^2)\\)\nVariance = \\(exp(2\\mu+\\sigma^2)[exp(\\sigma^2)-1]\\)\n\nWe can illustrate these relationships by using the parameter estimates \\(\\hat \\mu=-0.598\\) and \\(\\hat \\sigma=1.362\\) from the log TcCB data to produce the following fitted normal distributions and lognormal distributions are obtained:\n\n\n\nFig. Normal and Log Normal distributions for TcCB data\n\n\nAlso, using these parameter estimates, the mean and variance of the lognormal distribution are\n\nMean = \\(exp(-0.598+1/2\\times 1.362^2)=1.390\\) ppb\nVariance = \\(exp(2\\times -0.598+1.362^2)[exp(1.362^2)-1]=10.442\\) ppb\nStd. Dev = \\(\\sqrt{\\text{variance}} = \\sqrt{10.422} = 3.228\\) ppb,\n\nwhere -0.598 is the average of the logged data and 1.363 is the standard deviation of the logged data.\nNote the similarity of these to the sample mean (1.412) and sample standard deviation (3.098) of the raw TcCB data.",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#hypothesis-testing",
    "href": "module03/030-ttest1.html#hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Sometimes research questions are framed not as “What is a plausible range of values for such and such a parameter?” but rather “Are the data consistent with this particular value for the parameter?”. A hypothesis test is a test of such a hypothesised value. For example, we may simply wish to test whether the population mean yield of wheat in a particular region is 2 tons per hectare or not.\nStatistical hypothesis tests are based on research questions and hypotheses. Some examples of research questions are:\n\nDoes an increased use of fertilisers of farms in a catchment area result in increased river pollution?\nHow do different crop residue management systems affect the “health” of the soil?\nWhat effect will selective logging have on wildlife populations?\n\nThe diagram below also appears in Section 1. Here we see where statistical hypothesis testing fits into the research process at the point of statistical analysis.\n\n\n\nFig. The research process\n\n\n\n\n\nChoose the level of significance, \\(\\alpha\\) (most commonly \\(\\alpha= 0.05\\), but you will also see 0.01 and 0.10 mentioned regularly)\nWrite the null and alternate hypotheses\nCheck if the assumptions of the test hold (if they don’t - choose an appropriate transformation or choose another test!)\nCalculate the test statistic (& degrees of freedom if applicable)\n\nObtain a P-value OR\nObtain critical values\n\nMake a statistical conclusion by\n\nComparing this P-value to your chosen level of significance (if \\(P &lt; \\alpha\\), then reject null hypothesis) OR\nSeeing if the test statistic lies with the rejection region\n\nWrite a biological conclusion\n\n\n\n\nHypothesis tests about the population mean can take one of the three forms:\n\n\\(H_0: \\mu = c\\) or \\(H_1: \\ne c\\)\n\n\\(H_0: \\mu \\ge c\\) or \\(H_1: \\mu &lt; c\\)\n\n\\(H_0: \\mu \\le c\\) or \\(H_1: \\mu &gt; c\\)\n\nwhere \\(c\\) is a real number chosen before the data are gathered. Each \\(H_0\\) above is tested with a test statistic, and the decision about \\(H_0\\) is based on how far this test statistics deviates from expectation under a true \\(H_0\\). If the test statistic exceeds the critical value(s), \\(H_0\\) is rejected. Alternatively, if the \\(P\\) value for the test statistic is smaller than the predetermined alpha level, \\(H_0\\) is rejected.\nFor any particular experiment only one of the sets of hypotheses is appropriate and can be tested. \\(H_0\\) and \\(H_1\\) are predictions that follow naturally from the question posed and the result anticipated by the researcher. Also, hypotheses contain only parameters (Greek letters) and claimed values, never numbers that come from the sample itself. \\(H_0\\) always contains the equal sign and is the hypothesis that is examined by the test statistic.\nGenerally a) is the form of hypothesis test that we employ. Options b) or c) are used occasionally when we have evidence (quite independent of the data we have collected) to believe that the difference of the hypothesized value from the true population mean, if any, is in one direction only. Note that a one tailed test is NOT appropriate simply because the difference between the samples is clearly in one direction or the other.\n\n\n\nA Type I error (false positive) is made when we reject the null hypothesis when it is true. We might for example declare that a population mean is different from hypothesized value when, in fact, they are not. Equally we may err in the other direction, that is, we may accept a null hypothesis when it is false. We might, for example, fail to detect a difference between the population mean and a hypothesized value. In doing so, we make a Type II error (false negative). The definitions of each of these two errors is summarised in the table below.\n\n\n\nFig. Type I and Type II Error\n\n\nBecause , the level of significance, is chosen by the experimenter, it is under the control of the experimenter and it is known. When you reject and \\(H_0\\), therefore, you know the probability of an error (Type I). If you accept an \\(H_0\\) it is much more difficult to ascertain the probability of an error (Type II). This is because Type II errors depend on many factors, some of which may be unknown to the experimenter. So the rejection of \\(H_0\\) leads to the more satisfying situation because the probability of a mistake is easily quantifiable.\nYou may think that if the level of significance is the probability of a Type I error and is under our control, why not make the level of significance (\\(\\alpha\\) level) very small to eliminate or reduce Type I errors? Why not use 1 in 100 or 1 in 1000? Sometimes we may wish to do that (e.g. in human medical trials), but reduction of the \\(\\alpha\\) level (Type I error) always increases the probability of a Type II error.\nYou can read more about this in Chapter 5 of Glover & Mitchell (2008) or Chapter 5 of Clewer & Scarisbrick (2013).\nGlover, T. and Mitchell, K., 2008. An introduction to biostatistics. Waveland Press.\nClewer, A.G. and Scarisbrick, D.H., 2013. Practical statistics and experimental design for plant and crop science. John Wiley & Sons.\n\n\n\nIdeally, a test of significance should reject the null hypothesis when it is false. Power is the probability of rejecting \\(H_0\\) when \\(H_0\\) is false, \\(1-\\beta\\). A test becomes more powerful as the available data increases.\nYou’ll do more on this topic (including planning experiments and interpreting statistical differences in light of biological importance NEXT YEAR).",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#one-sample-z-tests",
    "href": "module03/030-ttest1.html#one-sample-z-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "The liquid effluent from a chemical manufacturing plant is to be evaluated. The plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\).\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). Assume that the claim is true, i.e. (population) mean is \\(\\mu g/l\\).\nScenario A\nTo test this claim, a single sample of effluent discharge was taken and found to be 0.4 \\(\\mu g/l\\). Does the data support their claim?\nWe want to see how likely it is to get an observation of 0.4 \\(\\mu g/l\\), or something even more extreme. By more extreme, we mean &gt; 0.4 \\(\\mu g/l\\), or &lt; 0.2 \\(\\mu g/l\\) (i.e. more than 0.1 \\(\\mu g/l\\) away from \\(\\mu = 0.3\\), in either direction). This probability is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.2), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.4), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.2, 0.4))\n\n\n\n\n\n\n\n\nSo the probability of this event is\n\\(P(Y&lt;0.2 \\text{ or } Y&gt;0.4)\\) \\(=\\left( Z&lt;\\frac{0.2-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.4-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-1 \\text{ or } Z&gt;1)\\) \\(=2 \\times P(Z&lt;-1)\\) \\(=2 \\times 0.1587 = 0.3174\\)\nThis is a large probability (\\(\\approx\\) 1 in 3), so obtaining a value of 0.4 \\(\\mu g/l\\) is not inconsistent with \\(\\mu = 0.3\\) \\(\\mu g/l\\). There is no reason to reject the hypothesis that the (population) mean is \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nScenario B\nSuppose now that the toxic substance concentration was 0.5 \\(\\mu g/l\\). What is the conclusion now?\nWe now need the probability of &gt; 0.5 \\(\\mu g/l\\), or &lt; 0.1 \\(\\mu g/l\\). This is represented in the sketch below.\n\nlibrary(ggplot2)\nggplot(data.frame(x = c(0.3-4*0.1, 0.3+4*0.1)), aes(x = x)) +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd = 0.1), geom = \"area\", fill = \"white\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3-4*0.1, 0.1), geom = \"area\", \n                fill = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = 0.3, sd=0.1), \n                xlim = c(0.3+4*0.1, 0.5), geom = \"area\", \n                fill = \"black\") +\n  xlab(\"x\") +\n  ylab(expression(N(0.3,0.1^2)~pdf)) +\n  scale_x_continuous(breaks = c(0.1, 0.5))\n\n\n\n\n\n\n\n\nSo the probability of this event is\n\\(P(Y&lt;0.1 \\text{ or } Y&gt;0.5)\\) \\(=\\left( Z&lt;\\frac{0.1-0.3}{0.1} \\text{ or } Z&gt;\\frac{0.5-0.3}{0.1} \\right)\\) \\(=P(Z&lt;-2 \\text{ or } Z&gt;2)\\) \\(=2 \\times P(Z&lt;-2)\\) \\(=2 \\times 0.0228 = 0.0456\\)\nThis is small (less than 1 in 20), so obtaining a concentration of 0.5 \\(\\mu g/l\\) is unlikely, if \\(\\mu = 0.3\\) \\(\\mu g/l\\).\nSo we reject the hypothesis that \\(\\mu = 0.5\\) \\(\\mu g/l\\) and conclude that the (population) mean is significantly higher than 0.3 \\(\\mu g/l\\).\nHOWEVER, in reality you would NOT make a recommendation based on this conclusion as it is based on a single value! You want to base your decision on a much larger sample.\nScenario C\nContinuing the liquid effluent example, recall the plant operators claim the mean concentration of a specific toxic substance is 0.3 \\(\\mu g/l\\). Now let’s say that to test this claim, six effluent discharge samples were taken at randomly chosen times and the resultant readings were 0.48 0.25 0.29 0.51 0.49 0.40. Does the data support their claim?\nLet y = toxic substance concentration (\\(\\mu g/l\\)). Assume data are normally distributed: \\(y \\sim N(\\mu, \\sigma^2)\\). Suppose (for the time being) we have prior information on the variability of similar operating plants, and can assume that \\(\\sigma = 0.1\\) \\(\\mu g/l\\). As we know the population standard deviation, \\(\\sigma\\), we will use a z-test.\nNull hypothesis: \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) Alternate hypothesis: \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\) where \\(\\mu\\) = mean toxic substance concentration\n\\(z=\\frac{\\bar{y}-\\mu}{\\sqrt{\\sigma^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nTest Statistic,  \n\\(z=\\frac{0.403-0.3}{\\sqrt{0.1^2/6}}=2.53\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then z = 2.53 is an observation from a standard normal distribution.\nWe now calculate the probability of obtaining this z-value, or something more extreme. This is the P value of the test:\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(Z \\le -2.53 \\text{ or } Z \\ge 2.53)\\) \\(=2 \\times P(Z \\le -2.53)\\) \\(=2 \\times 0.0057=0.011\\)\n\n2*pnorm(-2.53)\n\n[1] 0.01140625\n\n\n\n\n\nplot of \\(P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\) and \\(P(Z \\le -2.53 \\text{ or } \\bar{y} \\ge 2.53)\\)\n\n\nIf \\(H_0\\) is true, there is only a 1.1% chance of obtaining this value of or something more extreme. This is unlikely, so we reject the null hypothesis. Hence we conclude that the toxic substance concentration in the effluent has a mean significantly greater than 0.3 \\(\\mu g/l\\).",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#general-notes-on-hypothesis-testing",
    "href": "module03/030-ttest1.html#general-notes-on-hypothesis-testing",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Often researchers choose their level of significance (\\(\\alpha\\)) as 0.05. In that case…\n\nIf \\(P&lt;0.05\\) (less than 1 in 20) \\(\\Rightarrow\\) reject \\(H_0\\)\nIf \\(P&lt;0.05\\) (more than 1 in 20) \\(\\Rightarrow\\) retain \\(H_0\\)\n\nIf \\(H_0\\) is retained, this does not necessarily mean that \\(H_0\\) is true; the sample may be too small to detect a difference.\nEven though \\(H_0\\) might be rejected, there is a small chance that this will be in error. If you use a 5% cut off rule, 5% of your conclusions will be wrong when \\(H_0\\) is true!",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#one-sample-t-tests",
    "href": "module03/030-ttest1.html#one-sample-t-tests",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "For the toxic substance concentration in effluent example (with the 6 readings), now we will not make any assumption about the variability (i.e. we assume we don’t know sigma). How would the analysis change?\nAs before the null and alternate hypotheses are, \\(H_0: \\mu = 0.3\\) \\(\\mu g/l\\) vs. \\(H_1: \\mu \\ne 0.3\\) \\(\\mu g/l\\). From the data we calculate the sample mean and sample standard deviation to use in the construction of the test statistic, t. Here, \\(\\mu=0.403\\) \\(\\mu g/l\\) and \\(s = 0.111\\) \\(\\mu g/l\\).\nThe test statistic, t, is calculated using the following formula:\n\\(t=\\frac{\\bar{y}-\\mu}{\\sqrt{s^2/n}}=\\frac{\\bar{y}-\\mu}{se(\\bar{y})}\\)\nand the associated degrees of freedom as follows: degrees of freedom, \\(df = n-1\\).\nSo in the current example,\n\\(t=\\frac{0.403-0.3}{\\sqrt{0.111^2/6}}=2.29\\) and \\(df=6-1=5\\)\nAssuming that the null hypothesis is true (that \\(\\mu = 0.3\\) \\(\\mu g/l\\)), then t = 2.29 is now an observation from a t distribution with \\(n - 1 = 5\\) degrees of freedom.\nThe P-value for this t-test is\n\\(P=P(\\bar{y} \\le 0.197 \\text{ or } \\bar{y} \\ge 0.403)\\)\n\\(=P(T_5 \\le -2.29 \\text{ or } T_5 \\ge 2.29)\\) \\(=2 \\times P(T_5 \\le -2.29)\\) \\(=2 \\times 0.035=0.071\\)\nWe can look -2.29 up in the “old school” t-tables or we can use the pt function in R to calculate P.\n\n2*pt(-2.29,5)\n\n[1] 0.07064936\n\n\nThis time, the P-value is greater than 0.05, so we cannot reject \\(H_0\\). We can say that the data are consistent with the mean concentration of the toxic substance being 0.3 \\(\\mu g/l\\).\nRather than calculate the probabilities by hand, we can use R’s t.test command to run the test:\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  toxic\nt = 2.2891, df = 5, p-value = 0.07073\nalternative hypothesis: true mean is not equal to 0.3\n95 percent confidence interval:\n 0.2872928 0.5193739\nsample estimates:\nmean of x \n0.4033333 \n\n\nFrom the output we can see that we can see that\nt = 2.2891; df = 5; p-value = 0.07073\nOur conclusion is as above.\n\n\n\n\n\n\nt distribution versus z distribution\n\n\nThe t distribution has “heavier” tails than the normal distribution.\nAs degrees of freedom \\(\\uparrow\\), t \\(\\rightarrow\\) normal distribution.\nThe P-value for the t-test is larger than that for the z-test \\(\\therefore\\) the t-test is not as powerful. This is because some information must be used to estimate \\(\\sigma\\).",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "href": "module03/030-ttest1.html#steps-in-hypothesis-testing-via-a-confidence-interval",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Hypothesis testing via a t-based confidence interval is an alternative to conducting a one-sample t-test (via test statistic, df, and P-value). The same assumptions apply as for a t-test.\n\nWrite the null and alternate hypotheses.\nCheck if the assumptions of the test hold.\nCalculate the confidence interval.\nCheck whether the hypothesised value / mean lies within the confidence interval.\nMake a statistical conclusion. (If the hypothesized value / mean does not lie within the confidence interval, reject the null hypothesis.)\nWrite a biological conclusion.\n\n\ntoxic &lt;- c(0.48, 0.25, 0.29, 0.51, 0.49, 0.40)\nt.test(toxic, mu = 0.3, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  toxic\nt = 2.2891, df = 5, p-value = 0.07073\nalternative hypothesis: true mean is not equal to 0.3\n95 percent confidence interval:\n 0.2872928 0.5193739\nsample estimates:\nmean of x \n0.4033333 \n\n\nWe can see that the 95% confidence intervals are also provided in the R output above and that our hypothesised mean of 0.3 \\(\\mu g/l\\) is contained within (between) the CI’s.\nWe conclude that the true mean toxic substance concentration does not differ significantly from 0.3 \\(\\mu g/l\\), and that we are 95% confident that this unknown true mean value lies within the range 0.2873 to 0.5194 \\(\\mu g/l\\).\nPerforming a hypothesis test using a 95% confidence interval is equivalent to performing a t-test with a 5% level of significance – the conclusions drawn will be the same. Similarly the conclusions from a 90% CI and a t-test with \\(\\mu = 0.10\\) will be the same. Some journals prefer us to report the CI’s as they are more informative than the p-value alone. For example, the width of the CI’s says something about the precision of the estimate.",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module03/030-ttest1.html#one-sample-t-test-with-data-transformation",
    "href": "module03/030-ttest1.html#one-sample-t-test-with-data-transformation",
    "title": "One-sample \\(t\\)-test",
    "section": "",
    "text": "Example\nThere is evidence that total nitrogen levels in the river – like many other environmental quality data – are lognormally distributed. Consequently, it is more convenient to work on the logarithmic scale. For example,\nTN = log10[total nitrogen concentration]\nwhere the nitrogen concentration is measured in \\(\\mu g/l\\). (Often scientists will find it more convenient to use the \\(\\log_{10}\\) scale rather than \\(\\log_e\\), but this is of no real consequence).\nData from 29 observations of Total Nitrogen levels in the Nepean River @ Wallacia downloaded using Water NSW water insights API and can be found in the data set TN_Wallacia.csv. We are interested to test whether total nitrogen concentration differs significantly from the preferred water quality target of 500 ppb (note that ppm and \\(\\mu g/l\\) are equivalent). Nitrogen content would ideally be equal to or less than this target to reduce the risk of significant eutrophication.\nBe sure to include the following elements in your statistical test:\n\nnull and alternate hypotheses;\nconsideration of the analysis assumptions;\nmean and confidence interval on the original measurement scale;\nbiological conclusion of the test output including the confidence interval.\n\nSolution\nWe wish to perform a one-sample t-test to test the null hypothesis \\(H_0: \\mu = 500\\) \\(\\mu g/l\\). However to do this we need to be able to assume the data follows a normal distribution. A quick summary of the raw data shows that the data is skewed to the right (see boxplot below, also the mean of 855.9 \\(\\mu g/l\\) is greater than the median of 800 \\(\\mu g/l\\)) and we also see the typical “smiley” shape of the points on the qq normal plot. We also find that the skewness value of 1.10 is positive and greater than one. The Shapiro-Wilks normality test indicates non-normality at the 5% significance level (as the p-value &lt; 0.05).\n\ntn &lt;- read.csv(\"TN_Wallacia.csv\")\nsummary(tn$TN)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  310.0   610.0   800.0   855.9  1020.0  2130.0 \n\n\n\nlibrary(moments)\nskewness(tn$TN)\n\n[1] 1.078249\n\n\n\nshapiro.test(tn$TN)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tn$TN\nW = 0.92582, p-value = 0.04293\n\n\n\nggplot(tn, aes(sample = TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(tn, aes(x=TN)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nTransform data\nA log (base 10) transformation on the raw data was performed as suggested.\n\ntn$log10_TN &lt;- log10(tn$TN)\nmean(tn$log10_TN)\n\n[1] 2.886528\n\n\nWe can use the mean of this transformed data i.e. the mean transformed TN value (= 2.886528) to find the estimated geometric mean (GM) of the phosphorus levels. The \\(GM = 10^2.886528 = 770.066\\) \\(\\mu g/l\\). This is an indication of a typical phosphorus reading. Note how this is lower than the arithmetic mean.\nRecall from earlier that when a log-transformation is used, the back-transformed mean is known as the geometric mean (as opposed to the ordinary arithmetic mean). This measure is commonly used in describing environmental data, as in many cases data are highly positively skewed. Like the median, the geometric mean is less sensitive (or more robust) to these outlying values.\nAgain before proceeding with the t-test or obtaining a t-based CI, we need to perform a normal probability test on the log-transformed values, TP to test whether these log values can be assumed to follow a normal distribution.\n\nggplot(tn, aes(sample = log10_TN)) +\n  stat_qq() + # This adds the QQ plot points\n  stat_qq_line() + # This adds the QQ line\n  ggtitle(\"Normal Q-Q Plot\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nshapiro.test(tn$log10_TN)\n\n\n    Shapiro-Wilk normality test\n\ndata:  tn$log10_TN\nW = 0.97279, p-value = 0.6375\n\n\nBased on the qq-normal plot and the Shapiro Wilks test (p&gt;0.05), we can assume that the transformation has been successful.\nWe perform the t test on the log scale (using the newly generated data) where our null and alternate hypotheses are in effect:\n\n\\(H_0: \\mu_A = log_{10} 500\\) \\(mg/l\\)\n\\(H_1: \\mu_A \\ne log_{10} 500\\) \\(mg/l\\)\n\nwhere \\(\\mu_A\\) is the population arithmetic mean.\nHence the test statistic will be\n\\(t=\\frac{\\bar{y} \\text{ (of log10 data)}-\\log_{10}(500)}{s \\text{ (of log10 data)}/\\sqrt{n}}\\) and \\(df = n-1\\).\nLet’s test this in R\n\nt.test(tn$log10_TN, mu = log10(500), alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  tn$log10_TN\nt = 4.8768, df = 28, p-value = 3.884e-05\nalternative hypothesis: true mean is not equal to 2.69897\n95 percent confidence interval:\n 2.807748 2.965309\nsample estimates:\nmean of x \n 2.886528 \n\n\nThe P-value of &lt;0.001 indicates that we should reject \\(H_0\\). R also produces the CI (2.807748, 2.965309) with its t-test output. Confirming the rejection of \\(H_0\\) is the fact that the test mean of 2.69897 (\\(log_{10}500\\)) lies outside (and below) these confidence limits. Therefore we can conclude that the (population) geometric mean phosphorus concentration is significantly higher than 500 \\(\\mu g/l\\) and is therefore exceeding the water quality target.\nTo obtain the 95% confidence interval for the geometric mean, we need to back transform both limits of the CI given by R (above) which are on the \\(\\log_{10}\\) scale. The 95% CI for the mean TN is 2.807748 to 2.965309. So the 95% CI for the geometric mean phosphorus level is \\(10^{2.807748}\\) to \\(10^{2.965309}\\).\nSo the 95% CI for the geometric mean phosphorus level is 642.31 to 923.23 \\(\\mu g/l\\). The best estimate of the true geometric mean is 770.066 \\(\\mu g/l\\). However, the true value may be in the range 642.31 to 923.23 \\(\\mu g/l\\) with 95% certainty.",
    "crumbs": [
      "**MODULE 3**",
      "One-sample $t$-test"
    ]
  },
  {
    "objectID": "module01/021-probability_distributions.html",
    "href": "module01/021-probability_distributions.html",
    "title": "Probability distributions",
    "section": "",
    "text": "Biological systems are variable, and we consequently need statistical methods to cope with this variability in drawing valid inferences from our data. As a consequence of this variability, we can never say anything with absolute certainty based on biological data: there will always be some “doubt” due to chance variation. Not surprisingly, we need the tools and language of probability to assist us with quantifying our level of “doubt” when we make our conclusions.\nWe won’t deal with probability theory to any great degree in this unit of study, but just outline some of the key concepts HSC Maths concepts that we will build on.\n\n\nSimple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\n\n\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\nComplementary Events\nProbability of an event not occurring = 1 – probability of event occurring.\n\\(P(E) =1− P(E)\\)\nExercises\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) – P(A \\text{ and } B)\\)\nExercise\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\nProduct Rule\nWhen we do more than one thing (e.g. toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.\n\n\n\n\nProbability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.\n\n\n\nJust as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g. normal distribution, or discrete e.g. binomial distribution. In this unit of study we will only consider continuous distributions.\n\n\n\nFor discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g. the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don’t mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nFigure 3.1 The symmetric bell shaped distribution – just one distribution in the family of continuous distributions.\n\n\n\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ≤ Y ≤ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nFigure 3.2 The shaded area under the PDF curve for a normal distribution is interpreted as the probability of a value occurring within a defined range.\n\n\n\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student’s T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these… If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = 'Standard Normal')\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = 'Log-Normal')\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) + \n  geom_line() + \n  facet_wrap(~Distribution, scales = 'free_x') + \n  theme_minimal() + \n  labs(title = \"Log-Normal PDF vs. Normal PDF\", \n       x = \"Value\", \n       y = \"Density\")\n\n\n\n\nThe normal and log normal distributions\n\n\n\n\n\n\n\nWe began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the “bell-shaped” distribution commonly observed in histograms of biological and environmental data e.g. height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at μ and σ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ’the variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e. \\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g. \\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e. probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e. 50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n[1] 0.5\n\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n[1] 0.9678432\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1-pnorm(1.85)\n\n[1] 0.03215677\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\\(P(–1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; –1) \\approx  0.1587\\) (from R)\n\\(P(–1 &lt; Z &lt; 2) \\approx  0.9772 – 0.1587 \\approx  0.8185\\)\n\npnorm(2)\n\n[1] 0.9772499\n\npnorm(-1)\n\n[1] 0.1586553\n\npnorm(2) - pnorm(-1)\n\n[1] 0.8185946\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e. \\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n[1] 0.02275013\n\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n[1] 1.071029\n\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n[1] 1.728971\n\n\n\n\n\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we’ve obtained these probabilities we’ll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‘z’; ‘PDF via formula’; ‘PDF via NORMDIST’.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we’re going to obtain the corresponding probabilities in 2 different ways – you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don’t need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution…\n\n\n\nNormality tests are the first introduction you’ll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe’ll expand these steps soon…\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e. whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\n\n    Shapiro-Wilk normality test\n\ndata:  TcCB$TcCB\nW = 0.40034, p-value &lt; 2.2e-16\n\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "**MODULE 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/021-probability_distributions.html#probability",
    "href": "module01/021-probability_distributions.html#probability",
    "title": "Probability distributions",
    "section": "",
    "text": "Simple Probability\nProbability of an event occurring:\n\\(P(E)=\\frac{\\text{Number of ways an event can occur}}{\\text{Total number of possible outcomes}}\\)\n\n\\(P(E) = 0\\) the event is impossible\n\\(P(E) = 1\\) the event is certain (must happen)\n\nExample: A person is chosen at random to write about his/her favourite sport. Thirty-five people like tennis, 51 like cricket, 17 like squash, 23 like baseball and 62 like swimming. Find the probability that the article will be about:\n\nswimming: \\(\\frac{62}{188}=\\frac{31}{94}\\)\nsquash or tennis: \\(\\frac{17+35}{188}=\\frac{52}{188}=\\frac{13}{47}\\)\n\nComplementary Events\nProbability of an event not occurring = 1 – probability of event occurring.\n\\(P(E) =1− P(E)\\)\nExercises\n\nThe probability of rain on the 23rd January each year is \\(\\frac{17}{53}\\). What is the probability of no rain on the 23rd January 2007?\nThe probability of a seed producing a red flower is \\(\\frac{7}{8}\\). Find the probability of the flower producing a different colour.\n\nNon-Mutually Exclusive Events\nNon-mutually exclusive events have some overlap - more than one thing can happen at the same time.\n\\(P(A \\text{ or } B) = P(A) + P(B) – P(A \\text{ and } B)\\)\nExercise\n\nIn a group of 20 people, 14 like to watch the news on television and 17 like to watch old movies. Everyone watches one or the other or both. If I choose one person at random, find the probability that the person likes watching:\n\n\nBoth the news and the old movies;\nOnly the news.\n\nProduct Rule\nWhen we do more than one thing (e.g. toss 2 coins, plant 5 seeds, choose 3 people, throw 2 dice) we multiply the probabilities together.\n\\(P(A and B) = P(A).P(B)\\)\nExercises\n\nA box contains 3 black pens, 4 red pens, and 2 green pens. If I draw out 2 pens at random, find the probability that they are both red.\nThe probability of a seed germinating is 0.91. If I plant 5 seeds, find the probability that they all germinate.",
    "crumbs": [
      "**MODULE 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/021-probability_distributions.html#probability-density-functions",
    "href": "module01/021-probability_distributions.html#probability-density-functions",
    "title": "Probability distributions",
    "section": "",
    "text": "Probability density functions (PDFs) are a way of mathematically describing the shape of distributions. Examples:\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nPoisson: \\(P(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nNormal: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nFor continuous distributions we define the area under the curve as the probability. This area is equal to 1 or 100%.",
    "crumbs": [
      "**MODULE 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/021-probability_distributions.html#types-of-distributions",
    "href": "module01/021-probability_distributions.html#types-of-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "Just as there are different types of data (continuous, discrete etc.), there are different types of statistical distributions. Statistical distributions are generally categorized as either continuous e.g. normal distribution, or discrete e.g. binomial distribution. In this unit of study we will only consider continuous distributions.",
    "crumbs": [
      "**MODULE 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/021-probability_distributions.html#probability-for-continuous-distributions",
    "href": "module01/021-probability_distributions.html#probability-for-continuous-distributions",
    "title": "Probability distributions",
    "section": "",
    "text": "For discrete variables, it makes sense to talk about the probability of a specific outcome occurring, e.g. the probability of exactly three insects caught. However, for continuous variables, this is more problematic.\nExample:\nConsider the gestational period of cattle measured in days. What is the probability that it is exactly 295 days long? We don’t mean in the range 295-296 days, or 294.9999 to 295.0001 days, but exactly 295 days. Clearly, this probability must be infinitesimally small - effectively zero!\nThe way around this is to talk about the probability of getting a value within a range. For example, if Y represents gestational length, we might want the probability that it is between 285 and 305 days long, \\(P(285 \\le Y \\le 305)\\), or at least 295 days long, \\(P(Y \\ge 295)\\).\nWe summarise the probability distribution of a statistical distribution by means of a probability density function (PDF), which we graph against the outcome, Y. The PDF for gestational length might show the shape below.\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nFigure 3.1 The symmetric bell shaped distribution – just one distribution in the family of continuous distributions.\n\n\n\n\nWe interpret the area under the curve as the probability. Further, the total area under a curve is 1. For example, the probability of sampling a gestational length of between 285 and 305 days, \\(P(285 ≤ Y ≤ 300)\\) is:\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 284.3\nsd &lt;- 5.52\n\n# Define the range for the shaded area\nlower_bound &lt;- 285\nupper_bound &lt;- 300\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\nFigure 3.2 The shaded area under the PDF curve for a normal distribution is interpreted as the probability of a value occurring within a defined range.\n\n\n\n\nThere are other continuous distributions other than the commonly cited normal distribution. The continuous distributions you are likely to encounter during your undergraduate degree are: normal, student’s T, chi square, F, log normal, exponential, gamma.\nHighlighting just one of these… If a variable \\(\\log{y} = y'\\) has a normal \\(N(\\mu,\\sigma^2)\\) distribution, then the original variable has a log normal distribution.\n\nlibrary(ggplot2)\n\n# Data for standard normal distribution\nx_norm &lt;- seq(-5, 5, length.out = 1000)\ny_norm &lt;- dnorm(x_norm)\n\n# Data for log-normal distribution\nx_lognorm &lt;- seq(0.01, 3, length.out = 1000) # Avoid starting at 0 to prevent log(0)\ny_lognorm &lt;- dlnorm(x_lognorm)\n\n# Data frame for standard normal\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm, Distribution = 'Standard Normal')\n\n# Data frame for log-normal\ndf_lognorm &lt;- data.frame(x = x_lognorm, y = y_lognorm, Distribution = 'Log-Normal')\n\n# Combine data frames\ndf &lt;- rbind(df_norm, df_lognorm)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = Distribution)) + \n  geom_line() + \n  facet_wrap(~Distribution, scales = 'free_x') + \n  theme_minimal() + \n  labs(title = \"Log-Normal PDF vs. Normal PDF\", \n       x = \"Value\", \n       y = \"Density\")\n\n\n\n\nThe normal and log normal distributions",
    "crumbs": [
      "**MODULE 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/021-probability_distributions.html#the-normal-distribution",
    "href": "module01/021-probability_distributions.html#the-normal-distribution",
    "title": "Probability distributions",
    "section": "",
    "text": "We began speaking about the normal distribution in Section 2.5.1. Recall that it is also sometimes referred to as the Gaussian distribution (named after a man who contributed significantly to this area of mathematics). This is the “bell-shaped” distribution commonly observed in histograms of biological and environmental data e.g. height, weight, gestation lengths, etc. It is central to most statistical theory.\n The centre of the curve is located at μ and σ indicates the spread or width of the curve. For all distributions, a type of shorthand has been introduced to denote the name of the distribution that a particular variable, \\(y\\), follows. For example, you should read the abbreviation \\(y \\sim N(\\mu,\\sigma^2)\\) as ’the variable \\(y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nAs we will discover later, for data that follows a normal distribution we expect that 95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations. This is the basis for the following approximations (that you may already be familiar with):\n\n68% of data lie within \\(\\pm 1 \\sigma \\text{ of } \\mu\\)\n95% of data lie within \\(\\pm 2 \\sigma \\text{ of } \\mu\\)\n\nRecall that if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\nwhere \\(\\sigma\\) is the population standard deviation and \\(\\mu\\) is the population mean.\nThe \\(N(0,1)\\) distribution \\((\\mu = 0, \\sigma^2 = 1)\\) is called the standard normal distribution, usually termed \\(Z\\), i.e. \\(Z \\sim N(0,1)\\). Probability tables (including standard normal probability tables) are published in most statistical texts. They show the proportions of data found below a value in the distribution. For the normal distribution, only probabilities for the \\(N(0,1)\\) distribution are tabulated. For other normal distributions e.g. \\(N(20,5)\\) the probabilities are obtained by calculating the standardised value:\n\\(Z=\\frac{y-\\mu}{\\sigma}\\)\nIf we substitute \\(\\sigma = 1\\) and \\(\\mu = 0\\) into the PDF for the normal, we find that the PDF for the standard normal distribution is\n\\(f(z)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\n\nTo calculate probabilities in the standardised normal distribution, remember that the values given in the CDF are cumulative probabilities - i.e. probabilities of values of z occurring below a particular value. For example, looking at the pnorm(0) we see the probability associated with a Z value of 0 is 0.5. This means that half the values are below 0 (i.e. 50%). Using R - if we want to know what the probability of obtaining a value greater than the point of interest, then we subtract probability of obtaining a value less than point of interest from 1 (the total area under the curve). To find the probability of a value occurring between two points, subtract the probability of being less than the lower value from the probability of being less than the upper value. The easiest way to understand this is to draw the curve showing the area required, as in the figures below.\n\npnorm(0)\n\n[1] 0.5\n\n\n\n\n\nStandardized Normal Values\n\n\nExamples\n\n\n\n\\(P(Z &lt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 1.85) = 0.9678\\)\n\npnorm(1.85)\n\n[1] 0.9678432\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -4\nupper_bound &lt;- 1.85\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\\(P(Z &gt; 1.85)\\) where \\(Z \\sim N(0, 1)\\)\n\\(1 - P(Z &lt; 1.85) = 0.0322\\)\n\n1-pnorm(1.85)\n\n[1] 0.03215677\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- 1.85\nupper_bound &lt;- 4\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\\(P(–1 &lt; Z &lt; 2)\\) where \\(Z \\sim N(0, 1)\\)\n\\(P(Z &lt; 2)  \\approx  0.9772\\) (from R)\n\\(P(Z&lt; –1) \\approx  0.1587\\) (from R)\n\\(P(–1 &lt; Z &lt; 2) \\approx  0.9772 – 0.1587 \\approx  0.8185\\)\n\npnorm(2)\n\n[1] 0.9772499\n\npnorm(-1)\n\n[1] 0.1586553\n\npnorm(2) - pnorm(-1)\n\n[1] 0.8185946\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 0\nsd &lt;- 1\n\n# Define the range for the shaded area\nlower_bound &lt;- -1\nupper_bound &lt;- 2\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nSuppose that from long term studies, it is known that the gestation length of cattle (in days) is normally distributed with a mean of 285 days and a standard deviation of 10 days i.e. \\(y \\sim N(285, 10^2)\\). The following is a plot of the theoretical distribution (PDF).\n\nlibrary(ggplot2)\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n# Create a sequence of x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\n# Create a ggplot\nggplot(data.frame(x_values), aes(x = x_values)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd)) +\n  labs(title = \"PDF\", x = \"Gestation Period (days)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nApproximately 68% of data lie within \\(\\pm 1 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(275\\) to \\(295\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.68 that it is between 275 days and 295 days in duration.\nApproximately 95% of data lie within \\(\\pm 2 \\sigma\\) of \\(\\mu\\), i.e., \\(285 \\pm 10 =\\) \\(265\\) to \\(305\\) days. So if a pregnancy is selected at random, there is a probability of approximately 0.95 that it is between 265 and 305 days in duration.\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 275\nupper_bound &lt;- 295\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Load the ggplot2 library\nlibrary(ggplot2)\n\n# Define the mean and standard deviation\nmean &lt;- 285\nsd &lt;- 10\n\n# Define the range for the shaded area\nlower_bound &lt;- 265\nupper_bound &lt;- 305\n\n# Create a data frame for the x values\nx_values &lt;- seq(mean - 4*sd, mean + 4*sd, length.out = 1000)\ndf &lt;- data.frame(x = x_values)\n\n# Create the ggplot object\nggplot(df, aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = mean, sd = sd), colour = \"blue\") +\n  geom_area(stat = \"function\", fun = dnorm, args = list(mean = mean, sd = sd),\n            xlim = c(lower_bound, upper_bound), fill = \"blue\", alpha = 0.2) +\n  labs(title = \"PDF\", x = \"Value\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\nAssume that cabbage yields are known to be normally distributed with a mean \\(\\mu = 1.4\\) kg / plant, and a standard deviation \\(\\sigma = 0.2\\) kg / plant.\nFind \\(P(Y &lt; 1)\\) where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\n\npnorm(1, mean = 1.4, sd = 0.2)\n\n[1] 0.02275013\n\n\n\n\n\nFind the 5th and 95th percentile of cabbage yield Y, where we assume that \\(Y \\sim N(1.4, 0.2^2)\\).\nNow we are looking for the points on the x-axis given the probability (rather than finding a probability as we have to date).\nWe can use the qnorm function to find the quantiles.\n\nqnorm(0.05, mean = 1.4, sd = 0.2) # 5th percentile\n\n[1] 1.071029\n\n\n\nqnorm(0.95, mean = 1.4, sd = 0.2) # 95th percentile\n\n[1] 1.728971\n\n\n\n\n\n\nThere a many examples above of using ggplot to draw the normal distribution. However, you can also use Excel to draw the normal distribution. This is a useful skill to have as you can use Excel to draw the normal distribution for any mean and standard deviation, not just the standard normal distribution.\nPlotting the standard normal density function in Excel\nRecall that the probability density function (PDF) for the normal distribution is\n\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\)\n\nFor the case of the standard normal distribution \\(Z \\sim N(0,1)\\) the formula becomes\n\n\\(f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}z^2}\\)\n\nWe will evaluate this function over the range -3 &lt; Z &lt; 3. That is, we will substitute various values of z (between -3 and +3 in steps of 0.1) into the formula above to obtain their corresponding probability densities.\nOnce we’ve obtained these probabilities we’ll plot them on the y-axis and the z values on the x-axis to create our own bell-shaped normal curve in Excel.\n\nInstructions:\n\nIn cells A1:C1 type the following column headings: ‘z’; ‘PDF via formula’; ‘PDF via NORMDIST’.\nIn cells A2:A62 create a column of z values than range from -3 to +3 in incremental steps of 0.1. Type -3 in A2 and -2.9 in A3, highlight both cells and drag down with the black cross which should appear once you put the cursor near the bottom right hand corner.\n\nNow we’re going to obtain the corresponding probabilities in 2 different ways – you should get exactly the same answers.\n\nIn cell B2, enter the formula for the standard normal distribution using a cell references for z. Pick up the + at the bottom right hand corner and drag the mouse (left hand button) to drag this formula down the column to obtain the rest of the answers. You don’t need to re-enter the formula in each row.\nSome Excel functions for entering the formula are\n\n\n\n\nMathematical symbol/function\nExcel function\n\n\n\n\ne2\n=EXP(2)\n\n\n\\(\\pi\\)\n=PI()\n\n\n\\(\\sqrt{4}\\)\n=SQRT(4)\n\n\n\\(2^2\\)\n=2^2\n\n\n\n\nIn cell C2, insert the Excel function NORM.DIST and fill in the arguments [Remember the standard normal density function has a mean of 0 and variance of 1]. There is another shortcut method to apply this formula for Z from -3 to +3 in one step. Point the mouse to the bottom right hand corner of C2; the mouse will change shape to +; then simply double click on the + and the formula will automatically be filled down to as many cells as are not empty alongside.\nTidy up the formatting of your spreadsheet by centering columns and individual cells, and bolding important labels.\nUse the menus to plot the standard normal distribution, Insert &gt; Scatter &gt; Scatter with Smooth Lines. The screenshot below should help you.\n\n\n\n\nExcel\n\n\nConsider how you might do the above exercise for a non-standard normal distribution…\n\n\n\nNormality tests are the first introduction you’ll have to formal statistical hypothesis testing!\nFor every hypothesis test you (or the computer) need to perform the following steps (as a minimum):\n\nSet null & alternate hypotheses;\nCalculate test statistic;\nObtain P-value and or critical value;\nDraw a conclusion about your null hypothesis from the P-value (or by comparing the test statistic with the critical value).\n\nWe’ll expand these steps soon…\nThere are quite a few normality tests that statisticians have developed over the years. We will focus on one of these. The Shapiro-Wilk test is a test of the null hypothesis that the data is normally distributed. The test statistic is calculated using the data and then using this test statistic, a probability value (P-value) is obtained. From the P-value we make a decision whether or not to reject the null hypothesis (i.e. whether or not to reject the normality assumption).\nWe will be using tetrachlorobenzene levels (TcCB, ppb) in 100 soil samples as our example data set to show how R performs the test. We have seen previously that the distribution of this data is HIGHLY POSITIVELY skewed.\n\n# Load the data\nTcCB &lt;- read.csv(\"TcCB.csv\")\n\n# Perform the Shapiro-Wilk test\nshapiro.test(TcCB$TcCB)\n\n\n    Shapiro-Wilk normality test\n\ndata:  TcCB$TcCB\nW = 0.40034, p-value &lt; 2.2e-16\n\n\nThe null hypothesis is that the data is normally distributed. The P-value is 0.0001, which is less than 0.05. Therefore, we reject the null hypothesis and conclude that the data is not normally distributed.",
    "crumbs": [
      "**MODULE 1**",
      "Probability distributions"
    ]
  },
  {
    "objectID": "module01/020-exploring_data.html",
    "href": "module01/020-exploring_data.html",
    "title": "Exploratory Data Analysis EDA",
    "section": "",
    "text": "The summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g. sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).\n\n\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (“mu”) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (“sigma”) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read “y bar”) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)’s to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\n[1] 5\n\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the “middle” value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read “y tilde”) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\n[1] 5.5\n\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\n[1] 6\n\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g. height) or discrete (e.g. number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1   # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3   # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(dnorm(x_values, mean_value, std_dev_A), \n        dnorm(x_values, mean_value, std_dev_B), \n        dnorm(x_values, mean_value, std_dev_C)),\n  curve = factor(c(rep(\"A\", length(x_values)), \n                   rep(\"B\", length(x_values)), \n                   rep(\"C\", length(x_values)))\n  )\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(title = \"Normal Distributions with Different Spreads\",\n       x = \"Value\",\n       y = \"Density\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"),\n                     labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")) +\n  theme_minimal()\n\n\n\n\nFigure 1.1 A, B & C are distributions with the same mean (zero) but varying standard deviations.\n\n\n\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more “spread” out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or “spread” is the range – the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you’ve taken. [It’s unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y)-min(y)\n\n[1] 7\n\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the “spread”. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n 75% \n11.5 \n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25) \n\n75% \n 10 \n\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g. body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n“The degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e. the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.”\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]\n\n\n\n\n\n\nTables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5),  # Assuming a Poisson distribution for count data \n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nTreatment\nMeanSedgePlants\n\n\n\n\nControl\n5.060\n\n\nHigh\n20.050\n\n\nLow\n9.955\n\n\nMedium\n14.860\n\n\n\n\n\n\n\n\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e. \\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e. \\(\\Sigma^r_{i=1}f_i/n \\times 100\\)\n\n\n\n\n\n\nFrequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count)/sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\n\n\n\nFigure 1.2 Relative frequency histogram of sedge plant data.\n\n\n\n\nContinuous measurements are free to take any whole or fractional number within their range e.g. plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\n\n\n\nFigure 1.3 Frequency histogram of creeping bentgrass data.\n\n\n\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ≈ 95 mm (the centre of the data) and standard deviation ≈ 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e. a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e. 30 or more data values).\n\n\n\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) “plugs” were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\n\n\n\n\n\n\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g. Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.500   9.575  10.500  10.440  11.375  12.800 \n\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the “box”\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a “*” or \\(\\bullet\\).\n\nStep 3: Determining the “whisker” lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e. to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e. to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\n\n\n\n\n\n\n\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.\n\n\n\n\n\n\nThis is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we’ll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\n Root_length_mm  \n Min.   : 65.00  \n 1st Qu.: 86.75  \n Median : 93.00  \n Mean   : 93.86  \n 3rd Qu.:102.00  \n Max.   :135.00  \n\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the “x” at the right hand side of the graph. The “4” indicates that this is the 4th observation in the data set i.e. the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1+p2\n\n\n\n\nFigure 1.5 Boxplot (left) and histogram (right) of the creeping bentgrass data show symmetry.\n\n\n\n\n\n\n\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n    TcCB_ppb     \n Min.   : 0.010  \n 1st Qu.: 0.235  \n Median : 0.570  \n Mean   : 1.412  \n 3rd Qu.: 1.292  \n Max.   :26.600  \n\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1+p2\n\n\n\n\nFigure 1.6 Boxplot (left) and histogram (right) of the TcCB data showing positive skewness.\n\n\n\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n   AgeAtOnset    \n Min.   : 3.800  \n 1st Qu.: 7.225  \n Median :11.150  \n Mean   : 9.714  \n 3rd Qu.:12.000  \n Max.   :13.200  \n\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1+p2\n\n\n\n\nFigure 1.7 Boxplot (left) and histogram (right) of arthritis data showing negative skewness.\n\n\n\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\n\nRoot_length_mm \n    0.08475045 \n\nskewness(tccb)\n\nTcCB_ppb \n5.998866 \n\nskewness(arthritis)\n\nAgeAtOnset \n-0.8257301 \n\n\nKURTOSIS\nKurtosis is a measure of how “peaked” (leptokurtic) a frequency distribution is or how “flattened” (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\n\nRoot_length_mm \n      3.354701 \n\nkurtosis(tccb)\n\nTcCB_ppb \n46.11769 \n\nkurtosis(arthritis)\n\nAgeAtOnset \n  2.030982 \n\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g. weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.\n\n\n\n\n\n\nMost natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is “flatness”).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We’ll look at these later.",
    "crumbs": [
      "**MODULE 1**",
      "Exploratory Data Analysis EDA"
    ]
  },
  {
    "objectID": "module01/020-exploring_data.html#numerical-summaries",
    "href": "module01/020-exploring_data.html#numerical-summaries",
    "title": "Exploratory Data Analysis EDA",
    "section": "",
    "text": "The summary descriptive characteristics of a sample of objects, that is, a subset of the population, are called statistics. Sample statistics can have different values, depending on how the sample of the population was chosen. Statistics are denoted by various symbols, but (almost) never by Greek letters e.g. sample mean, \\(\\bar y\\) and sample standard deviation, \\(s\\).\n\n\nThese statistics are also sometimes referred to as measures of location.\nARITHMETIC MEAN\nThe most widely used measure of central tendency is the arithmetic mean or average. The population mean, \\(\\mu\\) (“mu”) is the sum of all the values of the variable under study divided by the total number of objects in the population, Each value is algebraically denoted by a \\(y\\) with a subscript denotation \\(i\\). E.g. a small theoretical population whose objects had values 1, 6, 4, 5, 6, 3, 8, 7 would be denoted:\n\\[y_1 = 1,\\ y_2 = 6,\\ y_3 = 4,\\ y_4 = 5,\\ y_5 = 6,\\ y_6 = 3,\\ y_7 = 8,\\ y_8 = 7\\]\n\n\n\n\n\n\nNote\n\n\n\nSome texts will use \\(X\\) instead of \\(x\\) or \\(Y\\) instead of \\(y\\) as the symbol for a value.\n\n\nWe would denote the population size with a capital \\(N\\). In our theoretical population \\(N = 8\\).\nThe population mean, \\(\\mu\\), would be:\n\\[\\frac{1+6+4+5+6+3+8+7}{8}=5\\]\nThe algebraic shorthand formula for a population mean is\n\\[\\mu = \\frac{1}{N}\\sum_{i=1}^{N} y_i\\]\nThe Greek letter \\(\\Sigma\\) (“sigma”) indicates summation, the subscript \\(i = 1\\) means to start with the first observation, and the superscript \\(N\\) means to continue until and including the \\(N\\)th observation.\nFor the example above,\n\\[\\sum_{i=2}^{5} y_i=y_2+y_3+y_4+y_5=6+4+5+6=21\\]\nTo reduce the clutter, if the summation sign is not indexed, for example \\(y_i\\), it is implied that the operation of addition begins with the first observation and continues through the last observation in a population, that is,\n\\[\\sum_{i=1}^{N} y_i = \\sum y_i\\]\nThe sample mean is defined by\n\\[\\bar y = \\frac{1}{n}\\sum_{i=1}^{n} y_i\\]\nwhere \\(n\\) is the sample size.\nThe symbol \\(\\bar y\\) (read “y bar”) indicates that the observations of a subset of size n from a population have been averaged. \\(y\\) is fundamentally different from \\(\\mu\\) because samples from a population can have different values for their sample mean, that is, they can vary from sample to sample within the population. The population mean, however, is constant for a given population.\nAgain consider the small theoretical population \\(1, 6, 4, 5, 6, 3, 8, 7\\). A sample size of 3 may consist of \\(5, 3, 4\\) with \\(\\bar y =6\\) OR it could be \\(1,3,5\\) with \\(\\bar y = 3\\)\nEach sample mean \\(\\bar y\\) is an unbiased estimate of \\(m\\) but depends on the values included in the sample and sample size for its actual value. We would expect the average of all possible \\(y\\)’s to be equal to the population parameter, \\(\\mu\\). This is, in fact, the definition of an unbiased estimator of the population mean.\nThe R function is mean().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmean(y)\n\n[1] 5\n\n\nThe Excel command is =AVERAGE()\nMEDIAN\nThe median is the “middle” value of an ordered list of observations. The population median \\(M\\) is the \\(\\left( \\frac{N+1}{2} \\right)th\\) sorted value, where \\(N\\) is the population size. Note that this parameter is not a Greek letter and is seldom computed in practice. A sample median \\(\\tilde y\\) (read “y tilde”) is the statistic used to approximate or estimate the population median. \\(\\tilde y\\) is the \\(\\left( \\frac{n+1}{2} \\right)th\\) sorted value where n is the sample size.\nThe R function is median().\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmedian(y)\n\n[1] 5.5\n\n\nThe Excel command is =MEDIAN().\nMODE\nThe mode is the most frequently occurring value in a data set.\nThere is no direct function for the mode in R, the following code is an example of how it can be calculated.\n\nmode_function &lt;- function(x) {\n  uniqx &lt;- unique(x)\n  uniqx[which.max(tabulate(match(x, uniqx)))]\n}\n\n# Example usage\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmode &lt;- mode_function(y)\nprint(mode)\n\n[1] 6\n\n\nThe Excel command is =MODE().\nOVERVIEW OF MEASURES OF CENTRAL TENDENCY:\n\nThe mean is a purposeful measure only for a quantitative variable, whether it is continuous (e.g. height) or discrete (e.g. number of nematodes).\nThe median can be calculated whenever a variable can be ranked (including when the variable is quantitative).\nThe mode can be calculated for categorical variables, as well as for quantitative and ranked variables.\nThe sample median expresses less information than the sample mean because it utilizes the ranks and not the actual values of each measurement.\nThe median, however, is resistant to the effects of outliers. Extreme values or outliers in a sample can drastically affect the sample mean, while having little effect on the median.\n\n\n\n\nMeasures of central tendency alone are not sufficient to fully describe a data set. The following figure illustrates 3 distributions that all have the same mean but different levels of dispersion or spread.\n\nlibrary(ggplot2)\n\n# Define the data for the three normal distributions\nmean_value &lt;- 0\nstd_dev_A &lt;- 1   # Least spread\nstd_dev_B &lt;- 2\nstd_dev_C &lt;- 3   # Most spread\n\n# Create a data frame for plotting\nx_values &lt;- seq(-10, 10, length.out = 300)\nnormal_data &lt;- data.frame(\n  x = c(x_values, x_values, x_values),\n  y = c(dnorm(x_values, mean_value, std_dev_A), \n        dnorm(x_values, mean_value, std_dev_B), \n        dnorm(x_values, mean_value, std_dev_C)),\n  curve = factor(c(rep(\"A\", length(x_values)), \n                   rep(\"B\", length(x_values)), \n                   rep(\"C\", length(x_values)))\n  )\n)\n\n# Generate the plot\nggplot(normal_data, aes(x = x, y = y, color = curve)) +\n  geom_line() +\n  labs(title = \"Normal Distributions with Different Spreads\",\n       x = \"Value\",\n       y = \"Density\") +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\"),\n                     labels = c(\"A: Least Spread\", \"B\", \"C: Most Spread\")) +\n  theme_minimal()\n\n\n\n\nFigure 1.1 A, B & C are distributions with the same mean (zero) but varying standard deviations.\n\n\n\n\nIn graph A, most of the values are concentrated around the mean. It has less dispersion (or spread of values) than the other distributions. Graph C has more dispersion than the others. Its data is more “spread” out. A measure of dispersion provides some indication of the amount of variation that the data exhibits.\nRANGE\nThe simplest measure of dispersion or “spread” is the range – the difference between the largest and smallest observations in a group of data.\nThe sample range is a crude and biased estimator of the population range as its dependent on the composition and size of the sample you’ve taken. [It’s unlikely that the sample will include the largest and smallest values from the population, so the sample range usually underestimates the population range and is, therefore, a biased estimator.]\nThe R function is max(y)-min(y)\n\ny &lt;- c(1, 6, 4, 5, 6, 3, 8, 7)\nmax(y)-min(y)\n\n[1] 7\n\n\nThe Excel command is =MAX()-MIN()\nINTERQUARTILE RANGE\nRather than describe variability in terms of variation around the mean, we more directly quantify the “spread”. Just as the median divides the sample into two, the quartiles divide the sample into four groups:\n\n25% of observations \\(\\le\\) lower quartile (Q1)\n50% of observations \\(\\le\\) median (Q2)\n75% of observations \\(\\le\\) upper quartile (Q3)\n\nThe example data below (the number of days pigs take to reach bacon weight) has been sorted from lowest to highest.\n98 100 100 103 105 107 110 113 115\nThe lower quartile Q1 is the \\(\\left( \\frac{n+1}{4} \\right)th\\) sorted value = \\(\\left( \\frac{9+1}{4} \\right)th = 2.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 100 + 0.5 \\times 100 = 100 \\text{ days}\\]\nThe upper quartile Q3 is the \\(\\left( \\frac{3(n+1)}{4} \\right)th\\) sorted value = \\(\\left( \\frac{3(9+1)}{4} \\right)th = 7.5th\\) sorted value.\nThis means we need to obtain a weighted average of the 2nd and 3rd sorted value: \\[0.5 \\times 110 + 0.5 \\times 113 = 111.5 \\text{ days}\\]\nThe inter quartile range = Upper Quartile - Lower Quartile\n\\[IQR = Q3-Q1=111.5-100=11.5 \\text{ days}\\]\nSo 50% of pigs reach bacon weight within a range of 11.5 days.\nThe following is an example of calculations in R - note that we use the type 6 calculation and the default in R is type 7 which we generally use.\nFor more information see:\n\nHyndman, R.J. and Fan, Y., 1996. Sample quantiles in statistical packages. The American Statistician, 50(4), pp.361-365.\n\n\ny &lt;- c(98, 100, 100, 103, 105, 107, 110, 113, 115)\n\nquantile(y, 0.75, type = 6) - quantile(y, 0.25, type = 6) ## Type 6\n\n 75% \n11.5 \n\n## Default which we will generally use going forward\nquantile(y, 0.75) - quantile(y, 0.25) \n\n75% \n 10 \n\n\nPERCENTILES\nThe 5th and 95th percentiles cut off 5% of the most extreme values in the distribution of values for the sample. The 1st and 99th percentiles may be similarly defined. As with quartiles, a list of percentiles may be far more informative than the standard deviation for summarizing the spread of values about the mean or median, especially when the spread is asymmetrical.\nVARIANCE\nIn general, we have \\(n\\) observations, so the general formula for the sample variance is\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nThe units for the variance are always the units of the original measurement squared. If units of measurement were kg (e.g. body weight), then the variance would have units \\(kg^2\\).\nSTANDARD DEVIATION\nTo have a measure of variability with the same units as the original measurement, we take the square root of the variance. This is the standard deviation of the observations (usual symbol is s).\nSample standard deviation,\n\\[s = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] DEGREES OF FREEDOM\nThe value n-1 in the above equations for variance and standard deviation is referred to as the degrees of freedom (df). Ashcroft & Pereira (2003) explain this concept in the following way.\n“The degrees of freedom in our analysis is the number of observations that are allowed to vary if our sample characteristic is to estimate precisely the population characteristic. For instance, when we are estimating just one population characteristic like the population variance and out sample size is n, the degrees of freedom is n-1 since control of just one observation (i.e. the rest are free to vary) is all that is required to make our sample variance exactly equal to the population variance.\nAs an example, suppose we have a sample of 5 observations \\((x_1, x_2, x_3, x_4, x_5; n=5)\\) from a population whose mean is 6. Observe in the table below how control of the last observation can make our sample mean exactly equal to the population mean of 6 when the first 4 observations are free to change their values.\n\n\n\nx1\nx2\nx3\nx4\n(Make x5)\nSample mean\n\n\n\n\n2\n4\n7\n8\n(9)\n6\n\n\n4\n6\n5\n7\n(8)\n6\n\n\n3\n7\n4\n9\n(7)\n6\n\n\n\nHere we see that a group of 5 observations being used to estimate a single population characteristic has 4 degrees of freedom. In general, when k population characteristics are being estimated from n observations, the degrees of freedom of the analysis is n-k.”\nCOEFFICIENT OF VARIATION\nThe coefficient of variation (CV) is used is used to aide in comparing the variability of two samples that have widely differing means. It is usually expressed as a percentage, and has no units.\n\\[CV = \\frac{s}{\\bar{y}} \\times 100\\%\\]",
    "crumbs": [
      "**MODULE 1**",
      "Exploratory Data Analysis EDA"
    ]
  },
  {
    "objectID": "module01/020-exploring_data.html#tabular-summaries",
    "href": "module01/020-exploring_data.html#tabular-summaries",
    "title": "Exploratory Data Analysis EDA",
    "section": "",
    "text": "Tables are a way of organizing the data collected or providing a summary presentation of the data. The two most common types of tabular summaries you will encounter are tables of means and frequency tables.\nAn example of a table of means is the following table that shows the mean number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadrat was randomly assigned to one of four treatments: control, low, medium, and high. The table shows the mean number of sedge plants found in each treatment. We will simulate the data for this example.\n\nlibrary(dplyr)\nlibrary(kableExtra)\n\nset.seed(123) # For reproducibility\n\n# Simulate data\nquadrats &lt;- data.frame(\n  Treatment = factor(rep(c(\"Control\", \"Low\", \"Medium\", \"High\"), each = 200)),\n  SedgePlants = c(\n    rpois(200, lambda = 5),  # Assuming a Poisson distribution for count data \n    rpois(200, lambda = 10),\n    rpois(200, lambda = 15),\n    rpois(200, lambda = 20)\n  )\n)\n\nquadrats &lt;- quadrats %&gt;%\n  group_by(Treatment) %&gt;%\n  summarise(MeanSedgePlants = mean(SedgePlants))\nkable(quadrats)\n\n\n\n\nTreatment\nMeanSedgePlants\n\n\n\n\nControl\n5.060\n\n\nHigh\n20.050\n\n\nLow\n9.955\n\n\nMedium\n14.860\n\n\n\n\n\n\n\n\nAn example of a tabular summary is the following table that shows the number of sedge plants, Carex flacca, found in 800 sample quadrats in an ecological study of grasses. Each quadra\nSedge plant data sourced from Glover & Mitchell (2002)\n\n\n\nPlants/quadrat \\((y_i)\\)\nFrequency \\((f_i)\\)\n\n\n\n\n0\n268\n\n\n1\n316\n\n\n2\n135\n\n\n3\n61\n\n\n4\n15\n\n\n5\n3\n\n\n6\n1\n\n\n7\n1\n\n\n\nThis table can be further organized into a relative frequency \\((f_i/n \\times 100)\\) table by expressing each row as a percentage of the total observation or into a cumulative frequency distribution by accumulating all observations up to and including each row i.e. \\(\\Sigma^r_{i=1}f_i\\) where \\(r\\) is the row number. The cumulative frequency distribution could be further manipulated into a relative cumulative frequency distribution by expressing each row of the cumulative frequency distributions as a percentage of the total i.e. \\(\\Sigma^r_{i=1}f_i/n \\times 100\\)",
    "crumbs": [
      "**MODULE 1**",
      "Exploratory Data Analysis EDA"
    ]
  },
  {
    "objectID": "module01/020-exploring_data.html#graphical-summaries",
    "href": "module01/020-exploring_data.html#graphical-summaries",
    "title": "Exploratory Data Analysis EDA",
    "section": "",
    "text": "Frequency tabulations can be represented as a graph of frequency (raw or percentage) against the measurement variable. Discrete data are sometimes expressed as a bar graph where bars are spaced equidistantly along the horizontal axis. Figure 1.2 is a relative frequency histogram (also known as a percentage frequency histogram) of the sedge plant data. The data represents data from sedge plant counts in 800 1m x 1m quadrats Ideally this data set (since it is discrete) would be plotted as a bar graph.\n\nlibrary(tidyverse)\n\nsedge &lt;- read_csv(\"Sedge.csv\", show_col_types = FALSE)\n\nggplot(sedge, aes(x = Plants)) +\n  geom_histogram(aes(y = after_stat(count)/sum(after_stat(count)) * 100), bins = 8) +\n  ylab(\"Percentage\") +\n  xlab(\"Number of plants per quadrat\")\n\n\n\n\nFigure 1.2 Relative frequency histogram of sedge plant data.\n\n\n\n\nContinuous measurements are free to take any whole or fractional number within their range e.g. plant height, soil pH, concentration of nitrates in a water sample. Histograms (with bars touching each other) are the norm for continuous data.\n\nbentgrass &lt;- read_csv(\"Bentgrass.csv\", show_col_types = FALSE)\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\n\n\n\n\nFigure 1.3 Frequency histogram of creeping bentgrass data.\n\n\n\n\nA histogram can give nearly complete information about the distribution of data. For example, from Figure 1.3 above (that shows a fairly symmetric distribution) we can estimate that the mean ≈ 95 mm (the centre of the data) and standard deviation ≈ 15 mm (since for symmetric distributions approximately 95% of data lies within 2 standard deviations either side of the mean i.e. a total of 4 standard deviations across 95% of the data values).\nA histogram needs a relatively large sample size for it to be informative (i.e. 30 or more data values).\n\n\n\nBoxplots show the shape of the distribution of data very clearly and are also helpful in identifying any outlying (or extreme) values.\nExample 1 Creeping bentgrass turf was laid in an experiment to assess root growth. Eighty (80) “plugs” were randomly sampled 4 weeks after laying. Root growth was measured by averaging the length (mm) of the ten longest roots in each plug.\n\nggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\n\n\n\n\n\n\n\n\nNotes on boxplots:\n\n50% of the data are contained within the box (inter quartile range).\nWhiskers are extended to a maximum of 1.5 x IQR\nAny data values beyond these maximum whisker lengths are plotted individually, usually by an asterisk or dot \\(\\Rightarrow\\) these may be outliers and distort the results of any further analysis\nA boxplot gives useful summary of the shape of the data distribution e.g. Is it symmetric or skewed? Are there any outliers?\nBoxplots do not need as many data values (as some other graphs such as histograms and dot plots) for them to be informative.\n\nGUIDELINES FOR MAKING A BOXPLOT\nWe will use the gravimetric water content of soil (%) from Method A in the irrigation data set to illustrate how a boxplot is constructed. In this example we have n = 10 observations. We can use the R function, summary() to calculate some of the important values in a boxplot such as Q1, Q2, and Median.\n\nsoil_water &lt;- data.frame(water_content = c(7.5, 9.0, 9.3, 10.4, 10.4, 10.6, 10.7, 11.6, 12.1, 12.8))\n\nsummary(soil_water$water_content)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.500   9.575  10.500  10.440  11.375  12.800 \n\n\nStep 1: Determining the Box:\n\nThe quartiles (Q1: 1st Qu. & Q3: 3rd Qu.) then become the basis for the “box”\nThe median is shown as a vertical line through the box.\n\nStep 2: Determining (potential) outliers\n\nCalculate IQR = Inter-quartile range = 11.375 - 9.575 = 1.8\nCalculate Q1 - 1.5 x IQR = 9.225 - 1.5 x 1.8 = 6.875 Since there are no observations smaller than 6.875, there are no low-valued outliers.\nCalculate Q3 + 1.5 x IQR = 11.375 + 1.5 x 1.8 = 14.075 Since there are no observations greater than 14.075, there are no high-valued outliers.\nIf any values were flagged to be (potential) outliers, they are plotted as individual points on the boxplot, usually as a “*” or \\(\\bullet\\).\n\nStep 3: Determining the “whisker” lengths\n\nExtend the whisker from the lower end of the box (Q1) to the smallest value that is not an outlier, i.e. to the smallest value greater then Q1 - 1.5 x IQR. That is, the whisker here will be extended down to the minimum value which is 7.5.\nExtend the whisker from the upper end of the box (Q3) to the largest value that is not an outlier, i.e. to the greatest value smaller then Q3 + 1.5 x IQR. That is, the whisker here will be extended up to 12.8.\n\nUsing these values, the following boxplot is obtained:\n\nggplot(soil_water, aes(x = water_content)) +\n  geom_boxplot() +\n  xlab(\"gravimetric water content of soil (%)\")\n\n\n\n\n\n\n\n\n\n\n\nScatter plots are used to represent graphically the relationship between two variables. The extent and nature of the relationship between two (or more) variables is quantified through tools such as correlation and regression. This will be covered in later chapters.",
    "crumbs": [
      "**MODULE 1**",
      "Exploratory Data Analysis EDA"
    ]
  },
  {
    "objectID": "module01/020-exploring_data.html#shapes-of-distributions",
    "href": "module01/020-exploring_data.html#shapes-of-distributions",
    "title": "Exploratory Data Analysis EDA",
    "section": "",
    "text": "This is a distribution such that the left hand side of the frequency polygon is a mirror image of the right hand side. For symmetrical distributions, the mean, median and mode all have the same value. Substantial differences in these three statistics could provide valuable information about the data set (as we’ll see in the sections on positively and negatively skewed distributions). Some examples of symmetric distributions appear below.\n\n\n\nFigure 1.4 Four examples of symmetric distributions.\n\n\nExample: Creeping bentgrass: root growth (mm)\n\nsummary(bentgrass)\n\n Root_length_mm  \n Min.   : 65.00  \n 1st Qu.: 86.75  \n Median : 93.00  \n Mean   : 93.86  \n 3rd Qu.:102.00  \n Max.   :135.00  \n\n\nNote that the mean and the median are similar (93.9, and 93.0 mm respectively). This is indicative of symmetric data. The boxplot and histogram below show that the data is symmetric about the mean. You will notice from the boxplot that there is one high value outlier indicated by the “x” at the right hand side of the graph. The “4” indicates that this is the 4th observation in the data set i.e. the value 135 mm.\n\nlibrary(patchwork)\np1 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_boxplot() +\n  xlab(\"Root length (mm)\")\np2 &lt;- ggplot(bentgrass, aes(x = Root_length_mm)) +\n  geom_histogram(bins = 20) +\n  xlab(\"Root length (mm)\")\np1+p2\n\n\n\n\nFigure 1.5 Boxplot (left) and histogram (right) of the creeping bentgrass data show symmetry.\n\n\n\n\n\n\n\nFor right-skewed distributions we find that the mode (if one exists) is always less than the median and the median is always less than the mean. Example: As part of an evaluation of a clean-up of a contaminated site, 100 soil samples were taken randomly across an area and the level of 1,2,3,4 Tetrachlorobenzene was recorded in parts per billion (TcCB, ppb).\nThe following descriptive analysis was undertaken.\n\ntccb &lt;- read_csv(\"TcCB.csv\", show_col_types = FALSE)\n\nsummary(tccb)\n\n    TcCB_ppb     \n Min.   : 0.010  \n 1st Qu.: 0.235  \n Median : 0.570  \n Mean   : 1.412  \n 3rd Qu.: 1.292  \n Max.   :26.600  \n\n\n\np1 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_boxplot() +\n  xlab(\"TcCB concentration (ppb)\")\np2 &lt;- ggplot(tccb, aes(x = TcCB_ppb)) +\n  geom_histogram(bins = 20) +\n  xlab(\"TcCB concentration (ppb)\")\np1+p2\n\n\n\n\nFigure 1.6 Boxplot (left) and histogram (right) of the TcCB data showing positive skewness.\n\n\n\n\nThe distribution is highly positively skewed (right skewed): there are extreme outliers at high levels. This is also demonstrated by the mean (1.412 ppb) being substantially greater than the median (0.570 ppb).\n\n\n\nFor left-skewed distributions we find that the mode is greater than the median and the median is greater than the mean.\nExample: - The age of onset of osteoarthritis was recorded in 13 dogs. - The majority of the values cluster around the 10-13 age range, which represents the more common onset age for the chronic condition in older dogs. - There are also some lower values representing the less common earlier onset of the condition.\n\narthritis &lt;- read_csv(\"Arthritis.csv\", show_col_types = FALSE)\n\nsummary(arthritis)\n\n   AgeAtOnset    \n Min.   : 3.800  \n 1st Qu.: 7.225  \n Median :11.150  \n Mean   : 9.714  \n 3rd Qu.:12.000  \n Max.   :13.200  \n\n\n\np1 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_boxplot() +\n  xlab(\"Age at onset (years)\")\np2 &lt;- ggplot(arthritis, aes(x = AgeAtOnset)) +\n  geom_histogram(bins = 5) +\n  xlab(\"Age at onset (years)\")\np1+p2\n\n\n\n\nFigure 1.7 Boxplot (left) and histogram (right) of arthritis data showing negative skewness.\n\n\n\n\nThe distribution is negatively skewed (left skewed): there are some outliers at low levels. This is also demonstrated by the mean (9.7 years) being marginally less than the median (11.1 years).\n\n\n\nThere are two statistics useful for describing shape.\nSKEWNESS\nSkewness is another name for asymmetry which means that one tail of the frequency distribution is drawn out more than the other. A skewness of zero implies a symmetrically shaped histogram, a negative value implies skewness to the left and a positive value implies skewness to the right.\n\nlibrary(moments)\nskewness(bentgrass)\n\nRoot_length_mm \n    0.08475045 \n\nskewness(tccb)\n\nTcCB_ppb \n5.998866 \n\nskewness(arthritis)\n\nAgeAtOnset \n-0.8257301 \n\n\nKURTOSIS\nKurtosis is a measure of how “peaked” (leptokurtic) a frequency distribution is or how “flattened” (platykurtic) it is. A negative value indicates platykurtosis (or flatness), and a positive value indicates leptokurtosis (peakedness).\n\nkurtosis(bentgrass)\n\nRoot_length_mm \n      3.354701 \n\nkurtosis(tccb)\n\nTcCB_ppb \n46.11769 \n\nkurtosis(arthritis)\n\nAgeAtOnset \n  2.030982 \n\n\nBIMODAL\nNote: Bimodal distributions expected indicate a mixture of samples from two populations (e.g. weights of male and females). While the mode is not often used in biological research, reporting the number of modes, if more than one, can be informative.",
    "crumbs": [
      "**MODULE 1**",
      "Exploratory Data Analysis EDA"
    ]
  },
  {
    "objectID": "module01/020-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "href": "module01/020-exploring_data.html#normality-versus-non-normality-in-the-descriptive-statistics-context",
    "title": "Exploratory Data Analysis EDA",
    "section": "",
    "text": "Most natural groups of objects show variation. Humans differ in height, even if of the same sex, race and age. In many instances, measurements of similar objects vary about their mean according to a well-defined function, the normal or Gaussian distribution function.\nThe normal distribution has the following characteristics:\n\nIt is symmetric about its mean, median and mode. Hence a normal distribution has a skewness of zero.\nIt is bell-shaped, with a kurtosis of zero (recall kurtosis is “flatness”).\nIt is a continuous curve defined for values from minus infinity to plus infinity.\nIt is completely defined by its mean and standard deviation. That is, if you know the mean and standard deviation of the normal curve, you can calculate its exact equation.\nPDF for normal distribution: \\(f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n95% of observations fall in the range defined by the mean plus or minus 1.96 standard deviations and 99% fall in the range defined by the mean plus or minus 2.58 standard deviations.\n\nSummary statistics for a sample drawn from a normally distributed population would usually include the range of values encountered, the arithmetic mean, the standard deviation and the size of the sample from which these statistics were calculated. All other information, including the frequency tabulation, the mode, median, percentiles, sample skewness and kurtosis would be superfluous.\nWe will look at the normal distribution in detail later in this section.\n\n\n\nFor data that do not conform to the theoretical normal distribution, the situation is more complex. No longer will the mean and standard deviation suffice in order to reconstruct the frequency distribution of the raw data. No longer would we expect only 5% of values to lie outside the mean plus or minus 1.96 standard deviations. A more detailed description of the characteristics of non-normal data is required.\nSubstantial differences between the model, median and arithmetic mean are apparent when a skewed distribution is considered. Clearly the three averages can have distinctly different values. Which is the most appropriate average? The mean is markedly affected by outlying observations whereas the median and mode are not.\nThe difference between the mean and median has important practical consequences for analysis of data that contains aberrant outlying values (perhaps because of errors at the time of measurement or during transcription in preparing the data). Such errors, if they go unnoticed, can seriously affect an analysis.\nMost modern statistical packages perform various tests to determine if your data are likely to have been drawn from a normally distributed population. We’ll look at these later.",
    "crumbs": [
      "**MODULE 1**",
      "Exploratory Data Analysis EDA"
    ]
  },
  {
    "objectID": "labs/Lab03.html",
    "href": "labs/Lab03.html",
    "title": "Lab 03 - Probability distributions",
    "section": "",
    "text": "Tip\n\n\n\nLearning Outcomes\nAt the end of this computer practical, students should be able to:\n\nCalculate Binomial probabilities\n\nUsing your calculator;\nR commands;\nR simulations.\n\nCalculate Poisson probabilities\n\nUsing your calculator;\nR commands;\nR simulations."
  },
  {
    "objectID": "labs/Lab03.html#notation",
    "href": "labs/Lab03.html#notation",
    "title": "Lab 03 - Probability distributions",
    "section": "Notation",
    "text": "Notation\nFactorials: \\(n! = n(n - 1)(n - 2)...(3)(2)(1)\\) for \\(n \\ge 1\\) and \\(0! = 1\\).\nBinomial coefficients: \\(\\left(\\begin{matrix}n\\\\x\\end{matrix}\\right)=\\frac{n!}{x!(n-x)!}\\) for \\(x=1,2,3,...,n\\)\nThe Binomial distribution models a context in which we have a fixed number n of independent Binary trials and a fixed likelihood of a success at each trial \\(p = P(success)\\).\nX = the number of successes in n trials \\(\\sim Bin(n,p)\\)\nProbability distribution function: \\(P(X = x) = \\left(\\begin{matrix}n\\\\x\\end{matrix}\\right)p^x(1-p)^{n-x}\\) for \\(x=1,2,3,...,n\\)\nCumulative distribution function (CDF): \\(F(x) = P(X \\le x)\\)."
  },
  {
    "objectID": "labs/Lab03.html#exercise-1---walk-through",
    "href": "labs/Lab03.html#exercise-1---walk-through",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 1 - Walk through",
    "text": "Exercise 1 - Walk through\nWith your neighbour, discuss how coin tossing is related to the Binomial distribution?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-2---walk-through",
    "href": "labs/Lab03.html#exercise-2---walk-through",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 2 - Walk through",
    "text": "Exercise 2 - Walk through\nPracticing using the Binomial distribution formula\n\nCalculate 4!, 3!, 2! and 1!.\n\n\\[4!=4 \\times 3 \\times 2 \\times 1\\]\nNow you calculate the rest\n\nShow that\n\n\\(\\left(\\begin{matrix}4\\\\0\\end{matrix}\\right)=1\\), \\(\\left(\\begin{matrix}4\\\\1\\end{matrix}\\right)=4\\), \\(\\left(\\begin{matrix}4\\\\2\\end{matrix}\\right)=6\\), \\(\\left(\\begin{matrix}4\\\\3\\end{matrix}\\right)=4\\), \\(\\left(\\begin{matrix}4\\\\4\\end{matrix}\\right)=1\\)\n\\(\\left(\\begin{matrix}4\\\\0\\end{matrix}\\right)=\\frac{4!}{0!(4-0)!}=\\frac{4!}{4!}=1\\)\n\\(\\left(\\begin{matrix}4\\\\1\\end{matrix}\\right)=\\frac{4!}{1!(4-1)!}=\\frac{4!}{1!\\times3!}=\\frac{4\\times3\\times2\\times1}{1\\times3\\times2\\times1}=\\frac{4}{1}=4\\)\n\nNow you calculate the rest!\nNote you can also calculate this using the nCr option on your calculator - watch this YouTube video which gives a nice demonstration with some elevator music in the background to relax you."
  },
  {
    "objectID": "labs/Lab03.html#exercise-3---walk-through",
    "href": "labs/Lab03.html#exercise-3---walk-through",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 3 - Walk through",
    "text": "Exercise 3 - Walk through\nCollaboration - Simulate the Binomial Distribution by Coin Tossing\n\nChoose a partner and together toss a coin 4 times counting the number of heads (use Flip a coin if you don’t have a coin). Record the number of heads in an excel spreadsheet called (Simulation 1) and set out a table in excel similar to below. Now repeat this process 19 more times.\n\n\n\n\n\nSimulation\nToss 1\nToss 2\nToss 3\nToss 4\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n\n\n3\n\n\n\n\n\n\n…\n\n\n\n\n\n\n…\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\nTally up the frequency of each number of heads and fill in a table similar to the following in excel.\n\n\n\n\nNumber heads \\(x\\)\n0\n1\n2\n3\n4\nTotal\n\n\n\n\nFrequency\n\n\n\n\n\n20\n\n\n\n\nNow fill in the probability distribution in excel = the proportion of number of heads in all simulations i.e. number of simulations ith 0 heads/20, number of simulations with 1 head/20, number of simulations with 2 heads/20,….\n\n\n\n\nNumber heads \\(x\\)\n0\n1\n2\n3\n4\nTotal\n\n\n\n\nProbability \\(P(X=x)\\)\n\n\n\n\n\n1\n\n\n\n\nnow see if you can make a barplot in Excel of the above table Excel instructinos\n\n\n\n\nBarchart 1\n\n\n\n\n\nBarchart 2\n\n\nQuestion: With your stats partner, discuss the shape of the distribution, is it what you would expect to see and compare this to what the barplot you made in excel, are they similar?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-4---walk-through",
    "href": "labs/Lab03.html#exercise-4---walk-through",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 4 - Walk through",
    "text": "Exercise 4 - Walk through\nNow we will use R to simulate 4 coin tosses representing 1 as heads and 0 as tails. Note that I have withheld the output to avoid “spoiling” the suprise.\n\ncoin &lt;- c(0,1) # vector representing coin tosses c(tails, heads)\nset.seed(1) # makes sure we all get the same answer i.e. we use the same randomly generated numbers\ntosses &lt;- sample(coin, size = 4*20, replace = T, prob = c(0.5, 0.5)) # this randomly picks a 0 or 1 from coin and there is an equal probability of sampling both. It creates a vector of results i.e. bo tosses in total\ndim(tosses) &lt;- c(20,4) # This creates a data frame with 4 columns and 20 rows that represents the 20 trials of 4 coin tosses\ntosses ## this prints the result\nNumHeads &lt;- rowSums(tosses) ## this sums the number of heads in each trial\nNumHeads # this prints the results  \n\n\nThe table function tallies each of the numbers of heads for each simulation. In each trial (4 tosses) How many times did you toss no heads, how many times did you toss 1 head, how many times did you toss 2 heads, how many times did you toss 3 heads and how many times did you toss 4 heads?\n\n\nFreq_table &lt;- table(NumHeads)\nFreq_table\n\nThe prob.table function divides each frequency by 20 i.e.\\(P(X=0)=2/20\\)\n\nprop.table(Freq_table) \n\n\nNow we can plot this. Notice how we customise the x-axis to ensure that 0 is displayed as even though there may be no instances where we got zero heads, it is still a possibility even if the probability is low. for this we use the argument , xaxt = “n” and then draw customised axis labels using the axis function.\n\n\nplot(prop.table(Freq_table), ylab=\"P(X=X)\", xlab = \"x\", xlim = c(0,4), xaxt = \"n\")\naxis(1, at = 0:4)"
  },
  {
    "objectID": "labs/Lab03.html#exercise-5",
    "href": "labs/Lab03.html#exercise-5",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 5",
    "text": "Exercise 5\nRepeat exercise 4, but this time change the ‘prob = c(0.5, 0.5)’ to ‘prob = c(0.3, 0.7)’.\nQuestion: What happens to the shape of the distribution? What type of coin does this change in probability represent?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-6",
    "href": "labs/Lab03.html#exercise-6",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 6",
    "text": "Exercise 6\nRepeat exercise 4, but this time change the set.seed(1) to set.seed(123).\nQuestion: What happens to the frequency table ‘Freq_table’ why do you think it might change?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-7",
    "href": "labs/Lab03.html#exercise-7",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 7",
    "text": "Exercise 7\nSix calves were born after artificial insemination (AI) with regular semen. Assuming that the probability for either being male or female is 0.5,\nQuestion: What is the probability that all 6 are male?\n\n\\(X=\\) number of male calves \\(\\sim Bin(n=6,p=0.5)\\) \\(P(X=6)=\\left(\\begin{matrix}6\\\\6\\end{matrix}\\right)0.5^6(1-0.5)^{6-6} = 0.015625\\)\nor in R we can calculate using the dbinom function which calculates the exact probability of having 6 male calfs born\n\ndbinom(6,6,0.5)\n\nQuestion: What is the probability that more than 4 calves are female?\nNOTE P(more than 4 calves are female) = P(less than 2 calcves are male) = P(X=0)+P(X=1)\n\\(=\\left(\\begin{matrix}6\\\\0\\end{matrix}\\right)0.5^0(1-0.5)^{6-0}+\\left(\\begin{matrix}6\\\\1\\end{matrix}\\right)0.5^1(1-0.5)^{6-1} = 0.109375\\)\nor in R\n\n1-pbinom(4,6,0.5)\n\nQuestion: What are your assumptions? What is a more accurate estimate of the P(male calf)? Beef Article\nAssumptions: Each of the births is independent and P(male calf = 0.5). (In reality, it is 1.06 males per every female born in large populations of cattle, which gives p = 0.5145631. See article link.)\nUsing the updated probability p = 0.5145631 and using R recalculate\n\nQuestion: What is the probability that all 6 are male?\n\nQuestion: What is the probability that more than 4 calves are female?\n\nQuestion: Finally, calculate what is the probability that exactly 4 females are born?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-1---walk-through-1",
    "href": "labs/Lab03.html#exercise-1---walk-through-1",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 1 - Walk through",
    "text": "Exercise 1 - Walk through\nLets take a look at an example. Let us assume that on average 5 shoppers enter a store each hour. Say the shop is open for 10 hours per day, what would a typical day look like. Assuming that the number of visits follows a Poisson distribution i.e. If \\(X \\sim Po(\\lambda)\\), and because we know \\(\\lambda=5\\) and \\(n=10\\), we can simulate this in R using the following:\n\nshoppers &lt;- rpois(10, 5)\nshoppers\n\nQuestion: Take a look at the minimum and maximum number of shoppers generated by the model and discuss with your neighbour how you, as a shop owner, might use this information?\n\nQuestion: Now run the simulation for a full 5 day week, is there any change?\n\nQuestion: Now suppose as a shop owner you are concerned that your shop assistant might get overwhelmed if more than 10 shoppers come in, in any one hour period, what is the probability of this?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-2",
    "href": "labs/Lab03.html#exercise-2",
    "title": "Lab 03 - Probability distributions",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe also often say that the Poisson Distribution is good for modeling rare events. For example, recent work in Drosophila suggests the spontaneous rate of deleterious mutations is 1.2 per diploid genome.\nOpen the following article and read the abstract and then search the word poisson and read the final two sentences of this paragraph.\nNature article\nAssume that X = the number of deleterious mutations \\(X \\sim Po(1.2)\\).\nQuestion: What is the probability that an individual has 0 mutations?\n\\(P(X=0)=\\frac{1.2^0e^{-1.2}}{0!}=0.3011942\\)\nOr in R\n\ndpois(0,1.2)\n\nQuestion: What is the probability that an individual has less than or equal to 2 mutations?\n\\(P(X\\le2)=P(X=0)+P(X=1)+P(X=2)\\) \\(=\\frac{1.2^0e^{-1.2}}{0!}+\\frac{1.2^1e^{-1.2}}{1!}+\\frac{1.2^2e^{-1.2}}{2!}\\) \\(=0.8794871\\)\nOr in R\n\nppois(2,1.2)\n\nMore recent research found that the average spontaneous rate of deleterious mutations was actually 1.4 per diploid genome. Using R recalculate the following.\n\nQuestion: What is the new probability that an individual has 0 mutations. How does this compare to the former probability, what does this suggest?\nQuestion: What is the new probability that an individual has less than or equal to 2 mutations How does this compare to the former probability, what does this suggest?"
  },
  {
    "objectID": "labs/Lab03.html#advanced-exercise",
    "href": "labs/Lab03.html#advanced-exercise",
    "title": "Lab 03 - Probability distributions",
    "section": "Advanced exercise",
    "text": "Advanced exercise\nFrom the tutorial we looked at Tomato germination. Use simulations to find approximate Binomial probabilities obtain an estimate of the probability of getting 7 or more germinating seeds, by doing simulations from a \\(Bin(8, 0.7)\\) in R.\nRemember that the exact probability is\n\n1-pbinom(6,8,0.7)\n\n[1] 0.2552983\n\n\n\nFirst try 10 simulations and fill out a similar table to below in excel. What is your estimate of the probability of 7 germinating seeds?\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 10, 8, 0.7) # randomly generates 10 values from Bin(8,0.7)\nseeds ## prints seeds\ntable(seeds)/10 # turns counts into probabilities\nbarplot(table(seeds)/10, xlab=\"Number Germinated x\", ylab=\"Probability\", col=\"green\")\n\n\nNext try 100 simulations.\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 100, 8, 0.7) # randomly generates 100 values from Bin(8,0.7)\nseeds ## prints seeds\ntable(seeds)/100 # turns counts into probabilities\nbarplot(table(seeds)/100, xlab=\"Number germinated (x)\", ylab=\"Probability\", col=\"green\")\n\n\nNow we will try 10000 simulations. What do you notice about the histogram?\n\n\nset.seed(123)\nseeds &lt;- rbinom(n = 10000, 8, 0.7) # randomly generates 100 values from Bin(8,0.7)\ntable(seeds)/10000 # turns counts into probabilities\nbarplot(table(seeds)/10000, xlab=\"Number germinated (x)\", ylab=\"Probability\", col=\"green\")\n\n\nNow we can compare the exact results and the simulated results by adding up the probabilities for 7 and 8 seeds germinating in each of the scenarios.\n\nNote that the numbers will may differ slightly each time as they a randomly generated unless you set the same seed.\n\n\n\nNumber germinated\n\\(P(X\\ge{7}\\))\n\n\n\n\nExact method using R\n\n\n\nSimulation (n=10)\n\n\n\nSimulation (n=100)\n\n\n\nSimulation (n=10000)\n\n\n\n\nQuestion: Which of the simulations gives the closest result to the exact method, why do you think this is? - try ask chatGPT or use the # q: What is ? prompt if you have enabled copilot."
  },
  {
    "objectID": "labs/Lab02.html",
    "href": "labs/Lab02.html",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "",
    "text": "This exercise encourages students to discuss academic integrity, and in particular the grey areas often present. Your demonstrator will provide you with a number of scenarios to discuss with each other in smaller groups, and then with the class.\nIf you are interested in more information on Academic Integrity at the University of Sydney, see the following link: Academic Integrity. Also ensure you have completed the Academic Honesty Education Module (AHEM). This must be complete before your first assessment is due (next week for ENVX1002).\n\n\n\n\n\n\nTip\n\n\n\nLearning Outcomes\nAt the end of this practical students should be able to:\n\nUse R to calculate simple summary statistics\nCreate basic plots using default R and ggplot\nDevelop your coding and Quarto skills\nDo basic data wrangling using dplyr\nProduce your own knitted Markdown document"
  },
  {
    "objectID": "labs/Lab02.html#walk-through-exercise---ggplot",
    "href": "labs/Lab02.html#walk-through-exercise---ggplot",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Walk through Exercise - ggplot",
    "text": "Walk through Exercise - ggplot\nIn this case, the package called ggplot2 to make awesome looking plots, create 1) plot the raw data using a strip chart and 2) a jitter boxplot for the SO4 data set from the tutorial. For each graphical summary you have used, describe what you see and relate that to the data i.e. is it symmetrical, what are the range of values and also comment on the advantages and disadvantages of each plot for describing the SO4 data set?\n\nStrip chart\n\n\n# install.packages(ggplot2)\nlibrary(ggplot2)\np &lt;- ggplot(water, aes(y=SO4, x=\"\")) +\n  geom_jitter(position=position_jitter(0))\np\n\n\n\n\n\n\n\n\n\nBoxplot with jittered points - the following is how to create a boxplot with the data points jittered\n\n\np &lt;- ggplot(water, aes(y=SO4, x=\"\")) +\n  geom_boxplot() + \n  geom_jitter(position=position_jitter(0.1))\np\n\n\n\n\n\n\n\n\nCheck out the cheat sheets here https://www.rstudio.com/resources/cheatsheets/ for more on making plots as well as manipulating data in R!"
  },
  {
    "objectID": "labs/Lab02.html#walk-through-exercise---skewness",
    "href": "labs/Lab02.html#walk-through-exercise---skewness",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Walk through Exercise - skewness",
    "text": "Walk through Exercise - skewness\nIn this case, the package called moments contains a function for calculating skew, called skewness. The skewness (\\(g_1\\)) of a data set gives an indication of its symmetry. The sign of the skewness tells us whether the data is positively or negatively skewed. It is useful as one source of evidence for determining whether the data has a symmetrical distribution, particularly when having to assess this for many variables at once. First we must install the package using the install.packages function and the load it using the library function. Note that I have put a comment # in front as I have already installed the package and you only need to install a package once!\n\n#install.packages(\"moments\")\nlibrary(moments)\n\nNow we can calculate the skewness of sulphate.\n\nskewness(water$SO4)\n\n[1] 0.1571807"
  },
  {
    "objectID": "labs/Lab02.html#exercise-1",
    "href": "labs/Lab02.html#exercise-1",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nLithology\nWhat is the most commonly sampled lithology?\nHint: use the table function to find out the frequency of each lithology. You could also create a bar plot of the lithology data (see land-use example above)."
  },
  {
    "objectID": "labs/Lab02.html#exercise-2",
    "href": "labs/Lab02.html#exercise-2",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nFor each of the clay and EC properties (at all depths),\n\nGive the most appropriate estimate of centre.\nIs the data symmetrical?\nAre there any unusual observations? Justify your answers.\n\nHint: use the summary values from above, use the skewness function from the moments package and also use hist and boxplot function to look for unusual observations and skewness."
  },
  {
    "objectID": "labs/Lab02.html#exercise-3",
    "href": "labs/Lab02.html#exercise-3",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Exercise 3",
    "text": "Exercise 3\nIs clay content more variable in the 0-30cm layer or in the 30-60cm layer? For the 0-30cm layer is clay or EC the most variable property. Justify your answers.\nHint: you may calculate the sd and/or var and/or IQR and/or the coefficient of variation (CV) for each property."
  },
  {
    "objectID": "labs/Lab02.html#exercise-4",
    "href": "labs/Lab02.html#exercise-4",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Exercise 4",
    "text": "Exercise 4\nUsing an appropriate measure of centre, which land use has the largest EC and clay content for each depth layer?"
  },
  {
    "objectID": "labs/Lab02.html#exercise-5",
    "href": "labs/Lab02.html#exercise-5",
    "title": "Lab 02 - Exploratory data analysis",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate boxplots of clay 0-30cm and clay 30-60cm for the different lithological classes. Are there any differences between the lithological classes based on the boxplots.\nThis is the end of the R component of the practical. Remember to save your files so you can access in the future."
  }
]